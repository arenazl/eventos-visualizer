#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Scraper automatizado para TODA EUROPA usando BrightData + Gemini AI
Procesa 21 paÃ­ses y 63 ciudades con scraping multi-tÃ©cnica

Uso:
    python scrape_europa_brightdata.py
    python scrape_europa_brightdata.py --cities 10  # Solo primeras 10 ciudades
    python scrape_europa_brightdata.py --country "Francia"  # Solo un paÃ­s
"""

import sys
import os
import json
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
from typing import List, Dict, Any, Optional

# Configurar UTF-8 para Windows
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')
    sys.stderr.reconfigure(encoding='utf-8')

# Paths
SCRIPT_DIR = Path(__file__).parent
BACKEND_DIR = SCRIPT_DIR.parent.parent
REGIONS_DIR = BACKEND_DIR / 'data' / 'regions' / 'europa'
RESULTS_DIR = BACKEND_DIR / 'data' / 'scrapper_results' / 'europa'
ENV_FILE = BACKEND_DIR / '.env'

# Cargar .env
if ENV_FILE.exists():
    load_dotenv(ENV_FILE)

# Configurar Gemini
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if not GEMINI_API_KEY:
    print('âŒ Error: GEMINI_API_KEY no encontrado en .env')
    sys.exit(1)

genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# BrightData config (opcional)
BRIGHTDATA_CONFIG = None
if os.getenv('BRIGHTDATA_USERNAME') and os.getenv('BRIGHTDATA_PASSWORD'):
    BRIGHTDATA_CONFIG = {
        'username': os.getenv('BRIGHTDATA_USERNAME'),
        'password': os.getenv('BRIGHTDATA_PASSWORD'),
        'endpoint': os.getenv('BRIGHTDATA_ENDPOINT', 'rotating-residential.brightdata.com:22225')
    }
    print('âœ… BrightData configurado')
else:
    print('âš ï¸  BrightData NO configurado - usando solo Gemini AI')


def load_european_cities() -> List[Dict]:
    """
    Carga todas las ciudades europeas de los archivos JSON
    Retorna lista de ciudades con paÃ­s, cÃ³digo, regiÃ³n, coordenadas
    """
    all_cities = []

    # Regiones de Europa
    regions = [
        'europa-occidental',
        'europa-meridional',
        'europa-septentrional',
        'europa-oriental',
        'europa-nororiental'
    ]

    for region in regions:
        region_path = REGIONS_DIR / region

        if not region_path.exists():
            print(f'âš ï¸  RegiÃ³n no encontrada: {region}')
            continue

        # Leer todos los JSON de la regiÃ³n
        for json_file in region_path.glob('*.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                country = data.get('country')
                country_code = data.get('country_code')
                region_name = data.get('region')
                cities = data.get('cities', [])

                # Agregar metadata a cada ciudad
                for city in cities:
                    city['country'] = country
                    city['country_code'] = country_code
                    city['region'] = region_name
                    city['source_file'] = json_file.stem
                    all_cities.append(city)

            except Exception as e:
                print(f'âŒ Error leyendo {json_file}: {e}')
                continue

    return all_cities


async def scrape_city_with_gemini(city: Dict, max_retries: int = 2) -> Optional[List[Dict]]:
    """
    Scrapea eventos de una ciudad usando Gemini AI
    """
    city_name = city['name']
    country = city['country']

    print(f'\nğŸ¤– Scrapeando {city_name}, {country} con Gemini AI...')

    # URLs comunes de eventos por paÃ­s
    search_queries = {
        'ES': f'eventos {city_name} espaÃ±a',
        'FR': f'Ã©vÃ©nements {city_name} france',
        'GB': f'events {city_name} uk',
        'DE': f'veranstaltungen {city_name} deutschland',
        'IT': f'eventi {city_name} italia',
        'PT': f'eventos {city_name} portugal',
        'NL': f'evenementen {city_name} nederland',
        'BE': f'Ã©vÃ©nements {city_name} belgique',
        'AT': f'veranstaltungen {city_name} Ã¶sterreich',
        'CH': f'Ã©vÃ©nements {city_name} suisse',
        'SE': f'evenemang {city_name} sverige',
        'NO': f'arrangementer {city_name} norge',
        'DK': f'begivenheder {city_name} danmark',
        'FI': f'tapahtumat {city_name} suomi',
        'PL': f'wydarzenia {city_name} polska',
        'CZ': f'udÃ¡losti {city_name} Äesko',
        'HU': f'esemÃ©nyek {city_name} magyarorszÃ¡g',
        'RO': f'evenimente {city_name} romÃ¢nia',
        'GR': f'ÎµÎºÎ´Î·Î»ÏÏƒÎµÎ¹Ï‚ {city_name} ÎµÎ»Î»Î¬Î´Î±',
        'IE': f'events {city_name} ireland',
        'RU': f'ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ {city_name} Ñ€Ğ¾ÑÑĞ¸Ñ'
    }

    country_code = city.get('country_code', 'ES')
    search_query = search_queries.get(country_code, f'events {city_name}')

    # Prompt para Gemini
    prompt = f"""
Busca en internet eventos actuales y prÃ³ximos en {city_name}, {country}.

Busca eventos de TODAS las categorÃ­as:
- Conciertos y mÃºsica (rock, pop, electrÃ³nica, jazz, clÃ¡sica)
- Teatro y espectÃ¡culos
- Deportes (fÃºtbol, bÃ¡squet, etc.)
- Festivales y ferias
- Arte y exposiciones
- GastronomÃ­a
- TecnologÃ­a y conferencias
- Eventos culturales

IMPORTANTE:
1. Solo eventos REALES que existan actualmente
2. Con fecha especÃ­fica (dÃ­a, mes, aÃ±o)
3. Con ubicaciÃ³n especÃ­fica en {city_name}
4. Incluir URL del evento si estÃ¡ disponible

Responde SOLO con JSON vÃ¡lido:
{{
  "eventos": [
    {{
      "title": "Nombre del evento",
      "date": "YYYY-MM-DD",
      "venue": "Lugar especÃ­fico",
      "city": "{city_name}",
      "country": "{country}",
      "description": "DescripciÃ³n breve",
      "category": "mÃºsica|teatro|deportes|cultural|gastronomÃ­a|tech|arte",
      "url": "URL del evento",
      "source": "Fuente de la informaciÃ³n"
    }}
  ]
}}

Si NO encuentras eventos reales, responde: {{"eventos": []}}
NO inventes eventos. Solo eventos REALES con fuente verificable.
"""

    try:
        response = model.generate_content(prompt)
        response_text = response.text.strip()

        # Limpiar markdown
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]

        response_text = response_text.strip()

        # Parsear JSON
        data = json.loads(response_text)
        eventos = data.get('eventos', [])

        # Agregar metadata de la ciudad
        for evento in eventos:
            evento['latitude'] = city.get('latitude')
            evento['longitude'] = city.get('longitude')
            evento['country_code'] = country_code
            evento['scraping_method'] = 'gemini_ai_search'
            evento['scraped_at'] = datetime.now().isoformat()

        print(f'  âœ… {len(eventos)} eventos encontrados')

        return eventos if eventos else None

    except json.JSONDecodeError as e:
        print(f'  âŒ Error JSON: {str(e)[:100]}')
        return None
    except Exception as e:
        print(f'  âŒ Error: {str(e)[:100]}')
        return None


async def scrape_city_with_brightdata(city: Dict) -> Optional[List[Dict]]:
    """
    Scrapea eventos usando BrightData (si estÃ¡ configurado)
    """
    if not BRIGHTDATA_CONFIG:
        return None

    city_name = city['name']
    country = city['country']

    print(f'\nğŸŒ Scrapeando {city_name}, {country} con BrightData...')

    # Importar el scraper de BrightData
    try:
        sys.path.append(str(BACKEND_DIR / 'legacy'))
        from facebook_bright_data_scraper import FacebookBrightDataScraper

        scraper = FacebookBrightDataScraper(BRIGHTDATA_CONFIG)

        # BÃºsqueda especÃ­fica por ciudad
        query = f"eventos {city_name} {country}"
        events = await scraper.method_3_facebook_search(query)

        if events:
            # Agregar metadata
            for event in events:
                event['city'] = city_name
                event['country'] = country
                event['latitude'] = city.get('latitude')
                event['longitude'] = city.get('longitude')
                event['scraping_method'] = 'brightdata_facebook'
                event['scraped_at'] = datetime.now().isoformat()

            print(f'  âœ… {len(events)} eventos encontrados con BrightData')
            return events

    except Exception as e:
        print(f'  âš ï¸  BrightData error: {str(e)[:100]}')

    return None


async def scrape_single_city(city: Dict, use_brightdata: bool = True) -> Dict:
    """
    Scrapea una ciudad usando todos los mÃ©todos disponibles
    """
    city_name = city['name']
    country = city['country']

    print(f'\n{"="*70}')
    print(f'ğŸŒ Procesando: {city_name}, {country}')
    print(f'ğŸ“ Coords: {city["latitude"]}, {city["longitude"]}')
    print(f'{"="*70}')

    all_events = []

    # MÃ©todo 1: BrightData (si estÃ¡ disponible)
    if use_brightdata and BRIGHTDATA_CONFIG:
        brightdata_events = await scrape_city_with_brightdata(city)
        if brightdata_events:
            all_events.extend(brightdata_events)

    # MÃ©todo 2: Gemini AI (siempre)
    gemini_events = await scrape_city_with_gemini(city)
    if gemini_events:
        all_events.extend(gemini_events)

    # Deduplicar eventos
    unique_events = []
    seen_titles = set()

    for event in all_events:
        title = event.get('title', '').lower().strip()
        if title and title not in seen_titles and len(title) > 5:
            seen_titles.add(title)
            unique_events.append(event)

    return {
        'city': city_name,
        'country': country,
        'country_code': city.get('country_code'),
        'region': city.get('region'),
        'latitude': city.get('latitude'),
        'longitude': city.get('longitude'),
        'total_events': len(unique_events),
        'events': unique_events,
        'scraped_at': datetime.now().isoformat(),
        'methods_used': ['brightdata' if BRIGHTDATA_CONFIG else None, 'gemini_ai']
    }


def save_results(result: Dict, region: str):
    """
    Guarda resultados en el directorio apropiado
    """
    city_name = result['city']
    country = result['country']
    total = result['total_events']

    if total == 0:
        print(f'âš ï¸  {city_name}: Sin eventos - no se guarda archivo')
        return

    # Normalizar nombres para archivos
    city_slug = city_name.lower().replace(' ', '_').replace('Ã¡', 'a').replace('Ã©', 'e').replace('Ã­', 'i').replace('Ã³', 'o').replace('Ãº', 'u')
    country_slug = country.lower().replace(' ', '_')

    # Directorio por regiÃ³n
    output_dir = RESULTS_DIR / region / country_slug
    output_dir.mkdir(parents=True, exist_ok=True)

    # Nombre de archivo
    timestamp = datetime.now().strftime('%Y%m%d')
    filename = f'{city_slug}_{timestamp}_brightdata.json'
    output_file = output_dir / filename

    # Guardar
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)

    print(f'\nğŸ’¾ Guardado: {output_file}')
    print(f'ğŸ“Š {total} eventos de {city_name}, {country}')


async def main():
    """Main scraping function"""
    parser = argparse.ArgumentParser(
        description='Scraper automatizado de toda Europa con BrightData + Gemini AI'
    )
    parser.add_argument('--cities', type=int, help='NÃºmero mÃ¡ximo de ciudades a procesar')
    parser.add_argument('--country', type=str, help='Procesar solo un paÃ­s especÃ­fico')
    parser.add_argument('--no-brightdata', action='store_true', help='No usar BrightData')

    args = parser.parse_args()

    print('\n' + '='*70)
    print('ğŸŒ SCRAPER EUROPA - BRIGHTDATA + GEMINI AI')
    print('='*70)

    # Cargar ciudades
    print('\nğŸ“‹ Cargando ciudades europeas...')
    all_cities = load_european_cities()

    # Filtrar por paÃ­s si se especifica
    if args.country:
        all_cities = [c for c in all_cities if c['country'].lower() == args.country.lower()]
        print(f'ğŸ” Filtrando solo: {args.country}')

    # Limitar cantidad
    if args.cities:
        all_cities = all_cities[:args.cities]

    print(f'âœ… {len(all_cities)} ciudades a procesar\n')

    # EstadÃ­sticas
    total_processed = 0
    total_events = 0
    total_errors = 0

    # Procesar cada ciudad
    for i, city in enumerate(all_cities, 1):
        try:
            print(f'\n[{i}/{len(all_cities)}] Procesando {city["name"]}, {city["country"]}...')

            result = await scrape_single_city(city, use_brightdata=not args.no_brightdata)

            # Guardar si hay eventos
            if result['total_events'] > 0:
                # Determinar regiÃ³n para organizar archivos
                region_map = {
                    'Europa Occidental': 'europa-occidental',
                    'Europa Meridional': 'europa-meridional',
                    'Europa Septentrional': 'europa-septentrional',
                    'Europa Oriental': 'europa-oriental',
                    'Europa Nororiental': 'europa-nororiental'
                }
                region_dir = region_map.get(city.get('region'), 'europa-occidental')

                save_results(result, region_dir)
                total_events += result['total_events']

            total_processed += 1

            # PequeÃ±a pausa entre ciudades para no saturar APIs
            await asyncio.sleep(2)

        except Exception as e:
            print(f'âŒ Error procesando {city["name"]}: {str(e)[:100]}')
            total_errors += 1
            continue

    # Resumen final
    print('\n' + '='*70)
    print('ğŸ‰ SCRAPING COMPLETADO')
    print('='*70)
    print(f'âœ… Ciudades procesadas: {total_processed}/{len(all_cities)}')
    print(f'ğŸ“Š Total eventos encontrados: {total_events}')
    print(f'âŒ Errores: {total_errors}')
    print(f'ğŸ’¾ Resultados guardados en: {RESULTS_DIR}')
    print('='*70 + '\n')


if __name__ == "__main__":
    asyncio.run(main())
