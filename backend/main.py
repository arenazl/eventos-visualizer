import os
import asyncio
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query, Request
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn
from dotenv import load_dotenv
from colorama import init, Fore, Back, Style
init(autoreset=True)  # Initialize colorama

# Heroku configuration
try:
    from heroku_config import apply_heroku_config, is_heroku
    apply_heroku_config()
    if is_heroku():
        print("üåê Running on Heroku - Production mode activated")
except ImportError:
    print("Running locally - Development mode")
import asyncpg
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import logging
import sys
import aiohttp
import time

# Add backend to path
import platform
if platform.system() == 'Windows':
    sys.path.append(r'C:\Code\eventos-visualizer\backend')
else:
    sys.path.append('/mnt/c/Code/eventos-visualizer/backend')

# Import advanced scrapers - ONLY WORKING ONES
# from services.multi_technique_scraper import MultiTechniqueScraper
# from services.facebook_bright_data_scraper import FacebookBrightDataScraper  # MOVED TO LEGACY
# from services.facebook_human_session_scraper import FacebookHumanSessionScraper  # MOVED TO LEGACY
# Hybrid scrapers removed - only real data sources allowed
# from services.daily_social_scraper import DailySocialScraper
# from services.cloudscraper_events import CloudscraperEvents
# from services.eventbrite_massive_scraper import EventbriteMassiveScraper  # MOVED TO LEGACY
# from services.hybrid_sync_scraper import HybridSyncScraper
# from services.progressive_sync_scraper import ProgressiveSyncScraper
# from services.teatro_optimizado_scraper import TeatroOptimizadoScraper
# from services.rapidapi_facebook_scraper import RapidApiFacebookScraper

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìã SISTEMA DE LOGS PROLIJAMENTE FORMATEADOS
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def log_method_start(method_name: str, **params):
    """Logs method start with parameters in a neat square format"""
    param_str = " | ".join([f"{k}={v}" for k, v in params.items() if v is not None])
    if param_str:
        param_str = f" | {param_str}"
    
    logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"‚îÇ üöÄ EJECUTANDO: {method_name}{param_str}")
    logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

def log_method_success(method_name: str, **results):
    """Logs successful method completion with results"""
    result_items = []
    for k, v in results.items():
        if v is not None:
            result_items.append(f"{k}={v}")
    
    result_str = " | ".join(result_items) if result_items else "Sin datos adicionales"
    
    logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"‚îÇ ‚úÖ √âXITO: {method_name} | {result_str}")
    logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

def log_method_error(method_name: str, error: str, **params):
    """Logs method error with parameters and error details"""
    param_str = " | ".join([f"{k}={v}" for k, v in params.items() if v is not None])
    if param_str:
        param_str = f" | {param_str}"
    
    logger.error("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.error(f"‚îÇ ‚ùå ERROR: {method_name}{param_str}")
    logger.error(f"‚îÇ üí• Detalle: {error}")
    logger.error("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

def log_scraper_summary(method_name: str, scrapers_called: list, events_by_scraper: dict, total_events: int):
    """Logs scraper summary with detailed breakdown"""
    logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"‚îÇ üï∑Ô∏è  RESUMEN SCRAPERS: {method_name}")
    logger.info(f"‚îÇ üìä Total scrapers llamados: {len(scrapers_called)}")
    logger.info("‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    
    for scraper in scrapers_called:
        events_count = events_by_scraper.get(scraper, 0)
        status = "‚úÖ √âXITO" if events_count > 0 else "‚ö†Ô∏è  SIN DATOS"
        logger.info(f"‚îÇ {scraper}: {status} ({events_count} eventos)")
        
        if events_count > 0 and scraper in events_by_scraper:
            # Mostrar nombres de eventos si est√°n disponibles
            event_names = events_by_scraper.get(f"{scraper}_names", [])[:3]  # Primeros 3
            if event_names:
                for name in event_names:
                    logger.info(f"‚îÇ   ‚îú‚îÄ {name[:50]}{'...' if len(name) > 50 else ''}")
    
    logger.info("‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"‚îÇ üéØ TOTAL EVENTOS OBTENIDOS: {total_events}")
    logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")

# Get configuration from environment - Production ready
# New approach: Use single BACKEND_URL instead of separate HOST+PORT
IS_PRODUCTION = os.getenv("ENVIRONMENT", "development") == "production"

# Parse BACKEND_URL to extract host and port for uvicorn
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8001" if not IS_PRODUCTION else "https://funaroundyou-f21e91cae36c.herokuapp.com")

# Force localhost on Windows
if platform.system() == 'Windows' and not IS_PRODUCTION:
    BACKEND_URL = "http://localhost:8001"

# Extract host and port from URL for uvicorn.run()
from urllib.parse import urlparse
parsed_url = urlparse(BACKEND_URL)
HOST = parsed_url.hostname or "localhost"
BACKEND_PORT = parsed_url.port or (443 if parsed_url.scheme == "https" else 8001)

# Override with Heroku's PORT if available (Heroku deployment)
if os.getenv("PORT"):
    BACKEND_PORT = int(os.getenv("PORT"))
    HOST = "0.0.0.0"  # Heroku requires binding to all interfaces

# Force localhost for Windows
if platform.system() == 'Windows':
    HOST = "localhost"
    BACKEND_URL = f"http://localhost:{BACKEND_PORT}"

DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/eventos_db")
# ‚ùå REMOVED: No more hardcoded "Buenos Aires"
# Location will always come from user geolocation or parameter

# Connection pool for PostgreSQL
pool = None

async def update_facebook_cache_background():
    """
    Background job que actualiza el cache de Facebook cada 6 horas
    üï∞Ô∏è Windows Service pattern en Python - Timer autom√°tico
    """
    try:
        logger.info("üîÑ BACKGROUND JOB: Iniciando actualizaci√≥n de cache Facebook...")
        
        facebook_scraper = RapidApiFacebookScraper()
        
        # Hacer el request pesado (20s) - nadie espera aqu√≠
        events = await facebook_scraper.scrape_facebook_events_rapidapi(
            city_name="Buenos Aires",  # Temporary fallback for chat context 
            limit=50, 
            max_time_seconds=30.0  # M√°s tiempo para background
        )
        
        if events:
            # Guardar en cache - el scraper ya lo hace internamente
            logger.info(f"‚úÖ BACKGROUND JOB: Cache actualizado con {len(events)} eventos")
        else:
            logger.warning("‚ö†Ô∏è BACKGROUND JOB: No se obtuvieron eventos")
            
    except Exception as e:
        logger.error(f"‚ùå BACKGROUND JOB ERROR: {e}")
        # No crashea el servidor, solo logea el error

# WebSocket connections manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                pass

manager = ConnectionManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    global pool
    try:
        # Create connection pool on startup
        logger.info("üöÄ Starting Eventos Visualizer Backend...")
        # Try PostgreSQL connection
        try:
            pool = await asyncpg.create_pool(
                DATABASE_URL,
                min_size=5,
                max_size=20,
                command_timeout=60
            )
            logger.info("‚úÖ Database pool created successfully")
        except:
            logger.warning("‚ö†Ô∏è PostgreSQL not available, running without database")
            pool = None
        
        # Initialize database schema only if pool exists
        if pool:
            async with pool.acquire() as conn:
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS events (
                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                        title VARCHAR(255) NOT NULL,
                        description TEXT,
                        start_datetime TIMESTAMP WITH TIME ZONE,
                        end_datetime TIMESTAMP WITH TIME ZONE,
                        venue_name VARCHAR(255),
                        venue_address TEXT,
                        latitude DECIMAL(10, 8),
                        longitude DECIMAL(11, 8),
                        category VARCHAR(100),
                        subcategory VARCHAR(100),
                        price DECIMAL(10,2),
                        currency CHAR(3) DEFAULT 'ARS',
                        is_free BOOLEAN DEFAULT false,
                        external_id VARCHAR(255),
                        source_api VARCHAR(50),
                        image_url TEXT,
                        event_url TEXT,
                        created_at TIMESTAMP DEFAULT NOW(),
                        updated_at TIMESTAMP DEFAULT NOW()
                    );
                    
                    CREATE TABLE IF NOT EXISTS users (
                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                        email VARCHAR(255) UNIQUE NOT NULL,
                        name VARCHAR(255) NOT NULL,
                        avatar_url TEXT,
                        google_id VARCHAR(255),
                        latitude DECIMAL(10, 8),
                        longitude DECIMAL(11, 8),
                        radius_km INTEGER DEFAULT 25,
                        timezone VARCHAR(100) DEFAULT 'America/Argentina/Buenos_Aires',
                        locale VARCHAR(10) DEFAULT 'es-AR',
                        push_enabled BOOLEAN DEFAULT false,
                        created_at TIMESTAMP DEFAULT NOW()
                    );
                    
                    CREATE TABLE IF NOT EXISTS user_events (
                        user_id UUID REFERENCES users(id),
                        event_id UUID REFERENCES events(id),
                        status VARCHAR(50) DEFAULT 'interested',
                        notification_24h_sent BOOLEAN DEFAULT false,
                        notification_1h_sent BOOLEAN DEFAULT false,
                        calendar_synced BOOLEAN DEFAULT false,
                        created_at TIMESTAMP DEFAULT NOW(),
                        PRIMARY KEY (user_id, event_id)
                    );
                    
                    CREATE INDEX IF NOT EXISTS idx_events_location ON events(latitude, longitude);
                    CREATE INDEX IF NOT EXISTS idx_events_datetime ON events(start_datetime);
                    CREATE INDEX IF NOT EXISTS idx_events_category ON events(category);
                ''')
                logger.info("‚úÖ Database schema initialized")
        
        logger.info("üöÄ Heroku-ready: Scheduler removido, usar Heroku Scheduler add-on")
        
        # üß† CHAT MEMORY MANAGER RECREADO - Intentar inicializar
        try:
            from services.chat_memory_manager import chat_memory_manager
            logger.info("üß† Inicializando Chat Memory Manager...")
            success = await chat_memory_manager.initialize_memory_context()
            if success:
                logger.info("‚úÖ Chat Memory Manager inicializado con √©xito")
            else:
                logger.warning("‚ö†Ô∏è Chat Memory Manager fall√≥ al inicializar")
        except Exception as e:
            logger.error(f"‚ùå Error inicializando Chat Memory Manager: {e}")
            logger.info("‚ÑπÔ∏è Chat Memory Manager no disponible")
        
        yield
        
    except Exception as e:
        logger.error(f"‚ùå Database connection error: {e}")
        raise
    finally:
        if pool:
            await pool.close()
            logger.info("Database pool closed")

app = FastAPI(
    title="Eventos Visualizer API",
    description="Sistema completo de eventos mobile-first con PWA",
    version="1.0.0",
    lifespan=lifespan
)

# CORS configuration for WSL IP
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins - Heroku friendly
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],  # Expose all headers including cache-control
)

# Session middleware for OAuth
from starlette.middleware.sessions import SessionMiddleware
app.add_middleware(SessionMiddleware, secret_key=os.getenv("JWT_SECRET_KEY", "your-secret-key-here"))

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîê AUTENTICACI√ìN CON GOOGLE OAUTH2
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    from api.auth import router as auth_router
    app.include_router(auth_router)
    logger.info("üîê Autenticaci√≥n Google OAuth2 habilitada")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è No se pudo cargar autenticaci√≥n: {e}")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Error al configurar autenticaci√≥n: {e}")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üìç GEOLOCALIZACI√ìN
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
try:
    from api.geolocation import router as geolocation_router
    app.include_router(geolocation_router, prefix="/api/location", tags=["location"])
    logger.info("üìç Geolocalizaci√≥n habilitada")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è No se pudo cargar geolocalizaci√≥n: {e}")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Error al configurar geolocalizaci√≥n: {e}")

# üö´ NO-CACHE MIDDLEWARE - Deshabilitar todo el cache
@app.middleware("http")
async def add_no_cache_headers(request: Request, call_next):
    response = await call_next(request)

    # üî• NO aplicar no-cache a SSE streams - preservar Content-Type
    if not request.url.path.startswith("/api/events/stream"):
        # Agregar headers para deshabilitar cache completamente
        response.headers["Cache-Control"] = "no-cache, no-store, must-revalidate, max-age=0"
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"

    return response

# üîç REQUEST/RESPONSE LOGGER - Para ver todo lo que entra y sale
from middleware.request_logger import LoggingRoute, log_request_middleware

# Opci√≥n 1: Middleware simple (menos detallado pero m√°s estable)
# @app.middleware("http")
# async def add_logging_middleware(request: Request, call_next):
#     return await log_request_middleware(request, call_next)

# Opci√≥n 2: LOGGING DETALLADO CON EMOJIS - DESACTIVADO TEMPORALMENTE PARA DEBUG
# app.router.route_class = LoggingRoute

# Manual Facebook cache update endpoint - HEROKU BACKUP
@app.post("/api/update-facebook-cache")
async def manual_facebook_cache_update():
    """
    Endpoint manual para actualizar cache Facebook
    üîß Backup para Heroku Scheduler o testing
    ‚ö° Puede ser llamado manualmente si es necesario
    """
    try:
        logger.info("üîß MANUAL UPDATE: Actualizando cache Facebook...")
        
        if not os.getenv("RAPIDAPI_KEY"):
            return {
                "status": "error",
                "error": "RAPIDAPI_KEY no configurado",
                "timestamp": datetime.now().isoformat()
            }
        
        # Ejecutar actualizaci√≥n
        await update_facebook_cache_background()
        
        # Verificar resultado
        facebook_scraper = RapidApiFacebookScraper()
        cache_data = facebook_scraper.load_cache()
        events_count = len(cache_data.get("events", []))
        
        return {
            "status": "success",
            "message": "Cache actualizado manualmente",
            "events_count": events_count,
            "timestamp": datetime.now().isoformat(),
            "cache_info": cache_data.get("cache_info", {})
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en actualizaci√≥n manual: {e}")
        return {
            "status": "error", 
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "healthy", "message": "OK"}

@app.get("/api/test/gemini")
async def test_gemini_api():
    """
    üß™ Test endpoint para verificar que la API key de Gemini funciona
    """
    # 1. Primero mostrar qu√© API key est√° configurada
    api_key = os.getenv("GEMINI_API_KEY", "NO_CONFIGURADA")
    api_key_preview = f"{api_key[:10]}...{api_key[-4:]}" if len(api_key) > 14 else api_key

    try:
        from services.gemini_factory import gemini_factory

        # 2. Probar detecci√≥n de ciudad principal
        test_location = "Moron"
        parent_city = await gemini_factory.get_parent_location(test_location)

        return {
            "status": "success",
            "gemini_working": True,
            "api_key_configured": api_key_preview,
            "test_query": test_location,
            "detected_parent_city": parent_city,
            "message": f"‚úÖ Gemini API funcionando - Detect√≥: {test_location} ‚Üí {parent_city}"
        }
    except Exception as e:
        logger.error(f"‚ùå Error testing Gemini API: {e}")
        return {
            "status": "error",
            "gemini_working": False,
            "api_key_configured": api_key_preview,
            "error": str(e),
            "message": "‚ùå Gemini API no est√° funcionando"
        }

@app.get("/api/debug/sources")
async def debug_sources(location: str = "Buenos Aires"):
    """üîç DEBUG COPADO: Ver cu√°nto devuelve cada fuente"""
    import asyncio
    import time
    from services.gemini_factory import gemini_factory
    # from services.eventbrite_api import EventbriteMassiveScraper  # MOVED TO LEGACY
    
    debug_info = {
        "location_buscada": location,
        "fuentes_debug": {},
        "total_final": 0,
        "problemas_encontrados": []
    }
    
    try:
        # 1. INDUSTRIAL FACTORY - ALL SCRAPERS
        logger.info("üè≠ DEBUG - Probando Industrial Factory...")
        start_time = time.time()
        factory = gemini_factory  # Singleton - no need to instantiate
        all_events = await factory.execute_global_scrapers(location, context_data={})
        factory_time = time.time() - start_time
        debug_info["fuentes_debug"]["gemini_factory"] = {
            "eventos_devueltos": len(all_events),
            "tiempo_segundos": round(factory_time, 2),
            "primeros_3_titulos": [e.get("title", "Sin t√≠tulo") for e in all_events[:3]],
            "status": "‚úÖ OK" if all_events else "‚ùå VACIO"
        }
        
        # 2. LEGACY COMPATIBILITY (same data, different view)
        logger.info("üîÑ DEBUG - Mostrando datos como legacy...")
        oficial_events = all_events  # Same data
        argentina_events = all_events  # Same data 
        debug_info["fuentes_debug"]["legacy_view"] = {
            "eventos_devueltos": len(all_events),
            "tiempo_segundos": round(argentina_time, 2),
            "primeros_3_titulos": [e.get("title", "Sin t√≠tulo") for e in argentina_events[:3]],
            "status": "‚úÖ OK" if argentina_events else "‚ùå VACIO"
        }
        
        # 3. EVENTBRITE MASIVO
        logger.info("üé´ DEBUG - Probando Eventbrite Masivo...")
        try:
            start_time = time.time()
            # eventbrite_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
            eventbrite_events = await eventbrite_scraper.fetch_events_by_location(location)
            eventbrite_time = time.time() - start_time
            debug_info["fuentes_debug"]["eventbrite_masivo"] = {
                "eventos_devueltos": len(eventbrite_events),
                "tiempo_segundos": round(eventbrite_time, 2),
                "primeros_3_titulos": [e.get("title", "Sin t√≠tulo") for e in eventbrite_events[:3]],
                "status": "‚úÖ OK" if eventbrite_events else "‚ùå VACIO"
            }
        except Exception as e:
            debug_info["fuentes_debug"]["eventbrite_masivo"] = {
                "eventos_devueltos": 0,
                "error": str(e),
                "status": "‚ùå ERROR"
            }
            debug_info["problemas_encontrados"].append(f"Eventbrite Error: {str(e)}")
        
        # 4. PROGRESSIVE SYNC (EL CACHE)
        logger.info("‚ö° DEBUG - Probando Progressive Sync...")
        try:
            from services.progressive_sync_scraper import ProgressiveSyncScraper
            start_time = time.time()
            progressive_scraper = ProgressiveSyncScraper()
            progressive_events = await progressive_scraper.get_cached_events()
            progressive_time = time.time() - start_time
            debug_info["fuentes_debug"]["progressive_sync_cache"] = {
                "eventos_devueltos": len(progressive_events),
                "tiempo_segundos": round(progressive_time, 2),
                "primeros_3_titulos": [e.get("title", "Sin t√≠tulo") for e in progressive_events[:3]],
                "status": "‚úÖ OK" if progressive_events else "‚ùå VACIO"
            }
        except Exception as e:
            debug_info["fuentes_debug"]["progressive_sync_cache"] = {
                "eventos_devueltos": 0,
                "error": str(e),
                "status": "‚ùå ERROR"
            }
            debug_info["problemas_encontrados"].append(f"Progressive Sync Error: {str(e)}")
        
        # RESUMEN TOTAL
        total_eventos = sum([
            debug_info["fuentes_debug"].get("oficial_venues", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("argentina_venues", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("eventbrite_masivo", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("progressive_sync_cache", {}).get("eventos_devueltos", 0)
        ])
        debug_info["total_final"] = total_eventos
        
        # AN√ÅLISIS AUTOM√ÅTICO
        if total_eventos < 10:
            debug_info["problemas_encontrados"].append("‚ö†Ô∏è MUY POCOS EVENTOS - Alguna fuente no est√° funcionando bien")
            
        if not debug_info["fuentes_debug"].get("eventbrite_masivo", {}).get("eventos_devueltos", 0):
            debug_info["problemas_encontrados"].append("‚ö†Ô∏è EVENTBRITE NO DEVUELVE EVENTOS - Esta es la fuente principal")
            
        logger.info(f"üîç DEBUG COMPLETADO: {total_eventos} eventos total de {len(debug_info['fuentes_debug'])} fuentes")
        
        return debug_info
        
    except Exception as e:
        logger.error(f"‚ùå ERROR en debug: {e}")
        debug_info["error_general"] = str(e)
        return debug_info

# Import routers
# Multi-source router no se usa m√°s - solo EventOrchestrator
# try:
#     from api.multi_source import router as multi_router
#     app.include_router(multi_router)
#     logger.info("‚úÖ Multi-source router loaded")
# except Exception as e:
#     logger.error(f"‚ùå Failed to load multi-source router: {e}")

try:
    from api.geolocation import router as geo_router
    app.include_router(geo_router, prefix="/api/location")
    logger.info("‚úÖ Geolocation router loaded")
except Exception as e:
    logger.error(f"‚ùå Failed to load geolocation router: {e}")

try:
    from api.event_ai_hover import router as ai_hover_router
    app.include_router(ai_hover_router)
    logger.info("‚úÖ AI Hover router loaded")
except Exception as e:
    logger.warning(f"Could not load AI hover router: {e}")

# SERVICIOS DE CHAT RECREADOS - Intentar cargar AI router
try:
    from api.ai import router as ai_router
    app.include_router(ai_router, prefix="/api/ai")
    
    # Global Router comentado - archivo faltante
    # from api.global_router import router as global_router_api
    # app.include_router(global_router_api)
    logger.info("‚úÖ AI Gemini router loaded - servicios recreados")
except Exception as e:
    logger.warning(f"Could not load AI Gemini router: {e}")
    logger.info("‚ÑπÔ∏è AI Gemini router temporalmente deshabilitado")

# API V1 con SSE Streaming
try:
    from api.v1 import router as v1_router
    app.include_router(v1_router)
    logger.info("‚úÖ API V1 with SSE streaming loaded")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not load API V1 router: {e}")

# External Events Router (BrightData, Claude Desktop, etc.)
try:
    from api.external_events import router as external_router
    app.include_router(external_router)
    logger.info("‚úÖ External Events router loaded (BrightData/Claude Desktop)")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not load External Events router: {e}")

# Events DB Router (Fast queries from PostgreSQL)
try:
    from api.events_db import router as events_db_router
    app.include_router(events_db_router)
    logger.info("‚úÖ Events DB router loaded (Fast PostgreSQL queries)")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Could not load Events DB router: {e}")

# ============================================================================
# üöÄ PARALLEL REST ENDPOINTS - Individual sources for maximum performance
# ============================================================================

@app.get("/api/sources/eventbrite")
async def get_eventbrite_events(location: str = Query(..., description="Location required")):
    """Eventbrite Argentina - Endpoint paralelo"""
    start_time = time.time()
    try:
        # from services.eventbrite_massive_scraper import EventbriteMassiveScraper  # MOVED TO LEGACY
        # scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
        
        events = await scraper.massive_scraping(max_urls=6)
        normalized = scraper.normalize_events(events)
        
        end_time = time.time()
        
        return {
            "source": "eventbrite",
            "status": "success", 
            "events": normalized,
            "count": len(normalized),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Eventbrite parallel error: {e}")
        return {
            "source": "eventbrite",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/argentina-venues")
async def get_argentina_venues_events(location: str = Query(..., description="Location required")):
    """Argentina Venues - Endpoint paralelo"""
    start_time = time.time()
    try:
        # from services.argentina_venues_scraper import ArgentinaVenuesScraper  # DELETED - FAKE DATA GENERATOR
        # scraper = ArgentinaVenuesScraper()  # DELETED - FAKE DATA GENERATOR
        # events = await scraper.scrape_all_sources()  # DELETED - FAKE DATA GENERATOR
        events = []  # No fake data - return empty array
        
        end_time = time.time()
        
        return {
            "source": "argentina_venues",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Argentina venues parallel error: {e}")
        return {
            "source": "argentina_venues", 
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/facebook")
async def get_facebook_events(location: str = Query(..., description="Location required")):
    """Facebook Events - Cache diario (una llamada por d√≠a)"""
    start_time = time.time()
    try:
        from services.daily_cache import daily_cache
        
        # Asegurar que cache est√© fresco
        await daily_cache.ensure_cache_is_ready()
        
        # Obtener eventos desde cache (no scraper directo)
        events = await daily_cache.get_cached_events("facebook")
        
        end_time = time.time()
        
        return {
            "source": "facebook",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Facebook parallel error: {e}")
        return {
            "source": "facebook",
            "status": "error", 
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/instagram") 
async def get_instagram_events(location: str = Query(..., description="Location required")):
    """Instagram Events - Endpoint paralelo"""
    start_time = time.time()
    try:
        from services.daily_cache import daily_cache
        
        # Asegurar que cache est√© fresco
        await daily_cache.ensure_cache_is_ready()
        
        # Obtener eventos desde cache (no scraper directo)
        events = await daily_cache.get_cached_events("instagram")
        
        end_time = time.time()
        
        return {
            "source": "instagram",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Instagram parallel error: {e}")
        return {
            "source": "instagram",
            "status": "error",
            "events": [],
            "count": 0, 
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/meetup")
async def get_meetup_events(location: str = Query(..., description="Location required")):
    """Meetup Events - Endpoint paralelo"""
    start_time = time.time()
    try:
        # Placeholder - implementar scraper de Meetup
        await asyncio.sleep(0.1)  # Simular trabajo
        
        end_time = time.time()
        
        return {
            "source": "meetup",
            "status": "success",
            "events": [],
            "count": 0,
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        return {
            "source": "meetup",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/ticketmaster")
async def get_ticketmaster_events(location: str = Query(..., description="Location required")):
    """Ticketmaster Events - Endpoint paralelo"""
    start_time = time.time()
    try:
        # Placeholder - implementar scraper de Ticketmaster
        await asyncio.sleep(0.1)  # Simular trabajo
        
        end_time = time.time()
        
        return {
            "source": "ticketmaster", 
            "status": "success",
            "events": [],
            "count": 0,
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        return {
            "source": "ticketmaster",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

# ============================================================================
# üéØ PARALLEL ORCHESTRATOR - Calls all sources simultaneously  
# ============================================================================

@app.get("/api/parallel/search")
async def parallel_search(location: str = Query(..., description="Location required")):
    """
    üöÄ PARALLEL SEARCH - All sources run simultaneously
    
    This is MUCH faster than WebSocket because:
    - All APIs run in parallel (not sequential)  
    - Results return as soon as each source completes
    - No single point of failure
    - Better resource utilization
    """
    start_time = time.time()
    
    # Define all source endpoints  
    base_url = BACKEND_URL
    sources = [
        f"{base_url}/api/sources/eventbrite?location={location}",
        f"{base_url}/api/sources/argentina-venues?location={location}",
        f"{base_url}/api/sources/facebook?location={location}",
        f"{base_url}/api/sources/instagram?location={location}",
        f"{base_url}/api/sources/meetup?location={location}",
        f"{base_url}/api/sources/ticketmaster?location={location}"
    ]
    
    all_events = []
    source_results = []
    
    async with aiohttp.ClientSession() as session:
        # Create all tasks simultaneously
        tasks = []
        for url in sources:
            task = asyncio.create_task(fetch_source_parallel(session, url))
            tasks.append(task)
        
        # Wait for all to complete (or timeout)
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for result in results:
            if isinstance(result, dict) and not isinstance(result, Exception):
                source_results.append(result)
                if result.get("events"):
                    all_events.extend(result["events"])
    
    end_time = time.time()
    total_duration = round((end_time - start_time) * 1000)
    
    # Calculate performance stats
    performance_stats = {
        "total_events": len(all_events),
        "total_duration_ms": total_duration,
        "sources_completed": len([r for r in source_results if r.get("status") == "success"]),
        "sources_failed": len([r for r in source_results if r.get("status") == "error"]),
        "fastest_source": None,
        "slowest_source": None,
        "source_breakdown": source_results
    }
    
    # Find fastest and slowest sources
    successful_sources = [r for r in source_results if r.get("status") == "success" and r.get("count", 0) > 0]
    if successful_sources:
        fastest = min(successful_sources, key=lambda x: x.get("timing", {}).get("duration_ms", 0))
        slowest = max(successful_sources, key=lambda x: x.get("timing", {}).get("duration_ms", 0))
        
        performance_stats["fastest_source"] = {
            "source": fastest.get("source"),
            "duration_ms": fastest.get("timing", {}).get("duration_ms"),
            "events": fastest.get("count")
        }
        performance_stats["slowest_source"] = {
            "source": slowest.get("source"), 
            "duration_ms": slowest.get("timing", {}).get("duration_ms"),
            "events": slowest.get("count")
        }
    
    return {
        "status": "success",
        "location": location,
        "events": all_events,
        "performance": performance_stats
    }

async def fetch_source_parallel(session, url):
    """Helper function to fetch from individual source"""
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status == 200:
                return await response.json()
            else:
                return {
                    "status": "error",
                    "error": f"HTTP {response.status}",
                    "events": [],
                    "count": 0
                }
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e),
            "events": [],
            "count": 0
        }

# Simple test endpoint
@app.get("/test")
async def test_endpoint():
    return {"status": "ok", "test": "working"}

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "Eventos Visualizer API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "endpoints": [
            "/api/events",
            "/api/events/search",
            "/api/events/categories",
            "/api/smart/search",
            "/api/geolocation/auto",
            "/ws/notifications"
        ]
    }

# Events endpoints

@app.get("/api/events/initial")
async def get_initial_events():
    """
    Endpoint especial para CARGA INICIAL (document.ready)
    Solo este endpoint puede usar Buenos Aires como fallback
    """
    return await get_events_internal(location="Buenos Aires")

async def get_events_internal(
    location: str,
    category: Optional[str] = None,
    limit: int = 20,
    offset: int = 0
):
    """Funci√≥n interna para obtener eventos sin validaci√≥n de ubicaci√≥n"""
    logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    logger.info(f"‚îÇ üöÄ EJECUTANDO: get_events_internal | location={location} | category={category} | limit={limit}")
    logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
    
    try:
        # üöÄ USE EVENT ORCHESTRATOR (reemplaza multi_source)
        from services.EventOrchestrator import EventOrchestrator
        orchestrator = EventOrchestrator()
        result = await orchestrator.get_events_comprehensive(location=location, category=category, limit=limit)
        
        # Extraer eventos del resultado del orchestrator
        events = result.get("events", [])
        
        # üñºÔ∏è MEJORAR IM√ÅGENES DE EVENTOS
        improved_count = 0
        try:
            from services.global_image_service import improve_events_images
            if events:
                image_start_time = time.time()
                improved_events = await improve_events_images(events)
                events = improved_events
                
                image_end_time = time.time()
                image_duration = image_end_time - image_start_time
                
                # Contar im√°genes mejoradas
                improved_count = sum(1 for e in improved_events if e.get('image_improved', False))
                logger.info(f"üñºÔ∏è IMAGE SERVICE: {improved_count}/{len(events)} im√°genes mejoradas en {image_duration:.2f}s")
        except Exception as img_error:
            logger.warning(f"‚ö†Ô∏è Error mejorando im√°genes: {img_error}")
        
        logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.info(f"‚îÇ ‚úÖ √âXITO: get_events_internal | total_eventos={len(events)} | im√°genes_mejoradas={improved_count}")
        logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        return {
            "status": "success",
            "location": location,
            "category": category,
            "total": len(events),
            "events": events,
            "images_improved": improved_count
        }
    except Exception as e:
        logger.error("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.error(f"‚îÇ ‚ùå ERROR: get_events_internal | {str(e)}")
        logger.error("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        return {
            "status": "error",
            "location": location,
            "category": category,  
            "total": 0,
            "events": [],
            "error": str(e)
        }

# Cache global para evitar enriquecer la misma ubicaci√≥n m√∫ltiples veces
_last_enriched_location = None

@app.get("/api/events/stream")
async def stream_events(
    location: Optional[str] = Query(None, description="Ubicaci√≥n requerida"),
    category: Optional[str] = Query(None),
    limit: int = Query(100)
):
    """
    üöÄ SSE STREAMING ENDPOINT - Devuelve eventos en tiempo real

    Usa async generators para devolver resultados apenas est√©n listos
    NO espera a que terminen todos los scrapers
    """
    from fastapi.responses import StreamingResponse
    import json

    async def event_generator():
        """Generator para SSE - Consulta MySQL en vez de Gemini"""
        import time
        try:
            if not location:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Ubicaci√≥n requerida'})}\n\n"
                return

            # Enviar evento de inicio
            yield f"data: {json.dumps({'type': 'start', 'message': f'Buscando eventos en {location}', 'location': location})}\n\n"

            # üóÑÔ∏è BUSCAR EN MYSQL EN VEZ DE GEMINI
            from services.events_db_service import search_events_by_location

            start_time = time.time()
            logger.info(f"üîç Consultando MySQL para ubicaci√≥n: {location}")
            logger.info(f"üîç Par√°metros: category={category}, limit={limit}, days_ahead=180")

            # Consultar base de datos (180 d√≠as = 6 meses hacia adelante)
            try:
                result = await search_events_by_location(
                    location=location,
                    category=category,
                    limit=limit,
                    days_ahead=180
                )
                # Extraer eventos del resultado (es un dict con 'events', 'parent_city_detected', etc.)
                events = result.get('events', []) if isinstance(result, dict) else result
                parent_city_detected = result.get('parent_city_detected') if isinstance(result, dict) else None
                original_location = result.get('original_location') if isinstance(result, dict) else location
                expanded_search = result.get('expanded_search', False) if isinstance(result, dict) else False

                logger.info(f"‚úÖ search_events_by_location devolvi√≥ {len(events)} eventos")
                logger.info(f"üîç DEBUG - parent_city: {parent_city_detected}, expanded: {expanded_search}, result type: {type(result)}")
            except Exception as search_err:
                logger.error(f"‚ùå Error en search_events_by_location: {search_err}")
                events = []
                parent_city_detected = None
                original_location = location
                expanded_search = False

            execution_time = f"{time.time() - start_time:.2f}s"
            total_events = len(events)
            logger.info(f"üìä Total eventos despu√©s de b√∫squeda: {total_events} en {execution_time}")

            if total_events > 0:
                # Enviar eventos desde base de datos con metadata de b√∫squeda expandida
                event_data = {
                    'type': 'events',
                    'scraper': 'mysql_database',
                    'events': events,  # ‚Üê Ahora es el array correcto
                    'count': total_events,
                    'total_events': total_events,
                    'execution_time': execution_time
                }

                # Agregar metadata de b√∫squeda expandida si existe
                if parent_city_detected:
                    event_data['parent_city'] = parent_city_detected
                    event_data['original_location'] = original_location
                    event_data['expanded_search'] = expanded_search

                yield f"data: {json.dumps(event_data)}\n\n"
                logger.info(f"üì° SSE: MySQL - {total_events} eventos enviados en {execution_time}")
            else:
                # ‚ùå NO HAY EVENTOS - NO buscar en ciudades cercanas (solo mostrar mensaje)
                logger.info(f"‚ùå No hay eventos en {location}")
                yield f"data: {json.dumps({'type': 'no_events', 'scraper': 'mysql_database', 'count': 0, 'message': f'No hay eventos disponibles para {location}'})}\n\n"

                # üîí B√öSQUEDA AUTOM√ÅTICA EN CIUDADES CERCANAS DESHABILITADA
                # El usuario debe buscar manualmente en otras ciudades
                # (El frontend mostrar√° shake animation en el search bar)

            # ‚úÖ Enviar evento de completado INMEDIATAMENTE (eventos ya enviados)
            yield f"data: {json.dumps({'type': 'complete', 'total_events': total_events, 'scrapers_completed': 1})}\n\n"
            logger.info(f"üèÅ SSE: Streaming completado - {total_events} eventos totales")

            # üö´ ENRIQUECIMIENTO DESACTIVADO
            # NO buscar ciudades cercanas autom√°ticamente
            # El usuario debe buscar manualmente si quiere ver otras ciudades

        except Exception as e:
            logger.error(f"‚ùå Error en SSE streaming desde MySQL: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    # üî• RETORNAR EL STREAMING RESPONSE
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/api/location/enrichment")
async def get_location_enrichment(
    location: str = Query(..., description="Ubicaci√≥n original del usuario"),
    
    force_refresh: bool = Query(False, description="Forzar nuevo enriquecimiento ignorando cach√©")
):
    """
    üåç ENRIQUECIMIENTO DE UBICACI√ìN - Solo obtiene info sin buscar eventos

    Retorna:vscode-webview://1d0l9k4dlisibkf6diqq2no75k7u891dcvs5vm2g0kvv31dus3kn/backend/services/gemini_factory.py#L221-L298
        - city: Ciudad
        - state: Provincia/Estado
        - country: Pa√≠s
        - nearby_cities: Array de 3 ciudades cercanas
        - needs_expansion: Si necesita expansi√≥n
    """
    try:
        from services.gemini_factory import gemini_factory
        import json
        import os

        factory = gemini_factory  # Singleton - no need to instantiate

        # Si force_refresh, limpiar cach√© de esta ubicaci√≥n espec√≠fica
        if force_refresh:
            # 1. Limpiar cach√© en memoria del factory
            keys_to_remove_memory = [key for key in factory._location_cache.keys() if location.lower() in key.lower()]
            for key in keys_to_remove_memory:
                del factory._location_cache[key]
                logger.info(f"üóëÔ∏è Memory cache cleared: {key}")

            # 2. Limpiar cach√© en archivo JSON
            cache_path = os.path.join(os.path.dirname(__file__), 'data', 'location_enrichments_cache.json')
            try:
                if os.path.exists(cache_path):
                    with open(cache_path, 'r', encoding='utf-8') as f:
                        cache_data = json.load(f)

                    # Eliminar todas las entradas que contengan esta ubicaci√≥n
                    enrichments = cache_data.get('enrichments', {})
                    keys_to_remove = [key for key in enrichments.keys() if location.lower() in key.lower()]

                    for key in keys_to_remove:
                        del enrichments[key]
                        logger.info(f"üóëÔ∏è Cache entry removed: {key}")

                    cache_data['metadata']['total_enrichments'] = len(enrichments)

                    with open(cache_path, 'w', encoding='utf-8') as f:
                        json.dump(cache_data, f, indent=2, ensure_ascii=False)

                    logger.info(f"‚úÖ Cache cleared for: {location}")
            except Exception as cache_err:
                logger.warning(f"‚ö†Ô∏è Error clearing cache: {cache_err}")

        # Enriquecer ubicaci√≥n
        enriched = await factory._enrich_location_once(location)

        return {
            "success": True,
            "location_info": enriched
        }

    except Exception as e:
        logger.error(f"‚ùå Error enriqueciendo ubicaci√≥n: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/cities/available")
async def get_available_cities(
    q: str = Query(..., min_length=2, description="Search query for location (city, province, country)"),
    limit: int = Query(10, description="Maximum number of locations to return")
):
    """
    üåç UBICACIONES DISPONIBLES - Retorna ciudades, provincias y pa√≠ses con eventos

    Autocomplete optimizado que busca en m√∫ltiples niveles geogr√°ficos:
    - üèôÔ∏è Ciudades (prioridad 1)
    - üìç Provincias/Estados (prioridad 2)
    - üåç Pa√≠ses (prioridad 3)

    Args:
        q: B√∫squeda (m√≠nimo 2 caracteres)
        limit: M√°ximo de resultados (default: 10)

    Returns:
        - locations: Lista de ubicaciones con eventos (ciudades, provincias, pa√≠ses)
        - total: Total de ubicaciones encontradas
    """
    try:
        from services.events_db_service import get_available_cities_with_events
        from services.gemini_factory import gemini_factory

        logger.info(f"üîç Buscando ubicaciones (ciudades/provincias/pa√≠ses) con eventos para: '{q}'")

        # Buscar ubicaciones en MySQL que coincidan y tengan eventos
        locations = await get_available_cities_with_events(search_query=q, limit=limit)

        logger.info(f"‚úÖ Encontradas {len(locations)} ubicaciones con eventos")

        # üî• SI NO HAY RESULTADOS, intentar detectar ciudad principal con Gemini
        if len(locations) == 0 and len(q) >= 3:
            logger.info(f"ü§ñ No hay resultados directos para '{q}', intentando detectar ciudad principal...")

            try:
                parent_city = await gemini_factory.get_parent_location(q)

                if parent_city:
                    logger.info(f"‚úÖ Gemini detect√≥: '{q}' es parte de '{parent_city}'")

                    # Buscar eventos de la ciudad principal
                    parent_locations = await get_available_cities_with_events(search_query=parent_city, limit=1)

                    if parent_locations:
                        # Agregar como sugerencia con indicador especial
                        # Usar el mismo formato que get_available_cities_with_events
                        suggestion = {
                            "location": parent_city,
                            "location_type": "city",
                            "event_count": parent_locations[0].get("event_count", 0),
                            "displayName": f"{parent_city} (cerca de {q.title()})",
                            "is_suggestion": True,
                            "original_query": q
                        }
                        locations = [suggestion]
                        logger.info(f"üí° Sugiriendo '{parent_city}' como alternativa")
            except Exception as gemini_err:
                logger.warning(f"‚ö†Ô∏è Error en detecci√≥n con Gemini: {gemini_err}")

        return {
            "success": True,
            "locations": locations,
            "total": len(locations)
        }

    except Exception as e:
        logger.error(f"‚ùå Error buscando ubicaciones disponibles: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/neighborhoods/{city}")
async def get_neighborhoods_by_city(city: str):
    """
    üèòÔ∏è BARRIOS POR CIUDAD - Retorna barrios con conteo de eventos

    Args:
        city: Nombre de la ciudad (ej: "Buenos Aires")

    Returns:
        - city: Ciudad consultada
        - neighborhoods: Lista de barrios con event_count
        - total_neighborhoods: Total de barrios con eventos
        - total_events: Total de eventos en todos los barrios
    """
    try:
        import pymysql

        logger.info(f"üèòÔ∏è Obteniendo barrios de {city}")

        # Conectar a MySQL
        connection = pymysql.connect(
            host=os.getenv('MYSQL_HOST', 'localhost'),
            port=int(os.getenv('MYSQL_PORT', 3306)),
            user=os.getenv('MYSQL_USER', 'root'),
            password=os.getenv('MYSQL_PASSWORD', ''),
            database=os.getenv('MYSQL_DATABASE', 'events'),
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        cursor = connection.cursor()

        # Query para obtener barrios con conteo
        query = '''
            SELECT neighborhood, COUNT(*) as event_count
            FROM events
            WHERE city = %s
            AND neighborhood IS NOT NULL
            AND neighborhood != ''
            GROUP BY neighborhood
            ORDER BY event_count DESC, neighborhood ASC
        '''

        cursor.execute(query, (city,))
        results = cursor.fetchall()

        cursor.close()
        connection.close()

        # Formatear respuesta
        neighborhoods = [
            {
                "name": row['neighborhood'],
                "event_count": row['event_count']
            }
            for row in results
        ]

        total_events = sum(n['event_count'] for n in neighborhoods)

        logger.info(f"‚úÖ Encontrados {len(neighborhoods)} barrios con {total_events} eventos")

        return {
            "success": True,
            "city": city,
            "neighborhoods": neighborhoods,
            "total_neighborhoods": len(neighborhoods),
            "total_events": total_events
        }

    except Exception as e:
        logger.error(f"‚ùå Error obteniendo barrios: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/popular-places/{location}")
async def get_popular_places_nearby(location: str):
    """
    üéØ LUGARES POPULARES CERCANOS - Barrios/zonas cerca de la ubicaci√≥n que tienen eventos

    Flujo:
    1. Grok sugiere 5 lugares geogr√°ficamente cercanos
    2. Verificamos cu√°les tienen eventos en MySQL
    3. Devolvemos solo los que tienen eventos

    Args:
        location: Ubicaci√≥n de referencia (ej: "Palermo", "Buenos Aires", "Mendoza")

    Returns:
        - places: Lista con lugares cercanos que tienen eventos
    """
    try:
        import pymysql
        from services.ai_manager import AIServiceManager

        logger.info(f"üéØ Buscando lugares populares cerca de: {location}")

        # PASO 1: Pedirle a Grok que sugiera 5 barrios/lugares cercanos
        ai_manager = AIServiceManager()

        prompt = f"""Listame 5 ciudades populares cercanas a {location}.

Responde SOLO con los 5 nombres separados por comas, sin n√∫meros ni explicaciones:
nombre1, nombre2, nombre3, nombre4, nombre5
"""

        logger.info(f"ü§ñ Preguntando a AI por lugares cerca de '{location}'...")
        response = await ai_manager.generate(
            prompt=prompt,
            temperature=0.3
        )

        # Parsear respuesta de Grok - obtener 5 sugerencias
        suggested_places = [name.strip() for name in response.split(',')[:5]]
        logger.info(f"‚úÖ Grok sugiri√≥ 5 lugares cercanos: {suggested_places}")

        # Verificar cu√°les de estos lugares tienen eventos en la base de datos
        verified_places = []

        # Reusar conexi√≥n para verificaci√≥n
        verify_connection = pymysql.connect(
            host=os.getenv('MYSQL_HOST', 'localhost'),
            port=int(os.getenv('MYSQL_PORT', 3306)),
            user=os.getenv('MYSQL_USER', 'root'),
            password=os.getenv('MYSQL_PASSWORD', ''),
            database=os.getenv('MYSQL_DATABASE', 'events'),
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        try:
            verify_cursor = verify_connection.cursor()

            for place_name in suggested_places:
                # Query para verificar si tiene eventos
                verify_query = '''
                    SELECT COUNT(*) as event_count
                    FROM events
                    WHERE start_datetime >= NOW()
                    AND (
                        city LIKE %s
                        OR neighborhood LIKE %s
                    )
                '''

                verify_cursor.execute(verify_query, (f'%{place_name}%', f'%{place_name}%'))
                result = verify_cursor.fetchone()

                if result and result['event_count'] > 0:
                    verified_places.append(place_name)
                    logger.info(f"‚úÖ '{place_name}' tiene {result['event_count']} eventos")
                else:
                    logger.info(f"‚è≠Ô∏è '{place_name}' sin eventos en DB")

            verify_cursor.close()
        finally:
            verify_connection.close()

        top_places = verified_places
        logger.info(f"‚úÖ Lugares verificados con eventos: {top_places}")

        return {
            "success": True,
            "places": top_places
        }

    except Exception as e:
        logger.error(f"‚ùå Error obteniendo lugares populares: {e}")
        # En caso de error, retornar vac√≠o
        return {
            "success": False,
            "places": []
        }


@app.get("/api/events/city")
async def get_city_events(
    city: str = Query(..., description="Ciudad donde buscar eventos"),
    original_location: Optional[str] = Query(None, description="Ubicaci√≥n original del usuario"),
    category: Optional[str] = Query(None),
    limit: int = Query(20)
):
    """
    üèôÔ∏è EVENTOS EN CIUDAD ESPEC√çFICA - Busca en una ciudad espec√≠fica (solo MySQL)

    Args:
        city: Ciudad donde buscar (ej: "Merlo", "Mor√≥n", etc.)
        original_location: Ubicaci√≥n original del usuario (ej: "Paso del Rey")

    Returns:
        - events: Lista de eventos
        - city: Ciudad donde se buscaron eventos
        - original_location: Ubicaci√≥n original
    """
    try:
        from services.events_db_service import search_events_by_location
        import time

        # Buscar eventos en la ciudad espec√≠fica
        logger.info(f"üèôÔ∏è Buscando eventos en: {city}" + (f" (desde {original_location})" if original_location else ""))

        start_time = time.time()

        # üóÑÔ∏è BUSCAR EN MYSQL (UN SOLO LLAMADO)
        events = await search_events_by_location(
            location=city,
            category=category,
            limit=limit,
            days_ahead=180
        )

        execution_time = f"{time.time() - start_time:.2f}s"

        # Agregar metadata a cada evento
        for event in events:
            event['search_city'] = city
            if original_location:
                event['original_location'] = original_location
                event['distance_note'] = f"Evento en {city}"

        logger.info(f"‚úÖ Encontrados {len(events)} eventos en {city} en {execution_time}")

        return {
            "events": events,
            "city": city,
            "original_location": original_location,
            "total_events": len(events),
            "message": f"Se encontraron {len(events)} eventos en {city}",
            "execution_time": execution_time,
            "source": "mysql_database"
        }

    except Exception as e:
        logger.error(f"‚ùå Error buscando eventos en ciudad: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# üíæ CACHE GLOBAL DE CATEGOR√çAS (evitar llamadas a IA repetidas)
_categories_cache = {}
_categories_cache_ttl = {}
import time

@app.get("/api/events/categories")
async def get_event_categories(
    location: Optional[str] = Query(None, description="Ubicaci√≥n para filtrar categor√≠as")
):
    """üè∑Ô∏è CATEGOR√çAS DIN√ÅMICAS - Obtiene categor√≠as √∫nicas de eventos (CON CACHE)"""
    try:
        cache_key = location or "Buenos Aires"
        current_time = time.time()

        # ‚úÖ Revisar cach√© (v√°lido por 5 minutos)
        if cache_key in _categories_cache:
            if current_time - _categories_cache_ttl.get(cache_key, 0) < 300:  # 5 minutos
                logger.info(f"‚úÖ Categor√≠as CACHEADAS para {cache_key}")
                return _categories_cache[cache_key]

        logger.info(f"üîÑ Generando categor√≠as NUEVAS para {cache_key}...")

        # Consulta DIRECTA a MySQL sin IA (mucho m√°s r√°pido)
        import os
        import pymysql
        from datetime import datetime, timedelta

        connection = pymysql.connect(
            host=os.getenv('MYSQL_HOST'),
            port=int(os.getenv('MYSQL_PORT', '3306')),
            database=os.getenv('MYSQL_DATABASE'),
            user=os.getenv('MYSQL_USER'),
            password=os.getenv('MYSQL_PASSWORD'),
            charset='utf8mb4'
        )

        cursor = connection.cursor()

        # Query r√°pida para categor√≠as
        future_date = datetime.now() + timedelta(days=180)
        cursor.execute("""
            SELECT DISTINCT category, COUNT(*) as count
            FROM events
            WHERE city LIKE %s
            AND (start_datetime >= NOW() OR start_datetime IS NULL)
            AND start_datetime <= %s
            GROUP BY category
            ORDER BY count DESC
        """, (f"%{cache_key}%", future_date))

        results = cursor.fetchall()
        cursor.close()
        connection.close()

        # Mapeo de categor√≠as para normalizar
        category_mapping = {
            # M√∫sica
            'music': 'M√∫sica',
            'musica': 'M√∫sica',
            'm√∫sica': 'M√∫sica',
            # Deportes
            'sports': 'Deportes',
            'deportes': 'Deportes',
            # Cultural
            'cultural': 'Cultural',
            # Tech
            'tech': 'Tech',
            'technology': 'Tech',
            'tecnologia': 'Tech',
            'tecnolog√≠a': 'Tech',
            # Fiestas
            'party': 'Fiestas',
            'fiestas': 'Fiestas',
            'nightlife': 'Fiestas',
            # Festival
            'festival': 'Festival',
            # Teatro
            'theater': 'Teatro',
            'theatre': 'Teatro',
            'teatro': 'Teatro',
            # Comedy
            'comedy': 'Comedia',
            'comedia': 'Comedia',
            # General/Other
            'general': 'General',
            'other': 'General'
        }

        # Procesar categor√≠as con sus counts reales de la DB
        category_counts = {}
        for row in results:
            category = row[0]
            count = row[1]  # ‚úÖ Usar el count real de la DB, no contar como 1

            if category and category.strip():
                # Normalizar: lowercase y sin acentos
                normalized = category.lower().strip()
                # Usar mapeo si existe, sino capitalizar la original
                display_name = category_mapping.get(normalized, category.capitalize())
                # Sumar counts (por si m√∫ltiples categor√≠as mapean a la misma)
                category_counts[display_name] = category_counts.get(display_name, 0) + count

        # Formatear resultado
        categories = [
            {'name': cat, 'count': count}
            for cat, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True)
        ]

        result = {'categories': categories, 'total': len(categories)}

        # üíæ Guardar en cach√© para pr√≥ximas requests (TTL 5 minutos)
        _categories_cache[cache_key] = result
        _categories_cache_ttl[cache_key] = current_time

        logger.info(f"‚úÖ {len(categories)} categor√≠as" + (f" para {location}" if location else "") + " (guardadas en cach√©)")

        return result

    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/events/nearby")
async def get_nearby_events(
    location: str = Query(..., description="Ubicaci√≥n para detectar ciudades cercanas")
):
    """
    üìç CIUDADES CERCANAS - Usa IA para detectar ciudades cercanas

    Este endpoint NO busca eventos, solo retorna nombres de ciudades cercanas
    para que el frontend muestre botones. Cuando el usuario presiona un bot√≥n,
    el frontend llama a /api/events/stream con esa ciudad para buscar en MySQL.

    Args:
        location: Ubicaci√≥n original (ej: "Villa Gesell", "Moreno")

    Returns:
        - nearby_cities: Lista de nombres de ciudades cercanas (para los botones)
        - original_location: Ubicaci√≥n original
    """
    try:
        from services.gemini_factory import gemini_factory

        # Extraer ciudad de location (puede venir como "Villa Gesell, Argentina")
        city = location.split(',')[0].strip()

        logger.info(f"üìç Detectando ciudades cercanas a: {city}")

        # Usar Gemini para enriquecer ubicaci√≥n y obtener ciudades cercanas
        enriched = await gemini_factory._enrich_location_once(city)

        nearby_cities = enriched.get('nearby_cities', [])

        logger.info(f"‚úÖ Encontradas {len(nearby_cities)} ciudades cercanas: {nearby_cities}")

        return {
            "success": True,
            "nearby_cities": nearby_cities[:3],  # Solo 3 para los botones
            "original_location": city,
            "message": f"Encontradas {len(nearby_cities[:3])} ciudades cercanas a {city}" if nearby_cities else f"No se encontraron ciudades cercanas a {city}"
        }

    except Exception as e:
        logger.error(f"‚ùå Error detectando ciudades cercanas: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/events/province")
async def get_province_events(
    location: str = Query(..., description="Ubicaci√≥n original del usuario"),
    category: Optional[str] = Query(None),
    limit: int = Query(20)
):
    """
    üåç EVENTOS EN LA PROVINCIA/ESTADO - Busca en toda la provincia

    Para localidades chicas como "Paso del Rey", busca eventos
    en toda la provincia (ej: "Buenos Aires")

    Returns:
        - events: Lista de eventos de la provincia
        - location_info: Informaci√≥n de enriquecimiento
        - province: Provincia donde se buscaron eventos
    """
    try:
        from services.gemini_factory import gemini_factory

        factory = gemini_factory  # Singleton - no need to instantiate

        # Enriquecer ubicaci√≥n UNA VEZ
        enriched = await factory._enrich_location_once(location)

        # Obtener la provincia
        province = enriched.get('state', '')

        if not province:
            return {
                "events": [],
                "location_info": enriched,
                "province": None,
                "message": f"No se pudo detectar la provincia para '{location}'"
            }

        # Buscar eventos en la provincia
        logger.info(f"üåç Buscando eventos en provincia: {location} ‚Üí {province}")

        result = await factory.execute_global_scrapers_with_details(
            location=province,
            category=category,
            limit=limit
        )

        events = result.get('events', [])

        # Agregar metadata a cada evento indicando que es de la provincia
        for event in events:
            event['is_province'] = True
            event['original_location'] = location
            event['province_location'] = province
            event['distance_note'] = f"Evento en {province}"

        return {
            "events": events,
            "location_info": enriched,
            "province": province,
            "total_events": len(events),
            "message": f"Se encontraron {len(events)} eventos en {province}",
            "scrapers_execution": result.get('scrapers_execution', {})
        }

    except Exception as e:
        logger.error(f"‚ùå Error buscando eventos de provincia: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/events")
async def get_events(
    location: Optional[str] = Query(None, description="Ubicaci√≥n requerida"),
    category: Optional[str] = Query(None),
    limit: int = Query(20)
):
    """
    üöÄ SSE STREAMING ENDPOINT - Devuelve eventos de MySQL en tiempo real

    IMPORTANTE: Frontend usa EventSource, requiere formato SSE
    """
    from fastapi.responses import StreamingResponse
    import json
    import time

    async def event_generator():
        """Generator para SSE - Consulta MySQL directamente"""
        try:
            if not location:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Ubicaci√≥n requerida'})}\n\n"
                return

            # Enviar evento de inicio
            yield f"data: {json.dumps({'type': 'start', 'message': f'Buscando eventos en {location}', 'location': location})}\n\n"

            # üóÑÔ∏è BUSCAR EN MYSQL
            from services.events_db_service import search_events_by_location

            start_time = time.time()
            logger.info(f"üîç [/api/events] Consultando MySQL para ubicaci√≥n: {location}")

            # Consultar base de datos (180 d√≠as = 6 meses hacia adelante)
            search_result = await search_events_by_location(
                location=location,
                category=category,
                limit=limit,
                days_ahead=180
            )

            # Extraer eventos y metadata de parent_city
            events = search_result.get('events', []) if isinstance(search_result, dict) else search_result
            parent_city = search_result.get('parent_city_detected') if isinstance(search_result, dict) else None
            original_location = search_result.get('original_location', location) if isinstance(search_result, dict) else location
            expanded_search = search_result.get('expanded_search', False) if isinstance(search_result, dict) else False

            execution_time = f"{time.time() - start_time:.2f}s"
            total_events = len(events)

            if total_events > 0:
                # Preparar datos del evento
                event_data = {
                    'type': 'events',
                    'scraper': 'mysql_database',
                    'events': events,
                    'count': total_events,
                    'total_events': total_events,
                    'execution_time': execution_time
                }

                # Agregar metadata de ciudad principal si existe
                if parent_city and expanded_search:
                    event_data['parent_city'] = parent_city
                    event_data['original_location'] = original_location
                    event_data['expanded_search'] = expanded_search
                    logger.info(f"üìç [/api/events] B√∫squeda expandida: {original_location} ‚Üí {parent_city}")

                # Enviar eventos desde base de datos
                yield f"data: {json.dumps(event_data)}\n\n"
                logger.info(f"üì° [/api/events] MySQL - {total_events} eventos enviados en {execution_time}")
            else:
                # No se encontraron eventos
                no_events_data = {
                    'type': 'no_events',
                    'scraper': 'mysql_database',
                    'count': 0,
                    'execution_time': execution_time,
                    'message': f'No hay eventos disponibles para {location}'
                }

                # Agregar metadata de ciudad principal si existe (incluso sin eventos)
                if parent_city and expanded_search:
                    no_events_data['parent_city'] = parent_city
                    no_events_data['original_location'] = original_location
                    no_events_data['expanded_search'] = expanded_search
                    logger.info(f"üìç [/api/events] B√∫squeda expandida sin eventos: {original_location} ‚Üí {parent_city}")

                yield f"data: {json.dumps(no_events_data)}\n\n"
                logger.info(f"üì° [/api/events] MySQL - 0 eventos para '{location}' en {execution_time}")

            # Enviar evento de completado
            yield f"data: {json.dumps({'type': 'complete', 'total_events': total_events, 'scrapers_completed': 1})}\n\n"
            logger.info(f"üèÅ [/api/events] Streaming completado - {total_events} eventos desde MySQL")

        except Exception as e:
            logger.error(f"‚ùå [/api/events] Error en SSE streaming desde MySQL: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    # üî• RETORNAR EL STREAMING RESPONSE
    return StreamingResponse(event_generator(), media_type="text/event-stream")

@app.get("/api/events/categories")
async def get_event_categories(
    location: Optional[str] = Query("Buenos Aires", description="Ubicaci√≥n para filtrar categor√≠as")
):
    """
    üìä CATEGOR√çAS DE EVENTOS - Devuelve categor√≠as √∫nicas con conteo

    Args:
        location: Ciudad/ubicaci√≥n (default: "Buenos Aires")

    Returns:
        [{'name': 'music', 'count': 50}, {'name': 'sports', 'count': 30}, ...]
    """
    try:
        log_method_start("get_event_categories", location=location)

        # üóÑÔ∏è BUSCAR TODOS LOS EVENTOS DE LA UBICACI√ìN
        from services.events_db_service import search_events_by_location

        # Consultar con l√≠mite alto para obtener todas las categor√≠as
        logger.info(f"üîµ ANTES de llamar search_events_by_location")
        search_result = await search_events_by_location(
            location=location,
            limit=1000,  # Alto l√≠mite para capturar todas las categor√≠as
            days_ahead=180  # 6 meses hacia adelante
        )
        logger.info(f"üü¢ DESPU√âS de llamar search_events_by_location")

        # Debug: Ver qu√© tipo de resultado obtuvimos
        logger.info(f"üîç search_result type: {type(search_result)}")
        logger.info(f"üîç search_result value: {search_result}")

        # Extraer eventos (manejar diferentes tipos de respuesta)
        events = []
        if isinstance(search_result, dict):
            events = search_result.get('events', [])
        elif isinstance(search_result, list):
            events = search_result
        else:
            logger.error(f"‚ùå Tipo inesperado de search_result: {type(search_result)}")
            events = []

        logger.info(f"üìä Total events encontrados: {len(events)}")

        # üìä CALCULAR CATEGOR√çAS √öNICAS CON CONTEO
        category_counts = {}
        for event in events:
            if isinstance(event, dict):
                category = event.get('category')
            else:
                logger.warning(f"‚ö†Ô∏è Evento no es dict: {type(event)}")
                continue
            if category:
                # Normalizar categor√≠a (lowercase)
                category = category.lower()
                category_counts[category] = category_counts.get(category, 0) + 1

        # Convertir a lista de objetos
        categories = [
            {'name': cat, 'count': count}
            for cat, count in category_counts.items()
        ]

        # Ordenar por count descendente
        categories.sort(key=lambda x: x['count'], reverse=True)

        log_method_success(
            "get_event_categories",
            location=location,
            total_categories=len(categories),
            total_events=len(events)
        )

        return {
            'success': True,
            'location': location,
            'categories': categories,
            'total_categories': len(categories),
            'total_events': len(events)
        }

    except Exception as e:
        log_method_error("get_event_categories", str(e), location=location)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/events-fast")
async def get_events_ultrafast(
    location: str = Query("Buenos Aires", description="Ciudad o ubicaci√≥n"),
    limit: int = Query(10, description="Cantidad m√°xima de eventos")
):
    """
    ‚ö° ENDPOINT ULTRARR√ÅPIDO: Eventos instant√°neos sin lentitud (0.01s)
    Resuelve problema de 4-5 segundos de cuelgue al arranque
    """
    try:
        # Importar Global Image Service
        from services.global_image_service import global_image_service
        
        # Cache est√°tico que responde instant√°neamente 
        fast_events = [
            {
                "title": "Real Madrid vs Barcelona - El Cl√°sico",
                "description": "El partido m√°s esperado del a√±o en Santiago Bernab√©u",
                "venue_name": "Santiago Bernab√©u",
                "venue_address": "Santiago Bernab√©u, Madrid, Espa√±a",
                "start_datetime": "2025-09-15T21:00:00",
                "end_datetime": "2025-09-15T23:00:00",
                "category": "sports",
                "subcategory": "football",
                "price": 85,
                "currency": "EUR",
                "is_free": False,
                "latitude": 40.4531,
                "longitude": -3.6883,
                "image_url": global_image_service.get_event_image("Real Madrid vs Barcelona", "sports", "Santiago Bernab√©u", "ES"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Concierto de Rock Nacional",
                "description": "La mejor m√∫sica argentina en vivo en el Luna Park",
                "venue_name": "Luna Park",
                "venue_address": "Luna Park, Buenos Aires, Argentina",
                "start_datetime": "2025-09-12T20:30:00",
                "end_datetime": "2025-09-12T23:30:00",
                "category": "music",
                "subcategory": "rock",
                "price": 4500,
                "currency": "ARS",
                "is_free": False,
                "latitude": -34.6118,
                "longitude": -58.3664,
                "image_url": global_image_service.get_event_image("Rock Nacional", "music", "Luna Park", "AR"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Festival de Samba Rio",
                "description": "Carnaval aut√©ntico en las calles de Copacabana",
                "venue_name": "Copacabana Beach",
                "venue_address": "Copacabana, Rio de Janeiro, Brasil",
                "start_datetime": "2025-09-20T19:00:00",
                "end_datetime": "2025-09-20T02:00:00",
                "category": "cultural",
                "subcategory": "festival",
                "price": 25,
                "currency": "BRL",
                "is_free": False,
                "latitude": -22.9711,
                "longitude": -43.1822,
                "image_url": global_image_service.get_event_image("Samba Festival", "cultural", "Copacabana", "BR"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Obra de Teatro en el Col√≥n",
                "description": "Teatro cl√°sico argentino en el hist√≥rico Teatro Col√≥n",
                "venue_name": "Teatro Col√≥n",
                "venue_address": "Teatro Col√≥n, Buenos Aires, Argentina", 
                "start_datetime": "2025-09-18T20:00:00",
                "end_datetime": "2025-09-18T22:00:00",
                "category": "theater",
                "subcategory": "classical",
                "price": 8000,
                "currency": "ARS",
                "is_free": False,
                "latitude": -34.6010,
                "longitude": -58.3837,
                "image_url": global_image_service.get_event_image("Teatro Nacional", "theater", "Teatro Col√≥n", "AR"),
                "source": "ultra_fast_cache",
                "status": "live"
            }
        ]
        
        # RESPUESTA INSTANT√ÅNEA (< 0.01 segundos)
        return {
            "status": "success",
            "location": location,
            "total": len(fast_events),
            "events": fast_events[:limit],
            "response_time": "instant",
            "source_info": [
                {"source": "ultra_fast_cache", "count": len(fast_events), "status": "success"}
            ],
            "note": "‚ö° Ultra-fast cached events with Global Image Service - No network delays"
        }
        
    except Exception as e:
        logger.error(f"Error in ultra-fast endpoint: {e}")
        return {
            "status": "error",
            "events": [],
            "error": str(e)
        }

# Search events endpoint
@app.get("/api/events/search")
async def search_events(
    q: str,
    location: str = "Buenos Aires",
    radius_km: int = 25
):
    global pool
    try:
        events = []
        if pool:
            async with pool.acquire() as conn:
                query = '''
                    SELECT 
                        id, title, description, start_datetime,
                        venue_name, venue_address, category,
                        price, is_free, image_url, event_url
                    FROM events
                    WHERE 
                        (title ILIKE $1 OR description ILIKE $1)
                        AND start_datetime > NOW()
                    ORDER BY start_datetime ASC
                    LIMIT 50
                '''
                
                search_pattern = f"%{q}%"
                rows = await conn.fetch(query, search_pattern)
                
                for row in rows:
                    event = dict(row)
                    if event.get('start_datetime'):
                        event['start_datetime'] = event['start_datetime'].isoformat()
                    if event.get('price'):
                        event['price'] = float(event['price'])
                    events.append(event)
            
            return {
                "query": q,
                "location": location,
                "radius_km": radius_km,
                "results": len(events),
                "events": events
            }
    except Exception as e:
        logger.error(f"Search error: {e}")
        return {
            "error": str(e),
            "events": []
        }
# Smart search endpoint with AI
@app.post("/api/smart/search")
async def smart_search(
    query: Dict[str, Any]
):
    """
    Smart search endpoint that processes natural language queries
    """
    try:
        # Handle both "query" and "search_query" fields
        search_query = query.get("query") or query.get("search_query", "")
        original_location = query.get("location", "Buenos Aires")

        logger.info(f"üìç SMART SEARCH - Query: '{search_query}', Location received: '{original_location}'")

        # üß† USAR UBICACI√ìN DE ANALYZE-INTENT SI EST√Å DISPONIBLE
        location = original_location  # Default fallback
        detected_country = None
        
        # Primero: Verificar si hay intent_analysis en user_context
        user_context = query.get("user_context", {})
        intent_analysis = user_context.get("intent_analysis", {})
        
        # Initialize variables to prevent UnboundLocalError
        detected_country = ""
        detected_province = ""
        detected_city = ""
        
        if intent_analysis.get("success") and intent_analysis.get("intent", {}).get("location"):
            # ‚úÖ USAR UBICACI√ìN YA DETECTADA POR ANALYZE-INTENT
            location = intent_analysis["intent"]["location"]
            detected_country = intent_analysis["intent"].get("detected_country", intent_analysis["intent"].get("country", ""))
            detected_province = intent_analysis["intent"].get("detected_province", "")
            detected_city = intent_analysis["intent"].get("detected_city", location)
            logger.info(f"‚úÖ USANDO UBICACI√ìN DE ANALYZE-INTENT: '{location}' ({detected_country})")
        else:
            # Fallback: Usar intent recognition como antes
            try:
                from services.intent_recognition import intent_service
                ai_result = await intent_service.get_all_api_parameters(search_query)
                if ai_result.get("success") and ai_result.get("intent"):
                    detected_city = ai_result["intent"].get("city") or ai_result["intent"].get("location", "")
                    detected_country = ai_result["intent"].get("country", "")
                    detected_province = ai_result["intent"].get("province", "")
                    if detected_city and detected_city != "Buenos Aires":
                        location = detected_city
                        logger.info(f"üß† FALLBACK INTENT AI: query='{search_query}' ‚Üí localidad: '{location}', provincia: '{detected_province}', pa√≠s: '{detected_country}'")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error en an√°lisis Intent AI: {e}, usando detecci√≥n manual")
            
            # üîç FALLBACK: DETECTAR CIUDAD EN QUERY MANUALMENTE
            query_lower = search_query.lower()
            city_detection = {
                "madrid": "Madrid", "barcelona": "Barcelona", "valencia": "Valencia",
                "sevilla": "Sevilla", "paris": "Paris", "miami": "Miami",
                "new york": "New York", "london": "London"
            }
            
            for city_keyword, city_name in city_detection.items():
                if city_keyword in query_lower:
                    location = city_name
                    logger.info(f"üîç DETECCI√ìN MANUAL: '{city_keyword}' ‚Üí location: {city_name}")
                    break
        
        # Buscar tags de ciudad en el query (#barcelona, #madrid, etc.)
        city_tags = {
            "#barcelona": "Barcelona", "#bcn": "Barcelona", "#barna": "Barcelona",
            "#madrid": "Madrid", "#valencia": "Valencia", "#sevilla": "Sevilla",
            "#paris": "Paris", "#par√≠s": "Paris", "#lyon": "Lyon", 
            "#mexicocity": "Mexico City", "#cdmx": "Mexico City"
        }
        
        # Si encuentra tag de ciudad, IGNORAR COMPLETAMENTE el par√°metro location
        query_lower = search_query.lower()
        for tag, city_name in city_tags.items():
            if tag in query_lower:
                location = city_name
                # Remover el tag del query para que no interfiera en la b√∫squeda
                search_query = search_query.lower().replace(tag, "").strip()
                logger.info(f"üè∑Ô∏è TAG DETECTADO: '{tag}' ‚Üí Ubicaci√≥n: {city_name} (IGNORANDO geo-location)")
                break
        
        logger.error(f"üö® POST EJECUT√ÅNDOSE - query: {query}")
        logger.info(f"üîç DEBUG POST - Received query: {query}")
        logger.info(f"üîç DEBUG POST - search_query: '{search_query}', location: '{location}'")
        logger.info(f"üîç Smart search: '{search_query}' in {location}")
        
        # LOG: Inicio de b√∫squeda inteligente
        logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.info(f"‚îÇ üé® SMART SEARCH INICIADO")
        logger.info(f"‚îÇ üîç Query: '{search_query}'")
        logger.info(f"‚îÇ üìç Ubicaci√≥n: {location}")
        logger.info(f"‚îÇ üè≠ M√©todo: Factory Pattern + Fallbacks")
        logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        # ‚è±Ô∏è LOG: Timing total del smart search
        smart_search_start_time = time.time()
        
        # üè≠ HIERARCHICAL FACTORY PATTERN - Country -> Provincial delegation
        try:
            from services.hierarchical_factory import fetch_from_all_sources_internal
            logger.info(f"üè≠ FACTORY JER√ÅRQUICO - Procesando ubicaci√≥n: {location}")
            
            # ‚è±Ô∏è LOG: Timing del factory jer√°rquico
            hierarchical_start_time = time.time()
            
            # Pasar todos los datos detectados al factory
            context_data = {
                'detected_country': detected_country,
                'detected_province': detected_province,
                'detected_city': detected_city
            }
            result = await fetch_from_all_sources_internal(location, detected_country=detected_country, context_data=context_data)
            
            hierarchical_end_time = time.time()
            hierarchical_duration = hierarchical_end_time - hierarchical_start_time
            
            logger.info(f"‚è±Ô∏è HIERARCHICAL TIMING: {hierarchical_duration:.2f}s para {result.get('count', 0)} eventos")
            
            # üéØ FALLBACK JER√ÅRQUICO: Si hay pocos eventos y es carga inicial, buscar en provincia
            is_initial_load = query.get("is_initial_load", False) or search_query == ""
            event_count = result.get("count", 0) if result else 0
            original_location = location  # Guardar ubicaci√≥n original
            
            # Solo aplicar si es carga inicial Y la ubicaci√≥n original (detectada) es una ciudad espec√≠fica
            should_apply_fallback = (
                is_initial_load and 
                event_count < 10 and 
                detected_city and 
                detected_province and
                original_location.lower() != "buenos aires" and  # No aplicar si ya es Buenos Aires
                detected_city.lower() != detected_province.lower()  # Solo si ciudad != provincia
            )
            
            if should_apply_fallback:
                # Mapeo ciudad -> provincia para Argentina
                city_to_province = {
                    "merlo": "Buenos Aires",
                    "la plata": "Buenos Aires", 
                    "quilmes": "Buenos Aires",
                    "san miguel": "Buenos Aires",
                    "tigre": "Buenos Aires",
                    "san isidro": "Buenos Aires",
                    "vicente lopez": "Buenos Aires",
                    "lomas de zamora": "Buenos Aires",
                    "avellaneda": "Buenos Aires",
                    "moron": "Buenos Aires",
                    "tres de febrero": "Buenos Aires",
                    "cordoba": "C√≥rdoba",
                    "rosario": "Santa Fe",
                    "mendoza": "Mendoza",
                    "tucuman": "Tucum√°n",
                    "salta": "Salta",
                    "neuquen": "Neuqu√©n",
                    "bahia blanca": "Buenos Aires"
                }
                
                city_lower = detected_city.lower()
                parent_province = city_to_province.get(city_lower, detected_province)
                
                if parent_province and parent_province != detected_city:
                    logger.info(f"üéØ FALLBACK JER√ÅRQUICO: {detected_city} ({event_count} eventos) -> {parent_province}")
                    
                    try:
                        # Buscar en la provincia padre
                        province_context = {
                            'detected_country': detected_country,
                            'detected_province': parent_province,
                            'detected_city': parent_province
                        }
                        province_result = await fetch_from_all_sources_internal(
                            parent_province, 
                            detected_country=detected_country, 
                            context_data=province_context
                        )
                        
                        if province_result and province_result.get("count", 0) > event_count:
                            logger.info(f"‚úÖ FALLBACK SUCCESS: {parent_province} -> {province_result.get('count')} eventos")
                            result = province_result
                            result["fallback_applied"] = f"Expandido de {detected_city} a {parent_province}"
                        else:
                            logger.info(f"‚ö†Ô∏è FALLBACK: {parent_province} no mejor√≥ resultados")
                    
                    except Exception as e:
                        logger.warning(f"‚ùå Error en fallback jer√°rquico: {e}")
            
            # El resultado ya viene en formato esperado con scrapers_execution
            if result and result.get("count", 0) > 0:
                # Agregar source compatible con el formato existente
                scraper_name = result.get("scraper_used", "Unknown_Factory")
                result["source"] = f"hierarchical_{scraper_name.replace(' ', '_')}"
                
                logger.info(f"üè≠ FACTORY JER√ÅRQUICO SUCCESS - {scraper_name} - {result.get('count')} eventos")
                
                # LOG: Factory jer√°rquico maneja su propio logging interno
                logger.info(f"üè≠ FACTORY JER√ÅRQUICO - Total eventos: {result.get('count', 0)}")
                logger.info(f"üè≠ SCRAPER USADO: {result.get('scraper_used', 'Unknown')}")
                
                # LOG: Scrapers execution details
                scrapers_execution = result.get("scrapers_execution", {})
                if scrapers_execution:
                    logger.info(f"üè≠ SCRAPERS EXECUTION: {scrapers_execution.get('summary', 'No summary')}")
            else:
                # Fallback to IndustrialFactory if hierarchical factory fails
                logger.warning(f"‚ö†Ô∏è Hierarchical factory failed for {location}, using IndustrialFactory fallback")
                from services.gemini_factory import gemini_factory
                factory = gemini_factory  # Singleton - no need to instantiate
                
                # ‚è±Ô∏è LOG: Timing del fallback
                fallback_start_time = time.time()
                
                detailed_result = await factory.execute_global_scrapers_with_details(location)
                events = detailed_result.get('events', [])
                
                fallback_end_time = time.time()
                fallback_duration = fallback_end_time - fallback_start_time
                
                logger.info(f"‚è±Ô∏è FALLBACK TIMING: {fallback_duration:.2f}s para {len(events)} eventos")
                
                result = {
                    "events": events, 
                    "count": len(events), 
                    "scraper_used": "GeminiFactory",
                    "scrapers_execution": detailed_result.get('scrapers_execution', {}),
                    "execution_time": f"{fallback_duration:.2f}s"
                }
                
                # LOG: Resultados del fallback multi_source
                if result.get("source_info"):
                    logger.info("üîÑ FALLBACK MULTI_SOURCE - Resultados por servicio:")
                    for source_info in result.get("source_info", []):
                        service_name = source_info.get("source", "unknown")
                        event_count = source_info.get("count", 0)
                        logger.info(f"  üìå {service_name}: {event_count} eventos")
                else:
                    logger.info("üîÑ FALLBACK MULTI_SOURCE - Sin informaci√≥n detallada de servicios")
                
        except Exception as e:
            logger.error(f"üö® ERROR EN FACTORY: {e}")
            # Ultimate fallback to prevent crashes
            try:
                logger.warning("üîÑ Using IndustrialFactory ultimate fallback...")
                from services.gemini_factory import gemini_factory
                factory = gemini_factory  # Singleton - no need to instantiate
                
                # ‚è±Ô∏è LOG: Timing del ultimate fallback
                ultimate_start_time = time.time()
                
                detailed_result = await factory.execute_global_scrapers_with_details(location)
                events = detailed_result.get('events', [])
                
                ultimate_end_time = time.time()
                ultimate_duration = ultimate_end_time - ultimate_start_time
                
                logger.info(f"‚è±Ô∏è ULTIMATE FALLBACK TIMING: {ultimate_duration:.2f}s para {len(events)} eventos")
                
                result = {
                    "events": events, 
                    "count": len(events), 
                    "scraper_used": "GeminiFactory",
                    "scrapers_execution": detailed_result.get('scrapers_execution', {}),
                    "execution_time": f"{ultimate_duration:.2f}s"
                }
                
                # LOG: Resultados del ultimate fallback
                if result.get("source_info"):
                    logger.info("üÜò ULTIMATE FALLBACK - Resultados por servicio:")
                    total_fallback_events = 0
                    for source_info in result.get("source_info", []):
                        service_name = source_info.get("source", "unknown")
                        event_count = source_info.get("count", 0)
                        total_fallback_events += event_count
                        if event_count > 0:
                            logger.info(f"  ‚úÖ {service_name}: {event_count} eventos")
                        else:
                            logger.info(f"  ‚ùå {service_name}: 0 eventos")
                    logger.info(f"üÜò ULTIMATE FALLBACK - Total eventos: {total_fallback_events}")
                else:
                    event_count = len(result.get("events", []))
                    logger.info(f"üÜò ULTIMATE FALLBACK - {event_count} eventos (sin detalle por servicio)")
                    
            except Exception as fallback_error:
                logger.error(f"‚ùå Fallback tambi√©n fall√≥: {fallback_error}")
                logger.error(f"üî¥ TODOS LOS SERVICIOS FALLARON - Devolviendo array vac√≠o")
                result = {"status": "error", "events": [], "message": f"All systems failed: {e}"}
        
        # IMPORTANTE: Si buscamos una ciudad espec√≠fica que no es la ciudad por defecto,
        # verificar si tenemos eventos para esa ubicaci√≥n
        if location and "Buenos Aires" not in location:
            # Si el resultado viene de scrapers espec√≠ficos, mantener los eventos
            source = result.get("source", "")
            if (source == "provincial_scraper" or 
                source.startswith("global_scraper_") or 
                source.startswith("factory_")):
                logger.info(f"‚úÖ Usando eventos del scraper espec√≠fico para {location} (source: {source})")
            # Si no hay eventos y no es un scraper espec√≠fico, informar
            elif not result.get("events"):
                logger.warning(f"‚ö†Ô∏è No tenemos eventos para {location}")
                result["message"] = f"No encontramos eventos en {location}."
                result["location_available"] = False
            # ELIMINADO: Ya no filtramos eventos v√°lidos de scrapers espec√≠ficos
        
        # Don't filter too strictly - always return some events
        if search_query and result.get("events"):
            search_lower = search_query.lower()
            filtered_events = []
            all_events = result.get("events", [])
            
            logger.info(f"üîç SMART SEARCH: Procesando {len(all_events)} eventos para query '{search_query}'")
            
            for event in all_events:
                title = event.get("title", "").lower()
                desc = event.get("description", "").lower()
                cat = event.get("category", "").lower()
                venue = event.get("venue_name", "").lower()
                
                logger.info(f"  üìã Procesando evento: '{event.get('title', 'Sin t√≠tulo')[:50]}...' (categoria: {event.get('category', 'N/A')})")
                
                # More flexible matching
                words = search_lower.split()
                match_score = 0
                
                for word in words:
                    if word in title:
                        match_score += 3
                    if word in desc:
                        match_score += 2
                    if word in cat:
                        match_score += 2
                    if word in venue:
                        match_score += 1
                
                if match_score > 0:
                    event["match_score"] = match_score
                    filtered_events.append(event)
                    logger.info(f"    ‚úÖ MATCH encontrado! Score: {match_score} - '{event.get('title', 'Sin t√≠tulo')[:40]}...'")
                else:
                    logger.info(f"    ‚ùå Sin match (score: {match_score}) - '{event.get('title', 'Sin t√≠tulo')[:40]}...')")
            
            # Sort by match score
            filtered_events.sort(key=lambda x: x.get("match_score", 0), reverse=True)
            
            logger.info(f"üéØ FILTRADO COMPLETADO: {len(filtered_events)} eventos con match de {len(all_events)} totales")
            
            # If no matches or very few, return all events anyway
            if len(filtered_events) < 3:
                logger.info(f"‚ö†Ô∏è  Pocos matches ({len(filtered_events)}), devolviendo TODOS los eventos ({len(all_events)})")
                result["events"] = all_events
                result["filtered_count"] = len(all_events)
                result["no_exact_match"] = True
            else:
                logger.info(f"‚úÖ Suficientes matches, devolviendo {len(filtered_events)} eventos filtrados")
                result["events"] = filtered_events
                result["filtered_count"] = len(filtered_events)
        
        result["query"] = search_query
        result["smart_search"] = True
        
        # üñºÔ∏è MEJORAR IM√ÅGENES DE EVENTOS
        try:
            from services.global_image_service import improve_events_images
            events_list = result.get("events", [])
            if events_list:
                image_start_time = time.time()
                improved_events = await improve_events_images(events_list)
                result["events"] = improved_events
                
                image_end_time = time.time()
                image_duration = image_end_time - image_start_time
                
                # Contar im√°genes mejoradas
                improved_count = sum(1 for e in improved_events if e.get('image_improved', False))
                logger.info(f"üñºÔ∏è IMAGE SERVICE: {improved_count}/{len(events_list)} im√°genes mejoradas en {image_duration:.2f}s")
                
                result["images_improved"] = improved_count
        except Exception as img_error:
            logger.warning(f"‚ö†Ô∏è Error mejorando im√°genes: {img_error}")
        
        # El frontend espera recommended_events, no events
        result["recommended_events"] = result.get("events", [])
        
        # LOG: Resumen final de smart search
        final_events = len(result.get("events", []))
        logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        if final_events == 0:
            logger.info(f"‚îÇ ‚ùå SMART SEARCH COMPLETADO - CERO EVENTOS ENCONTRADOS")
            logger.info(f"‚îÇ üî¥ NING√öN SERVICIO DEVOLVI√ì EVENTOS")
        else:
            logger.info(f"‚îÇ ‚úÖ SMART SEARCH COMPLETADO")
            logger.info(f"‚îÇ üéØ Total eventos retornados: {final_events}")
        logger.info(f"‚îÇ üìç Ubicaci√≥n final: {location}")
        logger.info(f"‚îÇ üè≠ Fuente: {result.get('source', 'unknown')}")
        if result.get("filtered_count"):
            logger.info(f"‚îÇ üîç Eventos filtrados: {result.get('filtered_count')}")
        if result.get("no_exact_match"):
            logger.info(f"‚îÇ ‚ö†Ô∏è Sin coincidencias exactas - retornando todos")
            
        # ‚è±Ô∏è LOG: Timing total del smart search
        smart_search_end_time = time.time()
        smart_search_total_duration = smart_search_end_time - smart_search_start_time
        logger.info(f"‚îÇ ‚è±Ô∏è SMART SEARCH TOTAL: {smart_search_total_duration:.2f}s")
        
        # Add scrapers execution to result if available
        if "scrapers_execution" not in result and hasattr(result, 'scrapers_execution'):
            result["scrapers_execution"] = getattr(result, 'scrapers_execution')
        
        logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        return result
        
    except Exception as e:
        logger.error(f"Smart search error: {e}")
        return {
            "error": str(e),
            "events": [],
            "query": query.get("query", ""),
            "location": query.get("location", "Buenos Aires")
        }

# GET version for frontend compatibility
@app.get("/api/events/smart-search")
async def smart_search_get(
    q: str = Query(..., description="Search query"),
    location: str = Query(..., description="Location required")
):
    """
    GET version of smart search for frontend compatibility
    """
    # Detectar si el query contiene una ubicaci√≥n
    query_lower = q.lower()
    detected_location = location
    
    # Lista COMPLETA de ciudades argentinas y GLOBALES üåç
    cities = ["c√≥rdoba", "cordoba", "mendoza", "rosario", "la plata", "mar del plata", 
              "salta", "tucum√°n", "tucuman", "bariloche", "neuqu√©n", "neuquen",
              "santa fe", "bah√≠a blanca", "bahia blanca", "moreno", "moron", "mor√≥n",
              "tigre", "quilmes", "lan√∫s", "lanus", "avellaneda", "san isidro",
              # Espa√±a - Global cities
              "barcelona", "bcn", "barna", "madrid", "capital espa√±a", "valencia", "sevilla", "seville",
              # Francia - Global cities  
              "paris", "par√≠s", "par√≠", "lyon", "marsella", "marseille",
              # M√©xico - Global cities
              "mexico city", "cdmx", "guadalajara", "monterrey", "tijuana", "canc√∫n", "cancun",
              # Otros pa√≠ses
              "china", "chile", "uruguay", "brasil", "peru", "colombia", "mexico"]
    
    # Detectar ciudad en el query - SI ENCUENTRA ALGO, IGNORAR COMPLETAMENTE location
    city_found = False
    global_cities = ["barcelona", "bcn", "barna", "madrid", "capital espa√±a", "valencia", "sevilla", "seville",
                     "paris", "par√≠s", "par√≠", "lyon", "marsella", "marseille",
                     "mexico city", "cdmx", "guadalajara", "monterrey", "tijuana", "canc√∫n", "cancun"]
    
    logger.error(f"üö®üö®üö® GET ENDPOINT EJECUT√ÅNDOSE - query: '{q}', location param: '{location}'")
    logger.info(f"üîç DEBUG - Buscando ciudades en: '{query_lower}'")
    logger.info(f"üîç DEBUG - Lista cities: {cities}")
    
    for city in cities:
        if city in query_lower:
            logger.info(f"üîç DEBUG - ENCONTRADA: '{city}' en '{query_lower}'")
            # üåç CIUDADES GLOBALES: Sin ", Argentina"
            if city in global_cities:
                if city in ["barcelona", "bcn", "barna"]:
                    location = "Barcelona"
                elif city in ["madrid", "capital espa√±a"]:
                    location = "Madrid"
                elif city in ["paris", "par√≠s", "par√≠"]:
                    location = "Paris"
                elif city in ["valencia"]:
                    location = "Valencia"
                elif city in ["sevilla", "seville"]:
                    location = "Sevilla"
                elif city in ["mexico city", "cdmx"]:
                    location = "Mexico City"
                else:
                    location = city.title()
            else:
                # üá¶üá∑ CIUDADES ARGENTINAS: Con ", Argentina"
                location = city.title() + ", Argentina"
            
            city_found = True
            
            # Si el query ES solo la ciudad, buscar "eventos"
            if query_lower.strip() == city:
                q = "eventos"
            else:
                # Remover la parte "en [ciudad]" del query
                q = q.lower().replace(f"en {city}", "").replace(f"en {city.lower()}", "").replace(city, "").strip()
            
            # Si queda vac√≠o despu√©s de remover la ciudad, buscar "eventos"
            if not q or q == "":
                q = "eventos"
            
            logger.info(f"üìç Ciudad detectada en query: {city.title()} - IGNORANDO location parameter")
            break
    
    # Si no se detect√≥ ciudad en el query, NO BUSCAR NADA
    if not city_found and q.lower().strip() in cities:
        # Si el query completo es una ciudad, usarla
        location = q.title() + ", Argentina"
        q = "eventos"
        city_found = True
        logger.info(f"üìç Query ES una ciudad: {location}")
    elif not city_found:
        # Si no hay ciudad, intentar detectar la ubicaci√≥n autom√°ticamente
        try:
            from api.geolocation import detect_location
            detected_location = await detect_location(request=None)
            if detected_location and detected_location.get('city'):
                location = f"{detected_location['city']}, {detected_location.get('country', 'Argentina')}"
                logger.info(f"üìç Ubicaci√≥n autodetectada: {location}")
            else:
                logger.warning(f"üìç No se pudo detectar ubicaci√≥n, eventos limitados")
                location = None  # No usar fallback hardcodeado
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en detecci√≥n autom√°tica: {e}")
            location = None  # No usar fallback hardcodeado
    
    # Redirect to POST version with proper format
    return await smart_search({
        "query": q,
        "location": location
    })

# üá™üá∏ BARCELONA DIRECT ENDPOINT - Bypass routing issues
@app.get("/api/barcelona")
async def get_barcelona_events():
    """Direct Barcelona endpoint that bypasses all routing complexity"""
    try:
        logger.error(f"üá™üá∏ BARCELONA DIRECTO - Ejecutando scraper")
        
        from services.barcelona_scraper import BarcelonaScraper
        scraper = BarcelonaScraper()
        events = await scraper.scrape_all_sources()
        
        logger.error(f"üá™üá∏ BARCELONA DIRECTO - Obtenidos {len(events)} eventos")
        
        return {
            "success": True,
            "location": "Barcelona",
            "events": events[:50],
            "recommended_events": events[:50], 
            "source": "direct_barcelona_scraper",
            "total_events": len(events),
            "message": f"Eventos de Barcelona (directo)",
            "strategy": "direct_scraper_bypass"
        }
    except Exception as e:
        logger.error(f"üá™üá∏ ERROR Barcelona directo: {e}")
        return {
            "success": False,
            "location": "Barcelona", 
            "events": [],
            "error": str(e),
            "source": "direct_barcelona_scraper"
        }

# Facebook RapidAPI endpoint - LA JOYITA üíé (DEBE ir ANTES de {event_id})
@app.get("/api/events/facebook")
async def get_facebook_events(
    location: str = Query(..., description="Location required"),
    limit: int = Query(30, description="Maximum number of events to return")
):
    """
    Endpoint ultrarr√°pido de Facebook - SOLO usa cache
    üï∞Ô∏è Background job (Windows Service pattern) actualiza cada 6 horas
    ‚ö° Usuario SIEMPRE ve respuesta instant√°nea
    """
    try:
        facebook_scraper = RapidApiFacebookScraper()
        
        logger.info(f"‚ö° CACHE-ONLY: Cargando eventos de Facebook para {location}")
        
        # SOLO CACHE - nunca m√°s esperas de 20s
        cache_data = facebook_scraper.load_cache()
        cached_events = cache_data.get("events", [])
        
        if cached_events:
            # Normalizar eventos desde cache
            normalized_events = facebook_scraper.normalize_facebook_events(cached_events)
            
            # Limitar resultados
            limited_events = normalized_events[:limit]
            
            cache_info = cache_data.get("cache_info", {})
            last_updated = cache_info.get("last_updated", "unknown")
            
            logger.info(f"‚ö° CACHE HIT: {len(limited_events)} eventos (cache: {last_updated})")
        
            return {
                "status": "success",
                "source": "rapidapi_facebook_cache",
                "location": location,
                "category": "facebook_events",
                "total": len(limited_events),
                "events": limited_events,
                "cache_info": {
                    "cached": True,
                    "last_updated": last_updated,
                    "background_job": "every_6_hours",
                    "method": "windows_service_pattern"
                }
            }
        else:
            # Cache vac√≠o - el background job a√∫n no corri√≥
            logger.warning(f"‚ö†Ô∏è CACHE VAC√çO: Background job a√∫n no actualiz√≥ el cache")
            return {
                "status": "success", 
                "source": "rapidapi_facebook_cache",
                "location": location,
                "category": "facebook_events", 
                "total": 0,
                "events": [],
                "cache_info": {
                    "cached": False,
                    "message": "Background job actualizar√° cache pronto",
                    "background_job": "every_6_hours"
                }
            }
        
    except Exception as e:
        logger.error(f"‚ùå Error en Facebook endpoint: {e}")
        return {
            "status": "error",
            "error": str(e),
            "location": location,
            "total": 0,
            "events": []
        }

# Teatro endpoint - especializado en obras de teatro (DEBE ir ANTES de {event_id})
@app.get("/api/events/teatro")
async def get_teatro_events(
    location: str = Query(..., description="Location required"),
    limit: int = Query(20, description="Maximum number of events to return")
):
    """
    Endpoint especializado en obras de teatro argentinas
    Fuentes: teatros oficiales, carteleras, medios culturales
    """
    try:
        teatro_scraper = TeatroOptimizadoScraper()
        
        logger.info(f"üé≠ Buscando obras de teatro en {location}")
        
        # Scraping de teatro con timeout optimizado
        raw_events = await teatro_scraper.scrape_teatro_optimizado(max_time_seconds=8.0)
        
        # Normalizar eventos para el sistema
        normalized_events = teatro_scraper.normalize_theater_events_optimizado(raw_events)
        
        # Aplicar l√≠mite
        events = normalized_events[:limit]
        
        logger.info(f"üé≠ Retornando {len(events)} obras de teatro para {location}")
        
        return {
            "status": "success",
            "source": "teatro_optimizado",
            "location": location,
            "category": "theater",
            "total": len(events),
            "events": events,
            "scraping_info": {
                "sources_found": len(raw_events),
                "sources_normalized": len(normalized_events),
                "method": "teatro_optimizado_scraper"
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en teatro endpoint: {e}")
        return {
            "status": "error",
            "error": str(e),
            "location": location,
            "total": 0,
            "events": []
        }

# Single event endpoint
@app.get("/api/events/{event_id}")
async def get_event_by_id(event_id: str):
    """
    Obtener un evento espec√≠fico por ID/t√≠tulo
    """
    try:
        # Buscar en eventos recientes de la ciudad por defecto
        from services.hierarchical_factory import fetch_from_all_sources_internal
        result = await fetch_from_all_sources_internal("Buenos Aires")

        events = result.get("events", [])

        # Buscar por t√≠tulo de manera flexible
        for event in events:
            # Generar diferentes versiones del ID para comparar
            title_lower = event['title'].lower()
            simple_id = title_lower.replace(" ", "-").replace(",", "").replace(".", "").replace("(", "").replace(")", "")
            title_match = title_lower.replace(" ", "-")
            event_id_lower = event_id.lower()
            event_id_spaces = event_id.replace("-", " ").lower()

            # M√∫ltiples formas de hacer match
            if (simple_id == event_id_lower or
                title_match == event_id_lower or
                title_lower == event_id_spaces or
                title_lower.startswith(event_id_spaces) or
                event_id_spaces in title_lower):

                return {
                    "status": "success",
                    "event": event
                }

        # Si no se encuentra
        raise HTTPException(
            status_code=404,
            detail=f"Event '{event_id}' not found"
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting event {event_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Update event category endpoint
@app.patch("/api/events/{event_id}/category")
async def update_event_category(event_id: str, request: dict):
    """
    Actualizar la categor√≠a de un evento
    """
    try:
        import pymysql

        new_category = request.get('category')
        if not new_category:
            raise HTTPException(status_code=400, detail="Category is required")

        # Conectar a MySQL
        connection = pymysql.connect(
            host=os.getenv('MYSQL_HOST', 'localhost'),
            port=int(os.getenv('MYSQL_PORT', 3306)),
            user=os.getenv('MYSQL_USER', 'root'),
            password=os.getenv('MYSQL_PASSWORD', ''),
            database=os.getenv('MYSQL_DATABASE', 'events'),
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor
        )

        cursor = connection.cursor()

        # Normalizar el event_id para buscar por t√≠tulo
        event_id_normalized = event_id.replace('-', ' ')

        # Actualizar categor√≠a
        update_query = '''
            UPDATE events
            SET category = %s
            WHERE LOWER(REPLACE(title, ' ', '-')) = LOWER(%s)
            OR LOWER(title) LIKE LOWER(%s)
        '''

        cursor.execute(update_query, (new_category, event_id, f'%{event_id_normalized}%'))
        connection.commit()

        rows_affected = cursor.rowcount

        cursor.close()
        connection.close()

        if rows_affected > 0:
            logger.info(f"‚úÖ Categor√≠a actualizada para evento {event_id}: {new_category}")
            return {
                "success": True,
                "message": f"Category updated to {new_category}",
                "rows_affected": rows_affected
            }
        else:
            raise HTTPException(status_code=404, detail="Event not found")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error updating category for {event_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Update event image endpoint
@app.post("/api/events/{event_id}/update-image")
async def update_event_image(event_id: str, request: dict):
    """
    Buscar y actualizar la imagen de un evento usando Google Images
    """
    try:
        import pymysql

        title = request.get('title')
        venue_name = request.get('venue_name', '')
        city = request.get('city', '')

        if not title:
            raise HTTPException(status_code=400, detail="Title is required")

        logger.info(f"üñºÔ∏è Buscando imagen para: {title} (venue: {venue_name}, city: {city})")

        # Usar el servicio de Google Images con 5 estrategias:
        # 1. Solo t√≠tulo
        # 2. T√≠tulo + venue
        # 3. Solo venue
        # 4. Primeras 3 palabras
        # 5. Solo ciudad
        from services.google_images_service import search_google_image

        image_url = await search_google_image(title, venue=venue_name, city=city)

        if not image_url or 'gstatic' in image_url:
            logger.warning(f"‚ö†Ô∏è No se encontr√≥ imagen v√°lida para: {title}")
            return {
                "success": False,
                "message": "No valid image found"
            }

        # Solo retornar la imagen encontrada (no actualizar DB porque eventos son de APIs)
        logger.info(f"‚úÖ Imagen encontrada para '{title}': {image_url[:50]}...")

        # Intentar actualizar en MySQL si existe (opcional, no falla si no existe)
        try:
            connection = pymysql.connect(
                host=os.getenv('MYSQL_HOST', 'localhost'),
                port=int(os.getenv('MYSQL_PORT', 3306)),
                user=os.getenv('MYSQL_USER', 'root'),
                password=os.getenv('MYSQL_PASSWORD', ''),
                database=os.getenv('MYSQL_DATABASE', 'events'),
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor
            )

            cursor = connection.cursor()
            event_id_normalized = event_id.replace('-', ' ')

            update_query = '''
                UPDATE events
                SET image_url = %s
                WHERE LOWER(REPLACE(title, ' ', '-')) = LOWER(%s)
                OR LOWER(title) LIKE LOWER(%s)
            '''

            cursor.execute(update_query, (image_url, event_id, f'%{event_id_normalized}%'))
            connection.commit()
            rows_affected = cursor.rowcount

            cursor.close()
            connection.close()

            if rows_affected > 0:
                logger.info(f"‚úÖ Imagen tambi√©n actualizada en DB (opcional)")
        except Exception as db_error:
            logger.debug(f"DB update skipped (evento probablemente de API): {db_error}")

        # Siempre retornar √©xito si encontramos imagen
        return {
            "success": True,
            "image_url": image_url,
            "message": "Image found successfully"
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error updating image for {event_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Test endpoint para probar b√∫squeda de im√°genes con palabra hardcodeada
@app.get("/api/test/image-search")
async def test_image_search():
    """
    üß™ Endpoint de prueba para b√∫squeda de im√°genes en Google
    Busca im√°genes para palabras f√°ciles como "pizza", "cafe", "music"
    """
    try:
        from services.google_images_service import search_google_image

        # Probar con palabras f√°ciles
        test_queries = [
            ("pizza", ""),
            ("cafe", "Buenos Aires"),
            ("music concert", ""),
            ("football", "")
        ]

        results = []
        for query, venue in test_queries:
            logger.info(f"üß™ Probando b√∫squeda: '{query}' (venue: '{venue}')")
            image_url = await search_google_image(query, venue=venue)
            results.append({
                "query": query,
                "venue": venue,
                "image_url": image_url,
                "found": image_url is not None
            })

        return {
            "success": True,
            "results": results
        }

    except Exception as e:
        logger.error(f"‚ùå Error en test: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Similar events streaming endpoint
@app.get("/api/events/{event_id}/similar/stream")
async def stream_similar_events(event_id: str):
    """
    üéØ SSE STREAMING - Eventos similares del mismo lugar

    Env√≠a progresivamente eventos similares basados en:
    - Mismo lugar/ciudad
    - Misma categor√≠a
    - Precio similar
    - Fechas pr√≥ximas
    """
    from fastapi.responses import StreamingResponse
    import json
    import asyncio

    async def similar_events_generator():
        try:
            # 1. Buscar el evento principal primero
            from services.hierarchical_factory import fetch_from_all_sources_internal
            result = await fetch_from_all_sources_internal("Buenos Aires")

            events = result.get("events", [])

            # Encontrar el evento solicitado
            found_event = None
            for event in events:
                title_lower = event['title'].lower()
                simple_id = title_lower.replace(" ", "-").replace(",", "").replace(".", "").replace("(", "").replace(")", "")
                title_match = title_lower.replace(" ", "-")
                event_id_lower = event_id.lower()
                event_id_spaces = event_id.replace("-", " ").lower()

                if (simple_id == event_id_lower or
                    title_match == event_id_lower or
                    title_lower == event_id_spaces or
                    title_lower.startswith(event_id_spaces) or
                    event_id_spaces in title_lower):
                    found_event = event
                    break

            if not found_event:
                yield f"data: {json.dumps({'type': 'error', 'message': 'Evento no encontrado'})}\n\n"
                return

            # 2. Extraer criterios del evento principal
            event_category = found_event.get('category', '').lower()
            event_location = found_event.get('location', 'Buenos Aires')
            event_price = found_event.get('price', 0) if not found_event.get('is_free') else 0
            event_title = found_event.get('title', '')
            event_venue = found_event.get('venue_name', '')

            yield f"data: {json.dumps({'type': 'start', 'message': f'Buscando eventos similares en {event_location}'})}\n\n"
            logger.info(f"üéØ Buscando eventos similares para '{event_title}' en {event_location}")

            # 3. Calcular score de similitud para cada evento
            similar_events = []
            for event in events:
                # Skip el evento actual
                if event.get('title') == event_title:
                    continue

                # Score de similitud
                score = 0

                # 1Ô∏è‚É£ Mismo lugar/ciudad (+50 puntos - M√ÅS IMPORTANTE)
                if event.get('location', '').lower() == event_location.lower():
                    score += 50

                # 2Ô∏è‚É£ Misma categor√≠a (+30 puntos)
                if event.get('category', '').lower() == event_category:
                    score += 30

                # 3Ô∏è‚É£ Precio similar (+15 puntos)
                event_event_price = event.get('price', 0) if not event.get('is_free') else 0
                if event_price == 0 and event_event_price == 0:
                    score += 15  # Ambos gratis
                elif event_price > 0 and event_event_price > 0:
                    price_ratio = min(event_price, event_event_price) / max(event_price, event_event_price)
                    if price_ratio > 0.5:  # Precios similares (dentro del 50%)
                        score += 15

                # 4Ô∏è‚É£ Mismo venue (+5 puntos bonus)
                if event.get('venue_name') == event_venue:
                    score += 5

                # Solo agregar eventos con score >= 50 (al menos mismo lugar)
                if score >= 50:
                    similar_events.append({
                        **event,
                        'similarity_score': score
                    })

            # 4. Ordenar por score y enviar progresivamente
            similar_events.sort(key=lambda x: x['similarity_score'], reverse=True)

            # Enviar eventos de a uno con delay para efecto progresivo
            sent_count = 0
            for similar_event in similar_events[:6]:  # Top 6 eventos similares
                yield f"data: {json.dumps({'type': 'event', 'event': similar_event, 'index': sent_count})}\n\n"
                sent_count += 1
                logger.info(f"üì° SSE: Evento similar enviado ({sent_count}/6): {similar_event['title']} (score: {similar_event['similarity_score']})")

                # Delay de 100ms entre eventos para efecto visual
                await asyncio.sleep(0.1)

            # 5. Enviar evento de completado
            yield f"data: {json.dumps({'type': 'complete', 'total': sent_count})}\n\n"
            logger.info(f"‚úÖ SSE completado: {sent_count} eventos similares enviados")

        except Exception as e:
            logger.error(f"‚ùå Error en stream de eventos similares: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(similar_events_generator(), media_type="text/event-stream")


# AI recommendation endpoint
# AI chat endpoint
@app.post("/api/ai/chat")
async def ai_chat(data: Dict[str, Any]):
    """AI chat endpoint for interactive recommendations"""
    message = data.get("message", "")
    context = data.get("context", {})
    
    # üîç PARSEO SIMPLE DE UBICACIONES
    detected_location = "Buenos Aires"
    message_lower = message.lower()
    
    # Mapeo de ciudades GLOBALES (traveler app)
    location_mapping = {
        # Argentina
        "c√≥rdoba": "C√≥rdoba", "cordoba": "C√≥rdoba", "la docta": "C√≥rdoba",
        "mendoza": "Mendoza", "mza": "Mendoza",
        "rosario": "Rosario",
        "la plata": "La Plata",
        "mar del plata": "Mar del Plata", "mardel": "Mar del Plata",
        "salta": "Salta",
        "tucum√°n": "Tucum√°n", "tucuman": "Tucum√°n",
        "bariloche": "Bariloche",
        "neuqu√©n": "Neuqu√©n", "neuquen": "Neuqu√©n",
        "buenos aires": "Buenos Aires", "caba": "Buenos Aires", "capital": "Buenos Aires",
        
        # Espa√±a - Proof of concept global
        "barcelona": "Barcelona", "bcn": "Barcelona", "barna": "Barcelona",
        "madrid": "Madrid", "capital espa√±a": "Madrid",
        "valencia": "Valencia",
        "sevilla": "Sevilla", "seville": "Sevilla",
        
        # Francia  
        "paris": "Paris", "par√≠s": "Paris",
        "lyon": "Lyon",
        "marseille": "Marseille", "marsella": "Marseille",
        
        # M√©xico
        "cdmx": "Mexico City", "ciudad de m√©xico": "Mexico City", "df": "Mexico City",
        "guadalajara": "Guadalajara", "gdl": "Guadalajara",
        "monterrey": "Monterrey",
        
        # Colombia
        "bogot√°": "Bogot√°", "bogota": "Bogot√°",
        "medell√≠n": "Medell√≠n", "medellin": "Medell√≠n",
        
        # Chile
        "santiago": "Santiago", "santiago chile": "Santiago"
    }
    
    # Buscar ciudad en el mensaje
    for city_key, city_name in location_mapping.items():
        if city_key in message_lower:
            detected_location = city_name
            logger.info(f"üèôÔ∏è Ubicaci√≥n detectada en texto: '{city_key}' ‚Üí {city_name}")
            break
    
    # Si no detect√≥ ciudad en el texto, usar geolocalizaci√≥n autom√°tica
    if detected_location == "Buenos Aires":
        try:
            from api.geolocation import detect_location
            from fastapi import Request
            
            # Crear un request mock para obtener la IP del usuario
            # En una implementaci√≥n real, deber√≠as pasar el request real
            logger.info("üìç No se detect√≥ ciudad, usando geolocalizaci√≥n autom√°tica...")
            detected_location_data = await detect_location(request=None)
            if detected_location_data and detected_location_data.get('city'):
                detected_location = detected_location_data['city']
                logger.info(f"üìç Ciudad autodetectada: {detected_location}")
            else:
                logger.warning("üìç No se pudo autodetectar ubicaci√≥n")
                detected_location = None  # No usar fallback hardcodeado
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en geolocalizaci√≥n autom√°tica: {e}")
            detected_location = "Buenos Aires"
    
    # üöÄ USE PROVINCIAL/GLOBAL SCRAPERS (no database)
    if detected_location == "Mendoza":
        logger.info(f"‚úÖ Usando eventos del scraper provincial para {detected_location}")
        from services.provincial_scrapers import MendozaScraper
        mendoza_scraper = MendozaScraper()
        events = await mendoza_scraper.scrape_all()
    elif detected_location == "Barcelona":
        logger.info(f"‚úÖ Usando eventos del scraper global para {detected_location}")
        from services.barcelona_scraper import BarcelonaScraper
        barcelona_scraper = BarcelonaScraper()
        events = await barcelona_scraper.scrape_all_sources()
    else:
        # Buenos Aires y otros - usar multi-source
        from services.hierarchical_factory import fetch_from_all_sources
        result = await fetch_from_all_sources(location=detected_location)
        events = result.get("events", [])
    
    # Score events by relevance to query
    scored_events = []
    query_words = message.lower().split()
    
    for event in events:
        score = 0
        title_lower = event.get('title', '').lower()
        desc_lower = event.get('description', '').lower()
        category_lower = event.get('category', '').lower()
        
        # Basic scoring
        for word in query_words:
            if word in title_lower:
                score += 3
            if word in desc_lower:
                score += 2
            if word in category_lower:
                score += 2
        
        if score > 0 or len(query_words) <= 1:  # Include all if generic query
            event['match_score'] = score
            scored_events.append(event)
    
    # Sort by score
    scored_events.sort(key=lambda x: x.get('match_score', 0), reverse=True)
    
    return {
        "success": True,
        "location": detected_location,
        "events": scored_events[:10],
        "recommended_events": scored_events[:5],
        "total_events": len(events),
        "sources_completed": [{"source": f"{detected_location} Scrapers", "count": len(events), "status": "success"}],
        "message": f"Eventos espec√≠ficos de {detected_location}" if scored_events else f"No encontramos eventos espec√≠ficos de {detected_location}.",
        "query": message,
        "smart_search": True,
        "filtered_count": len(scored_events)
    }

# AI plan weekend endpoint
@app.post("/api/ai/plan-weekend")
async def ai_plan_weekend(data: Dict[str, Any]):
    """Plan weekend with AI"""
    location = data.get("location", "Buenos Aires")
    from services.hierarchical_factory import fetch_from_all_sources
    result = await fetch_from_all_sources(location=location)
    
    # Filter for weekend events
    weekend_events = result.get("events", [])[:10]
    
    return {
        "plan": "Weekend plan generated",
        "events": weekend_events,
        "location": location
    }

# AI trending now endpoint
@app.get("/api/ai/trending-now")
async def ai_trending_now():
    """Get trending events"""
    from services.hierarchical_factory import fetch_from_all_sources
    result = await fetch_from_all_sources(location="Buenos Aires")

    return {
        "trending": result.get("events", [])[:5],
        "timestamp": datetime.utcnow().isoformat()
    }

# AI nearby cities endpoint
@app.get("/api/ai/nearby-cities")
async def ai_nearby_cities(location: str, limit: int = 10):
    """Get nearby cities using Gemini AI"""
    try:
        from services.ai_service import get_nearby_cities_with_ai

        logger.info(f"üåç Getting nearby cities for: {location}")

        # Use AI to get nearby cities
        cities = await get_nearby_cities_with_ai(location, limit)

        if not cities:
            return {
                "success": False,
                "error": "No nearby cities found",
                "cities": []
            }

        return {
            "success": True,
            "location": location,
            "cities": cities,
            "count": len(cities)
        }

    except Exception as e:
        logger.error(f"‚ùå Error getting nearby cities: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "cities": []
        }

# Advanced scraping endpoint
@app.get("/api/scraping/multi-technique")
async def scraping_multi_technique():
    """
    Test all advanced scraping techniques
    """
    try:
        logger.info("üöÄ Starting advanced multi-technique scraping...")
        
        # Initialize scrapers
        multi_scraper = MultiTechniqueScraper()
        cloudscraper = CloudscraperEvents()
        # massive_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
        
        # Run scraping in parallel
        tasks = [
            multi_scraper.scrape_all_methods(),
            cloudscraper.fetch_all_events("Buenos Aires"),
            massive_scraper.massive_scraping(max_urls=8)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Extract results
        multi_events = results[0] if isinstance(results[0], list) else []
        cloud_events = results[1] if isinstance(results[1], list) else []
        massive_events = results[2] if isinstance(results[2], list) else []
        
        # Combine results
        all_events = multi_events + cloud_events + massive_events
        
        # Normalize events
        if multi_events:
            normalized_multi = multi_scraper.normalize_events(multi_events)
        else:
            normalized_multi = []
            
        if cloud_events:
            normalized_cloud = cloud_events  # Already normalized
        else:
            normalized_cloud = []
            
        if massive_events:
            normalized_massive = massive_scraper.normalize_events(massive_events)
        else:
            normalized_massive = []
        
        all_normalized = normalized_multi + normalized_cloud + normalized_massive
        
        logger.info(f"‚úÖ Advanced scraping completed: {len(all_normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(all_normalized),
            "events": all_normalized,
            "sources": {
                "multi_technique": len(normalized_multi),
                "cloudscraper": len(normalized_cloud),
                "massive_eventbrite": len(normalized_massive)
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Advanced scraping error: {e}")
        return {
            "status": "error", 
            "message": str(e),
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Facebook human session scraping endpoint
@app.get("/api/scraping/facebook-human")
async def scraping_facebook_human():
    """
    Facebook scraping using human session (requires setup)
    """
    try:
        logger.info("üöÄ Starting Facebook human session scraping...")
        
        scraper = FacebookHumanSessionScraper()
        
        # Check if session exists
        if not scraper.session_file.exists():
            return {
                "status": "setup_required",
                "message": "Human session not configured. Run setup first.",
                "events": [],
                "setup_instructions": "Run: python -m backend.services.facebook_human_session_scraper setup"
            }
        
        # Run scraping
        events = await scraper.scrape_all_events(venues_limit=4, searches_limit=2)
        
        # Normalize events
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"‚úÖ Facebook human scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "events": normalized,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Facebook human scraping error: {e}")
        return {
            "status": "error",
            "message": str(e),
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Facebook hybrid removed - only real data sources allowed

# Instagram hybrid removed - only real data sources allowed

# Bright Data scraping endpoint  
@app.get("/api/scraping/bright-data")
async def scraping_bright_data():
    """
    Facebook scraping using Bright Data proxies
    """
    try:
        logger.info("üöÄ Starting Bright Data scraping...")
        
        # Note: Bright Data config would need to be set up
        # scraper = FacebookBrightDataScraper()  # MOVED TO LEGACY
        
        events = await scraper.scrape_all_venues(limit_per_venue=3)
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"‚úÖ Bright Data scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "events": normalized,
            "note": "Bright Data config not set - using fallback methods",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Bright Data scraping error: {e}")
        return {
            "status": "error",
            "message": str(e), 
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Massive Eventbrite scraper endpoint  
@app.get("/api/scraping/eventbrite-massive")
async def scraping_eventbrite_massive():
    """
    Massive Eventbrite scraping - obtiene MUCHOS m√°s eventos
    """
    try:
        logger.info("üöÄ Starting massive Eventbrite scraping...")
        
        scraper = EventbriteMassiveScraper()
        
        # Scraping masivo
        events = await scraper.massive_scraping(max_urls=12)
        
        # Normalizar eventos
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"‚úÖ Massive Eventbrite scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "raw_events": len(events),
            "events": normalized,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Massive Eventbrite scraping error: {e}")
        return {
            "status": "error",
            "message": str(e), 
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# üß† AI intent analysis endpoint - POWERED BY GEMINI
@app.post("/api/ai/analyze-intent")
async def analyze_intent(
    data: Dict[str, Any]
):
    """
    üß† Analyze user intent from natural language using GEMINI AI
    """
    try:
        query = data.get("query", "")
        current_location = data.get("current_location", "Buenos Aires")
        
        logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.info(f"‚îÇ üöÄ EJECUTANDO: analyze_intent | query='{query[:40]}...' | location={current_location}")
        logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        if not query.strip():
            logger.error("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
            logger.error(f"‚îÇ ‚ùå ERROR: analyze_intent | Query vac√≠o requerido")
            logger.error("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
            return {
                "success": False,
                "error": "Query is required",
                "intent": {}
            }
        
        # Usar servicio unificado de reconocimiento de intenciones
        from services.intent_recognition import intent_service
        
        result = await intent_service.get_all_api_parameters(query)
        
        # Formatear respuesta para compatibilidad con frontend existente
        intent = {
            "query": query,
            "categories": [result['intent']['category']] if result['intent']['category'] != 'Todos' else [],
            "location": result['intent']['location'],
            "time_preference": None,  # Se puede agregar despu√©s
            "price_preference": None,
            
            # Informaci√≥n adicional de Gemini - ESTRUCTURA CORREGIDA
            "confidence": result['intent']['confidence'],
            "detected_country": result['intent']['country'],  # Usar 'country' en lugar de 'detected_country'
            "detected_city": result['intent']['city'],        # Usar 'city' en lugar de 'location'
            "detected_province": result['intent'].get('province', ''),
            "keywords": result['intent'].get('keywords', []),
            "intent_type": result['intent']['type'],
            
            # Agregar jerarqu√≠a geogr√°fica completa
            "geographic_hierarchy": result['intent'].get('geographic_hierarchy', {}),
            "scraper_config": result['intent'].get('scraper_config', {})
        }
        
        # Crear user_context con l√≥gica de prioridades
        # 1. PRIORIDAD: Ubicaci√≥n detectada por IA (override)
        # 2. FALLBACK: current_location del frontend
        detected_location = result['intent']['location']
        final_location = detected_location if detected_location else current_location
        
        user_context = {
            "location": final_location,
            "coordinates": None,  # Se podr√≠a agregar geocoding despu√©s
            "detected_country": result['intent']['country']  # Usar 'country' en lugar de 'detected_country'
        }
        
        logger.info("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.info(f"‚îÇ ‚úÖ √âXITO: analyze_intent | categor√≠a={result['intent']['category']} | ubicaci√≥n={detected_location}")
        logger.info(f"‚îÇ confianza={result['intent']['confidence']:.2f} | pa√≠s={result['intent']['country']} | tipo={result['intent']['type']}")
        logger.info(f"‚îÇ üó∫Ô∏è JERARQU√çA: {result['intent'].get('geographic_hierarchy', {}).get('full_location', 'N/A')}")
        logger.info("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        
        return {
            "success": True,
            "intent": intent,
            "user_context": user_context,  # ‚Üê Nuevo: contexto actualizado
            "apis": {
                "location": detected_location,
                "category": result['intent']['category']
            }
        }
        
    except Exception as e:
        logger.error("‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        logger.error(f"‚îÇ ‚ùå ERROR: analyze_intent | {str(e)}")
        logger.error("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ")
        return {
            "success": False,
            "error": str(e),
            "intent": {
                "query": query,
                "categories": [],
                "location": None,
                "fallback": True
            },
            "user_context": {
                "location": current_location,  # Usar current_location si falla
                "coordinates": None,
                "detected_country": None
            }
        }


@app.post("/api/ai/event-insight")
async def event_insight(data: Dict[str, Any]):

    try:

        from services.ai_manager import AIServiceManager


        title = data.get("title", "")

        category = data.get("category", "")

        venue_name = data.get("venue_name", "")

        price = data.get("price", "")



        logger.info(f"‚ú® Generando insight con IA para: {title[:40]}... ({category})")



        # Prompt detallado para describir qu√© esperar del evento

        prompt = f"""Genera una descripci√≥n detallada y atractiva (2-3 oraciones, m√°ximo 250 caracteres) de qu√© puede esperar el asistente en este evento:

T√≠tulo: {title}

Categor√≠a: {category}

Lugar: {venue_name}

Precio: {price if price else 'Gratis'}



Describe la experiencia, ambiente, y qu√© hace especial a este evento. S√© espec√≠fico y entusiasta. Responde SOLO la descripci√≥n, sin t√≠tulos ni explicaciones adicionales."""



        manager = AIServiceManager()



        # Generar con IA (Grok es muy r√°pido, <1 segundo)

        response = await manager.generate(

            prompt=prompt,

            temperature=0.7,

            use_fallback=True

        )



        if response:

            result = {

                "success": True,

                "insight": {

                    "quick_insight": response[:300]  # Limitar a 300 caracteres para descripciones detalladas

                }

            }

            logger.info(f"‚úÖ Insight generado: {response[:50]}...")

            return result

        else:

            # Fallback a texto gen√©rico si IA falla

            fallback_text = f"Gran evento de {category} en {venue_name or 'la ciudad'}"

            return {

                "success": True,

                "fallback": {

                    "quick_insight": fallback_text

                },

                "insight": {

                    "quick_insight": fallback_text

                }

            }



    except Exception as e:

        logger.error(f"‚ùå Error generando insight: {e}")



        # Fallback gen√©rico en caso de error

        fallback_text = f"Evento imperdible de {data.get('category', 'entretenimiento')}"

        return {

            "success": False,

            "error": str(e),

            "fallback": {

                "quick_insight": fallback_text

            },

            "insight": {

                "quick_insight": fallback_text

            }

        }

# ============================================
# ü§ñ ENDPOINTS DE GESTI√ìN DE AI PROVIDERS
# ============================================

@app.get("/api/ai/provider/status")
async def get_ai_provider_status():
    """
    üìä ESTADO DE PROVIDERS DE IA

    Retorna el estado de todos los providers de IA disponibles
    y cu√°l est√° configurado como preferido

    Returns:
        {
            "preferred": "grok",
            "providers": {
                "grok": {"configured": true, "name": "GrokProvider"},
                "groq": {"configured": false, "name": "GroqProvider"},
                "gemini": {"configured": true, "name": "GeminiProvider"},
                ...
            }
        }
    """
    try:
        from services.ai_manager import AIServiceManager

        manager = AIServiceManager()
        status = manager.get_provider_status()

        logger.info(f"üìä Estado de providers solicitado - Preferido: {status['preferred']}")
        return status

    except Exception as e:
        logger.error(f"‚ùå Error obteniendo estado de providers: {e}")
        return {
            "error": str(e),
            "preferred": "unknown",
            "providers": {}
        }


@app.post("/api/ai/provider/set")
async def set_ai_provider(data: Dict[str, Any]):
    """
    üîÑ CAMBIAR PROVIDER DE IA PREFERIDO

    Args:
        provider: Nombre del provider (grok, groq, gemini, perplexity, openrouter)

    Returns:
        {
            "success": true,
            "provider": "grok",
            "message": "Provider cambiado exitosamente"
        }
    """
    try:
        from services.ai_manager import AIServiceManager

        provider = data.get("provider", "").lower()

        if not provider:
            return {
                "success": False,
                "error": "Provider no especificado"
            }

        manager = AIServiceManager()
        success = manager.set_preferred_provider(provider)

        if success:
            logger.info(f"‚úÖ Provider cambiado a: {provider}")
            return {
                "success": True,
                "provider": provider,
                "message": f"Provider cambiado a {provider} exitosamente"
            }
        else:
            logger.warning(f"‚ö†Ô∏è No se pudo cambiar a provider: {provider}")
            return {
                "success": False,
                "error": f"Provider {provider} no est√° configurado o no es v√°lido"
            }

    except Exception as e:
        logger.error(f"‚ùå Error cambiando provider: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.post("/api/ai/generate-event-context")
async def generate_event_context_endpoint(data: Dict[str, Any]):
    """
    üé® GENERAR CONTEXTO ADICIONAL PARA EVENTO

    Usa IA para generar informaci√≥n adicional interesante sobre un evento:
    - Curiosidades
    - Qu√© llevar
    - Ambiente esperado
    - Tips locales

    Args:
        event_data: {
            title, category, venue_name, city, start_datetime
        }

    Returns:
        {
            "curiosidades": [...],
            "que_llevar": [...],
            "ambiente_esperado": "...",
            "tip_local": "..."
        }
    """
    try:
        from services.ai_manager import generate_event_context

        event_data = data.get("event_data", data)  # Soportar ambos formatos

        logger.info(f"üé® Generando contexto para: {event_data.get('title', 'Unknown')}")

        context = await generate_event_context(event_data)

        if context:
            return {
                "success": True,
                **context
            }
        else:
            return {
                "success": False,
                "error": "No se pudo generar contexto"
            }

    except Exception as e:
        logger.error(f"‚ùå Error generando contexto de evento: {e}")
        return {
            "success": False,
            "error": str(e)
        }


# WebSocket endpoint for notifications
@app.websocket("/ws/notifications")
async def websocket_notifications(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Process received message
            message = json.loads(data)
            
            # Broadcast notification to all connected clients
            notification = {
                "type": "event_reminder",
                "message": message.get("message", "New event notification"),
                "timestamp": datetime.utcnow().isoformat()
            }
            
            await manager.broadcast(json.dumps(notification))
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket)

# WebSocket streaming scrapers progress endpoint
@app.websocket("/ws/scrapers-progress")
async def websocket_scrapers_progress(websocket: WebSocket):
    """üîÑ WebSocket para mostrar progreso de scrapers en tiempo real"""
    await manager.connect(websocket)
    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "connection",
            "status": "connected",
            "message": "üîÑ Conexi√≥n establecida - Streaming de scrapers activado"
        })
        
        while True:
            # Wait for scraping request
            data = await websocket.receive_json()
            
            if data.get("action") == "start_scraping":
                location = data.get("location", "Buenos Aires")
                category = data.get("category")
                limit = data.get("limit", 30)
                
                # Stream scrapers with real-time progress
                await stream_scrapers_with_progress(websocket, location, category, limit)
                
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket scrapers progress error: {e}")
        manager.disconnect(websocket)

# WebSocket streaming search endpoint
@app.websocket("/ws/search-events")
async def websocket_search_events(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "connection",
            "status": "connected",
            "message": "üî• Conexi√≥n establecida - Listo para b√∫squeda streaming"
        })
        
        while True:
            # Wait for search request
            data = await websocket.receive_json()
            
            if data.get("action") == "search":
                # üîç PARSEO DE UBICACI√ìN EN WEBSOCKET (como IA-First)
                message = data.get("message", "")
                base_location = data.get("location", "Buenos Aires")
                detected_location = base_location
                
                # Si hay mensaje, intentar detectar ubicaci√≥n
                if message:
                    message_lower = message.lower()
                    
                    # Mapeo de ciudades GLOBALES (traveler app)
                    location_mapping = {
                        # Argentina
                        "c√≥rdoba": "C√≥rdoba", "cordoba": "C√≥rdoba", "la docta": "C√≥rdoba",
                        "mendoza": "Mendoza", "mza": "Mendoza",
                        "rosario": "Rosario",
                        "la plata": "La Plata",
                        "mar del plata": "Mar del Plata", "mardel": "Mar del Plata",
                        "salta": "Salta",
                        "tucum√°n": "Tucum√°n", "tucuman": "Tucum√°n",
                        "bariloche": "Bariloche",
                        "neuqu√©n": "Neuqu√©n", "neuquen": "Neuqu√©n",
                        "buenos aires": "Buenos Aires", "caba": "Buenos Aires", "capital": "Buenos Aires",
                        
                        # Espa√±a - Proof of concept global
                        "barcelona": "Barcelona", "bcn": "Barcelona", "barna": "Barcelona",
                        "madrid": "Madrid", "capital espa√±a": "Madrid",
                        "valencia": "Valencia",
                        "sevilla": "Sevilla", "seville": "Sevilla",
                        
                        # Francia  
                        "paris": "Paris", "par√≠s": "Paris",
                        "lyon": "Lyon",
                        "marseille": "Marseille", "marsella": "Marseille",
                        
                        # M√©xico
                        "cdmx": "Mexico City", "ciudad de m√©xico": "Mexico City", "df": "Mexico City",
                        "guadalajara": "Guadalajara", "gdl": "Guadalajara",
                        "monterrey": "Monterrey",
                        
                        # Colombia
                        "bogot√°": "Bogot√°", "bogota": "Bogot√°",
                        "medell√≠n": "Medell√≠n", "medellin": "Medell√≠n",
                        
                        # Chile
                        "santiago": "Santiago", "santiago chile": "Santiago"
                    }
                    
                    # Buscar ciudad en el mensaje
                    for city_key, city_name in location_mapping.items():
                        if city_key in message_lower:
                            detected_location = city_name
                            logger.info(f"üèôÔ∏è WebSocket - Ubicaci√≥n detectada: '{city_key}' ‚Üí {city_name}")
                            break
                
                # Send search started con ubicaci√≥n detectada
                await websocket.send_json({
                    "type": "search_started",
                    "status": "searching",
                    "location": detected_location,
                    "message": f"üöÄ Iniciando b√∫squeda en {detected_location}...",
                    "location_source": "detected" if detected_location != base_location else "default"
                })
                
                # Start streaming search con ubicaci√≥n correcta
                await stream_events_optimized(websocket, detected_location)
                
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket search error: {e}")
        manager.disconnect(websocket)

async def stream_events_search(websocket: WebSocket, location: str):
    """Stream events from multiple Argentine sources via WebSocket"""
    total_events = 0
    batch_size = 3
    
    try:
        # üéØ SCRAPERS ESPEC√çFICOS POR UBICACI√ìN
        logger.info(f"üîç WebSocket - Iniciando b√∫squeda espec√≠fica para: {location}")
        
        # 1. Scrapers espec√≠ficos por provincia
        if location in ["C√≥rdoba", "Cordoba"]:
            await websocket.send_json({
                "type": "source_started", 
                "source": "cordoba_provincial",
                "message": f"üèõÔ∏è Fuentes espec√≠ficas de C√≥rdoba...",
                "progress": 10
            })
            
            try:
                from services.provincial_scrapers import CordobaScraper
                cordoba_scraper = CordobaScraper()
                cordoba_events = await cordoba_scraper.scrape_all()
                
                # Send C√≥rdoba events in batches
                for i in range(0, len(cordoba_events), batch_size):
                    batch = cordoba_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "cordoba_provincial",
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(cordoba_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de C√≥rdoba: {e}")
                
        elif location == "Mendoza":
            await websocket.send_json({
                "type": "source_started", 
                "source": "mendoza_provincial",
                "message": f"üç∑ Fuentes espec√≠ficas de Mendoza...",
                "progress": 10
            })
            
            try:
                from services.provincial_scrapers import MendozaScraper
                mendoza_scraper = MendozaScraper()
                mendoza_events = await mendoza_scraper.scrape_all()
                
                # Send Mendoza events in batches
                for i in range(0, len(mendoza_events), batch_size):
                    batch = mendoza_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "mendoza_provincial", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(mendoza_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de Mendoza: {e}")
                
        elif location == "Barcelona":
            await websocket.send_json({
                "type": "source_started", 
                "source": "barcelona_global",
                "message": f"üá™üá∏ Fuentes espec√≠ficas de Barcelona...",
                "progress": 10
            })
            
            try:
                from services.barcelona_scraper import BarcelonaScraper
                barcelona_scraper = BarcelonaScraper()
                barcelona_events = await barcelona_scraper.scrape_all_sources()
                
                # Send Barcelona events in batches
                for i in range(0, len(barcelona_events), batch_size):
                    batch = barcelona_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "barcelona_global", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(barcelona_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de Barcelona: {e}")
        
        # 2. Fuentes generales (solo si es Buenos Aires o para complementar)
        if location == "Buenos Aires" or total_events < 10:
            # Eventbrite Argentina (m√°s r√°pido)
            await websocket.send_json({
                "type": "source_started", 
                "source": "eventbrite",
                "message": "üé´ Buscando en Eventbrite Argentina...",
                "progress": 45
            })
            
            try:
                # eventbrite_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
                eventbrite_events = await eventbrite_scraper.massive_scraping(max_urls=6)
                eventbrite_normalized = eventbrite_scraper.normalize_events(eventbrite_events)
                
                # Send Eventbrite events in batches
                for i in range(0, len(eventbrite_normalized), batch_size):
                    batch = eventbrite_normalized[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "eventbrite", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(45 + (i/max(len(eventbrite_normalized), 1)) * 15, 60)
                    })
                    await asyncio.sleep(0.3)
            except Exception as e:
                logger.error(f"Error en Eventbrite: {e}")
            
            # Venues Argentinos Oficiales
            await websocket.send_json({
                "type": "source_started",
                "source": "argentina_venues", 
                "message": "üèõÔ∏è Venues oficiales de Argentina...",
                "progress": 60
            })
            
            try:
                # Import venue scrapers
                from services.oficial_venues_scraper import OficialVenuesScraper
                # from services.argentina_venues_scraper import ArgentinaVenuesScraper  # DELETED
                
                # Argentina Venues Scraper - DELETED - FAKE DATA GENERATOR
                # argentina_scraper = ArgentinaVenuesScraper()  # DELETED - FAKE DATA GENERATOR
                # argentina_events = await argentina_scraper.fetch_all_events()  # DELETED - FAKE DATA GENERATOR
                argentina_events = []  # No fake data - return empty array
                
                # Send Argentina venues events (now empty)
                for i in range(0, len(argentina_events), batch_size):
                    batch = argentina_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "argentina_venues",
                        "events": batch,
                        "batch_count": len(batch), 
                        "total_so_far": total_events,
                        "progress": min(60 + (i/max(len(argentina_events), 1)) * 15, 75)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en venues argentinos: {e}")
        
        # 3. Fuentes adicionales (si necesario)
        if total_events < 20:
            # Ticketek Argentina (si existe)
            await websocket.send_json({
                "type": "source_started",
                "source": "ticketmaster",
                "message": "üé™ Ticketek/Ticketmaster Argentina...", 
                "progress": 80
            })
            
            try:
                # Try to scrape Ticketek if available
                from services.ticketek_scraper import TicketekArgentinaScraper
                ticketek_scraper = TicketekArgentinaScraper()
                ticketek_events = await ticketek_scraper.fetch_all_events()
                
                for i in range(0, len(ticketek_events), batch_size):
                    batch = ticketek_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "ticketmaster",
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(80 + (i/max(len(ticketek_events), 1)) * 15, 95)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Ticketek no disponible: {e}")
        
        # B√∫squeda completada
        await websocket.send_json({
            "type": "search_completed",
            "status": "success",
            "total_events": total_events,
            "location": location,
            "sources_completed": 3 if location in ["Mendoza", "C√≥rdoba", "Cordoba"] else 2,
            "progress": 100,
            "message": f"üéâ B√∫squeda completada en {location}! {total_events} eventos encontrados"
        })
        
    except Exception as e:
        logger.error(f"Error en b√∫squeda streaming: {e}")
        await websocket.send_json({
            "type": "search_error",
            "status": "error", 
            "message": f"Error en la b√∫squeda: {str(e)}",
            "progress": 100
        })

@app.get("/api/cache/status")
async def get_cache_status():
    """Estado del cache diario"""
    try:
        from services.daily_cache import daily_cache
        stats = await daily_cache.get_cache_stats()
        return {
            "status": "success",
            "cache": stats
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@app.post("/api/cache/refresh")
async def refresh_cache():
    """Forzar actualizaci√≥n del cache (usar con cuidado)"""
    try:
        from services.daily_cache import daily_cache
        results = await daily_cache.update_cache_from_scrapers()
        return {
            "status": "success",
            "message": "Cache refreshed",
            "results": results
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@app.get("/api/multi/fetch-stream") 
async def fetch_stream_progressive(location: str = Query(..., description="Location required")):
    """
    Streaming endpoint que devuelve eventos en fases progresivas
    Para frontend que quiera mostrar datos conforme llegan
    """
    try:
        logger.info(f"üì° Progressive stream iniciado para: {location}")
        
        scraper = ProgressiveSyncScraper()
        
        # Recolectar todas las fases
        phases = []
        async for phase_result in scraper.progressive_fetch_stream(location):
            phases.append(phase_result)
        
        return {
            "status": "success",
            "location": location,
            "phases": phases,
            "strategy": "progressive_stream",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en progressive stream: {e}")
        return {
            "status": "error",
            "error": str(e),
            "phases": [],
            "message": f"‚ùå Error durante progressive stream: {str(e)}"
        }

# üóìÔ∏è DAILY SOCIAL SCRAPER ENDPOINTS

@app.get("/api/daily-scraper/run")
async def run_daily_social_scraping():
    """
    Ejecuta el scraping diario de Facebook e Instagram
    Pensado para ser llamado por un cron job una vez por d√≠a
    """
    try:
        logger.info("üóìÔ∏è Iniciando scraping diario de redes sociales...")
        
        scraper = DailySocialScraper()
        result = await scraper.run_daily_scraping()
        
        # Aqu√≠ ser√≠a donde guardas en la base de datos
        # await save_daily_events_to_db(result['events'])
        
        logger.info(f"‚úÖ Scraping diario completado: {result['total_events_found']} eventos")
        
        return {
            "status": "success", 
            "message": "Daily scraping completed successfully",
            "summary": {
                "total_events": result['total_events_found'],
                "facebook_events": result['facebook_events'],
                "instagram_events": result['instagram_events'],
                "execution_time": f"{result['execution_time_seconds']:.1f}s",
                "scraping_date": result['scraping_date'],
                "next_scraping": result['next_scraping']
            },
            "events_sample": result['events'][:5],  # Solo muestra 5 como ejemplo
            "cron_job_info": {
                "frequency": "Once per day",
                "recommended_time": "03:00 AM local time",
                "endpoint": "/api/daily-scraper/run",
                "storage": "Database storage ready"
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error en daily scraping: {e}")
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/api/daily-scraper/status")
async def daily_scraper_status():
    """
    Estado del sistema de scraping diario
    """
    return {
        "status": "ready",
        "strategy": "Daily Facebook & Instagram scraping with DB storage",
        "features": [
            "üïí Runs once per day via cron job",
            "üìò Facebook real event extraction", 
            "üì∏ Instagram real event extraction",
            "üíæ Database storage for retrieved events",
            "üö´ No simulated/generated events",
            "üîç Smart deduplication across platforms"
        ],
        "target_venues": {
            "facebook": 8,
            "instagram": 8 
        },
        "implementation": "‚úÖ DailySocialScraper class ready",
        "database_integration": "‚ö†Ô∏è Pending implementation",
        "next_steps": "Set up cron job to call /api/daily-scraper/run"
    }

@app.get("/api/debug/scrapers-status")
async def get_scrapers_debug():
    """
    üîç DEBUG ENDPOINT - Para inspector del browser
    Muestra estado de todos los scrapers: execution_status y results_count
    """
    try:
        from services.country_scraper_factory import get_events_by_location
        
        # Test con Miami para obtener debug info
        result = await get_events_by_location("Miami")
        
        debug_info = []
        if result.get("scrapers_executed"):
            for scraper_name, scraper_info in result["scrapers_executed"].items():
                debug_info.append({
                    "scraper_name": scraper_name,
                    "execution_status": scraper_info.get("status", "unknown"),
                    "results_count": scraper_info.get("events", 0),
                    "execution_time": "N/A",  # Lo agregaremos despu√©s
                    "country": "USA" if "Miami" in scraper_name else "Unknown",
                    "city": "Miami" if "Miami" in scraper_name else "Unknown"
                })
        
        return {
            "total_scrapers": len(debug_info),
            "scrapers": debug_info,
            "timestamp": datetime.utcnow().isoformat(),
            "inspector_info": "üîç Use this endpoint to debug scrapers from browser inspector"
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "scrapers": [],
            "total_scrapers": 0
        }

# üßπ DAILY CLEANUP ENDPOINTS
@app.post("/api/admin/cleanup")
async def run_manual_cleanup():
    """
    üßπ Ejecutar limpieza manual de eventos expirados
    √ötil para testing o limpieza bajo demanda
    """
    try:
        from services.daily_cleanup import run_manual_cleanup
        result = await run_manual_cleanup()
        return result
    except Exception as e:
        logger.error(f"‚ùå Error en limpieza manual: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/admin/cleanup/status")
async def get_cleanup_status():
    """
    üîç Obtener estado del sistema de limpieza
    """
    try:
        from services.daily_cleanup import DailyCleanupManager
        
        cleanup_manager = DailyCleanupManager()
        now = datetime.now()
        
        return {
            "current_time": now.isoformat(),
            "cleanup_hour": cleanup_manager.cleanup_hour,
            "should_run_now": cleanup_manager.should_run_cleanup(),
            "time_until_next_cleanup": f"{(cleanup_manager.cleanup_hour - now.hour) % 24} horas",
            "retention_days": cleanup_manager.retention_days,
            "cache_files_monitored": len(cleanup_manager.cache_files),
            "status": "active"
        }
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e)
        }

# üß™ ADMIN TESTING ENDPOINTS - Individual Source Testing
@app.post("/api/test/facebook")
async def test_facebook_source(request: Request):
    """Test Facebook scraping individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Barcelona")
        
        # from services.rapidapi_facebook_scraper import RapidApiFacebookScraper
        scraper = RapidApiFacebookScraper()
        events = await scraper.scrape_facebook_events_rapidapi(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Facebook",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Facebook",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/eventbrite")
async def test_eventbrite_source(request: Request):
    """Test Eventbrite scraping individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Barcelona")
        
        from services.eventbrite_web_scraper import EventbriteWebScraper
        scraper = EventbriteWebScraper()
        events = await scraper.fetch_events_by_location(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Eventbrite",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Eventbrite",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/venues")
async def test_venues_source(request: Request):
    """Test Official Venues scraping individual"""
    start_time = time.time()
    try:
        from services.oficial_venues_scraper import OficialVenuesScraper
        scraper = OficialVenuesScraper()
        events = await scraper.scrape_all_oficial_venues()
        
        execution_time = time.time() - start_time
        return {
            "source": "Official Venues",
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Official Venues",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/provincial")
async def test_provincial_source(request: Request):
    """Test Provincial scrapers individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "C√≥rdoba")
        
        from services.provincial_scrapers import get_events_by_province
        events = await get_events_by_province(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Provincial",
            "location": location,
            "events": events.get("events", []) if isinstance(events, dict) else events,
            "count": len(events.get("events", [])) if isinstance(events, dict) else len(events) if events else 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Provincial",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/multi-source")
async def test_multi_source(request: Request):
    """Test Multi-Source API individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Buenos Aires")
        
        from services.hierarchical_factory import fetch_from_all_sources
        result = await fetch_from_all_sources(location=location, fast=True)
        events = result.get("events", []) if result else []
        
        execution_time = time.time() - start_time
        return {
            "source": "Multi-Source",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Multi-Source",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/factory")
async def test_factory_source(request: Request):
    """Test Hierarchical Factory individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "C√≥rdoba")
        
        from services.hierarchical_factory import get_events_hierarchical
        result = await get_events_hierarchical(location)
        events = result.get("events", []) if result else []
        
        execution_time = time.time() - start_time
        return {
            "source": "Hierarchical Factory",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success",
            "scraper_used": result.get("scraper_used", "Unknown") if result else "Unknown"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Hierarchical Factory",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

async def stream_scrapers_with_progress(websocket: WebSocket, location: str, category: Optional[str] = None, limit: int = 30):
    """
    üîÑ STREAMING DE SCRAPERS CON PROGRESO EN TIEMPO REAL
    
    Env√≠a informaci√≥n detallada de cada scraper mientras se ejecuta:
    - Estado de inicio/ejecuci√≥n/finalizaci√≥n
    - Tiempo de ejecuci√≥n en vivo
    - Eventos encontrados por scraper
    - Detalles t√©cnicos (URLs generadas, PatternService, etc.)
    """
    try:
        # Inicializar contexto para scrapers
        from services.intent_recognition import intent_service
        
        await websocket.send_json({
            "type": "scraping_started",
            "message": f"üß† Analizando ubicaci√≥n: {location}",
            "location": location,
            "progress": 0,
            "timestamp": datetime.now().isoformat()
        })
        
        # An√°lisis de ubicaci√≥n con IA
        start_ai = time.time()
        ai_result = await intent_service.get_all_api_parameters(location)
        end_ai = time.time()
        
        detected_location = ai_result.get("location", location)
        detected_country = ai_result.get("country", "")
        detected_province = ai_result.get("province", "")
        detected_city = ai_result.get("city", "")
        
        await websocket.send_json({
            "type": "location_analyzed",
            "message": f"‚úÖ Ubicaci√≥n analizada: {detected_city}, {detected_province}, {detected_country}",
            "location_data": {
                "original": location,
                "detected": detected_location,
                "city": detected_city,
                "province": detected_province,
                "country": detected_country
            },
            "analysis_time": f"{end_ai - start_ai:.2f}s",
            "progress": 15,
            "timestamp": datetime.now().isoformat()
        })
        
        # Preparar contexto para scrapers
        context_data = {
            'detected_country': detected_country,
            'detected_province': detected_province,
            'detected_city': detected_city
        }
        
        await websocket.send_json({
            "type": "scrapers_discovery",
            "message": "üîç Descubriendo scrapers disponibles...",
            "progress": 20,
            "timestamp": datetime.now().isoformat()
        })
        
        # Usar IndustrialFactory con streaming personalizado
        from services.gemini_factory import gemini_factory
        factory = gemini_factory  # Singleton - no need to instantiate
        
        # Auto-descubrir scrapers
        scrapers_map = factory.discovery_engine.discover_all_scrapers()
        global_scrapers = scrapers_map.get('global', {})
        
        await websocket.send_json({
            "type": "scrapers_discovered",
            "message": f"üåç {len(global_scrapers)} scrapers globales descubiertos",
            "scrapers": list(global_scrapers.keys()),
            "progress": 30,
            "timestamp": datetime.now().isoformat()
        })
        
        # Ejecutar cada scraper individualmente con progreso detallado
        total_events = []
        scraper_results = []
        progress_step = 60 // len(global_scrapers) if global_scrapers else 60
        current_progress = 30
        
        for i, (scraper_name, scraper_instance) in enumerate(global_scrapers.items()):
            scraper_start = time.time()
            
            await websocket.send_json({
                "type": "scraper_started",
                "scraper_name": scraper_name.title(),
                "message": f"üöÄ Ejecutando {scraper_name.title()}Scraper...",
                "progress": current_progress,
                "timestamp": datetime.now().isoformat()
            })
            
            try:
                # Configurar contexto si el scraper lo soporta
                if hasattr(scraper_instance, 'set_context'):
                    scraper_instance.set_context(context_data)
                
                # Ejecutar scraper con timeout
                events = await asyncio.wait_for(
                    scraper_instance.scrape_events(detected_location, category, limit),
                    timeout=5.0
                )
                
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                events_count = len(events) if isinstance(events, list) else 0
                
                if events_count > 0:
                    total_events.extend(events)
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'success',
                    'events_count': events_count,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_completed",
                    "scraper_name": scraper_name.title(),
                    "message": f"‚úÖ {scraper_name.title()}: {events_count} eventos en {execution_time:.2f}s",
                    "events_count": events_count,
                    "execution_time": f"{execution_time:.2f}s",
                    "events": events[:3] if events_count > 0 else [],  # Solo primeros 3 como preview
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
                
            except asyncio.TimeoutError:
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'timeout',
                    'events_count': 0,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_timeout",
                    "scraper_name": scraper_name.title(),
                    "message": f"‚è∞ {scraper_name.title()}: Timeout despu√©s de 5s",
                    "execution_time": f"{execution_time:.2f}s",
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'error',
                    'events_count': 0,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_error",
                    "scraper_name": scraper_name.title(),
                    "message": f"‚ùå {scraper_name.title()}: {str(e)}",
                    "error": str(e),
                    "execution_time": f"{execution_time:.2f}s",
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
            
            current_progress += progress_step
        
        # Enviar resultado final
        await websocket.send_json({
            "type": "scraping_completed",
            "message": f"üéØ Scraping completado: {len(total_events)} eventos totales",
            "total_events": len(total_events),
            "scrapers_summary": scraper_results,
            "successful_scrapers": len([s for s in scraper_results if s['status'] == 'success']),
            "total_scrapers": len(global_scrapers),
            "events": total_events,  # Todos los eventos
            "progress": 100,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå Error en streaming de scrapers: {str(e)}")
        await websocket.send_json({
            "type": "scraping_error",
            "message": f"‚ùå Error general: {str(e)}",
            "error": str(e),
            "progress": 0,
            "timestamp": datetime.now().isoformat()
        })

async def stream_events_optimized(websocket: WebSocket, location: str):
    """
    üöÄ STREAMING OPTIMIZADO CON INDUSTRIAL FACTORY
    
    Entrega eventos en tiempo real usando:
    - Sistema de cach√© JSON (0 llamadas a IA)
    - Solo scrapers habilitados (2 en lugar de 9) 
    - Streaming por scraper individual (no esperar a todos)
    """
    try:
        await websocket.send_json({
            "type": "search_started",
            "message": f"üöÄ B√∫squeda optimizada iniciada para {location}",
            "location": location,
            "enabled_scrapers": ["Eventbrite", "Meetup"],
            "progress": 0
        })
        
        # Usar IndustrialFactory para obtener scrapers optimizados
        from services.gemini_factory import gemini_factory
        factory = gemini_factory  # Singleton - no need to instantiate
        
        # Auto-discovery con flags - solo scrapers habilitados
        scrapers_map = factory.discovery_engine.discover_all_scrapers()
        global_scrapers = scrapers_map.get('global', {})
        
        await websocket.send_json({
            "type": "scrapers_discovered",
            "message": f"üîß {len(global_scrapers)} scrapers habilitados listos",
            "scrapers": list(global_scrapers.keys()),
            "progress": 10
        })
        
        # Ejecutar scrapers INDIVIDUALMENTE con streaming
        total_events = 0
        scraper_count = len(global_scrapers)
        
        for i, (scraper_name, scraper_instance) in enumerate(global_scrapers.items()):
            scraper_progress = 10 + (i * 80 // scraper_count)
            
            await websocket.send_json({
                "type": "scraper_started",
                "scraper": scraper_name.title(),
                "message": f"‚ö° {scraper_name.title()} iniciado...",
                "progress": scraper_progress
            })
            
            # Ejecutar scraper individual con timeout
            start_time = time.time()
            try:
                events = await asyncio.wait_for(
                    scraper_instance.scrape_events(location, None, 30),
                    timeout=5.0
                )
                execution_time = time.time() - start_time
                
                if events and len(events) > 0:
                    total_events += len(events)
                    
                    # ENTREGAR EVENTOS INMEDIATAMENTE
                    await websocket.send_json({
                        "type": "events_batch",
                        "scraper": scraper_name.title(),
                        "events": events,
                        "count": len(events),
                        "execution_time": f"{execution_time:.2f}s",
                        "total_events": total_events,
                        "progress": scraper_progress + 15,
                        "status": "success"
                    })
                    
                    logger.info(f"üöÄ WebSocket - {scraper_name.title()}: {len(events)} eventos enviados en {execution_time:.2f}s")
                else:
                    await websocket.send_json({
                        "type": "scraper_completed",
                        "scraper": scraper_name.title(),
                        "count": 0,
                        "execution_time": f"{execution_time:.2f}s",
                        "message": f"‚ùå {scraper_name.title()}: 0 eventos",
                        "progress": scraper_progress + 15,
                        "status": "empty"
                    })
                    
            except asyncio.TimeoutError:
                await websocket.send_json({
                    "type": "scraper_timeout",
                    "scraper": scraper_name.title(),
                    "message": f"‚è∞ {scraper_name.title()}: Timeout despu√©s de 5s",
                    "progress": scraper_progress + 15,
                    "status": "timeout"
                })
                logger.warning(f"‚è∞ WebSocket - {scraper_name.title()}: Timeout")
                
            except Exception as e:
                await websocket.send_json({
                    "type": "scraper_error",
                    "scraper": scraper_name.title(),
                    "message": f"‚ùå {scraper_name.title()}: {str(e)}",
                    "progress": scraper_progress + 15,
                    "status": "error"
                })
                logger.error(f"‚ùå WebSocket - {scraper_name.title()}: {str(e)}")
        
        # Enviar resumen final
        await websocket.send_json({
            "type": "search_completed",
            "total_events": total_events,
            "scrapers_used": len(global_scrapers),
            "message": f"‚úÖ B√∫squeda completada: {total_events} eventos totales",
            "progress": 100
        })
        
        logger.info(f"üéØ WebSocket - B√∫squeda completada: {total_events} eventos de {len(global_scrapers)} scrapers")
        
    except Exception as e:
        await websocket.send_json({
            "type": "search_error",
            "message": f"‚ùå Error en b√∫squeda: {str(e)}",
            "progress": 100
        })
        logger.error(f"‚ùå WebSocket streaming error: {str(e)}")

@app.get("/api/admin")
async def admin_page():
    """Serve admin test page"""
    try:
        with open("/mnt/c/Code/eventos-visualizer/backend/templates/admin.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except Exception as e:
        return HTMLResponse(content=f"<h1>Error loading admin page: {e}</h1>", status_code=500)

if __name__ == "__main__":
    # Fix encoding for Windows console to support emojis
    import sys
    if sys.platform == 'win32':
        import io
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

    # Beautiful startup banner with colors
    print("\n" + Fore.CYAN + "="*60)
    print(Fore.YELLOW + " "*20 + "üéâ EVENTOS VISUALIZER üéâ")
    print(Fore.WHITE + " "*18 + "Backend Server Starting...")
    print(Fore.CYAN + "="*60 + Style.RESET_ALL)

    print(Fore.MAGENTA + "\nüìç ENDPOINTS DISPONIBLES:")
    print(Fore.BLUE + "‚îÄ"*60)
    print(Fore.GREEN + f"  üè† Server:     {Fore.WHITE}{BACKEND_URL}")
    print(Fore.GREEN + f"  üíö Health:     {Fore.WHITE}{BACKEND_URL}/health")
    print(Fore.GREEN + f"  üìö API Docs:   {Fore.WHITE}{BACKEND_URL}/docs")
    print(Fore.GREEN + f"  üîß Swagger UI: {Fore.WHITE}{BACKEND_URL}/redoc")

    ws_url = BACKEND_URL.replace('http://', 'ws://').replace('https://', 'wss://')
    print(f"  üîå WebSocket:  {ws_url}/ws/notifications")

    print(Fore.MAGENTA + "\nüî• SERVICIOS ACTIVOS:")
    print(Fore.BLUE + "‚îÄ"*60)
    print(Fore.GREEN + "  ‚úÖ Gemini AI Integration")
    print(Fore.GREEN + "  ‚úÖ Location Detection Service")
    print(Fore.GREEN + "  ‚úÖ Multi-Source Event Scraping")
    print(Fore.GREEN + "  ‚úÖ Real-time WebSocket Support")
    print(Fore.GREEN + "  ‚úÖ Chat Memory Manager")

    print(Fore.MAGENTA + "\nüéØ APIS CONFIGURADAS:")
    print(Fore.BLUE + "‚îÄ"*60)
    if os.getenv("EVENTBRITE_API_KEY"):
        print(Fore.GREEN + "  ‚úÖ Eventbrite API")
    else:
        print(Fore.YELLOW + "  ‚ö†Ô∏è  Eventbrite API (no key)")
    if os.getenv("TICKETMASTER_API_KEY"):
        print(Fore.GREEN + "  ‚úÖ Ticketmaster API")
    else:
        print(Fore.YELLOW + "  ‚ö†Ô∏è  Ticketmaster API (no key)")
    if os.getenv("RAPIDAPI_KEY"):
        print(Fore.GREEN + "  ‚úÖ Facebook Events (RapidAPI)")
    else:
        print(Fore.YELLOW + "  ‚ö†Ô∏è  Facebook Events (no key)")
    if os.getenv("GEMINI_API_KEY"):
        print(Fore.GREEN + "  ‚úÖ Google Gemini AI")
    else:
        print(Fore.YELLOW + "  ‚ö†Ô∏è  Google Gemini AI (no key)")

    print(Fore.MAGENTA + "\nüíæ DATABASE STATUS:")
    print(Fore.BLUE + "‚îÄ"*60)
    if os.getenv("DATABASE_URL"):
        print(Fore.GREEN + "  üîó PostgreSQL: Configured")
    else:
        print(Fore.YELLOW + "  ‚ö†Ô∏è  PostgreSQL: Not configured (using in-memory)")

    print("\n" + Fore.CYAN + "="*60)
    print(Fore.YELLOW + Style.BRIGHT + " "*15 + "üöÄ SERVER READY TO ROCK! üöÄ")
    print(Fore.CYAN + "="*60 + Style.RESET_ALL + "\n")
    
    # Use app directly instead of module string to avoid re-import issues
    # üî• REGLA CR√çTICA: Escuchar en todas las interfaces (0.0.0.0) para SSE desde frontend
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",  # Todas las interfaces - permite SSE desde frontend
            port=BACKEND_PORT,
            reload=False,
            log_level="info"
        )
    except Exception as e:
        print(Fore.RED + "\n" + "="*60)
        print(Fore.RED + Style.BRIGHT + "‚ùå ERROR FATAL AL INICIAR SERVIDOR:")
        print(Fore.RED + "="*60)
        print(Fore.YELLOW + f"\n{type(e).__name__}: {str(e)}")
        print(Fore.WHITE + "\nDetalles completos del error:")
        import traceback
        traceback.print_exc()
        print(Fore.RED + "\n" + "="*60 + Style.RESET_ALL)
        raise