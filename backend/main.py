import os
import asyncio
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, Query, Request
from fastapi.responses import HTMLResponse
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import uvicorn
from dotenv import load_dotenv

# Heroku configuration
try:
    from heroku_config import apply_heroku_config, is_heroku
    apply_heroku_config()
    if is_heroku():
        print("ğŸŒ Running on Heroku - Production mode activated")
except ImportError:
    print("ğŸ“± Running locally - Development mode")

# Import timeout interceptor
from middleware.timeout_interceptor import TimeoutInterceptor
import asyncpg
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import logging
import sys
import aiohttp
import time

# Add backend to path
sys.path.append('/mnt/c/Code/eventos-visualizer/backend')

# Import advanced scrapers - ONLY WORKING ONES
# from services.multi_technique_scraper import MultiTechniqueScraper
# from services.facebook_bright_data_scraper import FacebookBrightDataScraper  # MOVED TO LEGACY
# from services.facebook_human_session_scraper import FacebookHumanSessionScraper  # MOVED TO LEGACY
# Hybrid scrapers removed - only real data sources allowed
# from services.daily_social_scraper import DailySocialScraper
# from services.cloudscraper_events import CloudscraperEvents
# from services.eventbrite_massive_scraper import EventbriteMassiveScraper  # MOVED TO LEGACY
# from services.hybrid_sync_scraper import HybridSyncScraper
# from services.progressive_sync_scraper import ProgressiveSyncScraper
# from services.teatro_optimizado_scraper import TeatroOptimizadoScraper
# from services.rapidapi_facebook_scraper import RapidApiFacebookScraper

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ“‹ SISTEMA DE LOGS PROLIJAMENTE FORMATEADOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def log_method_start(method_name: str, **params):
    """Logs method start with parameters in a neat square format"""
    param_str = " | ".join([f"{k}={v}" for k, v in params.items() if v is not None])
    if param_str:
        param_str = f" | {param_str}"
    
    logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"â”‚ ğŸš€ EJECUTANDO: {method_name}{param_str}")
    logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

def log_method_success(method_name: str, **results):
    """Logs successful method completion with results"""
    result_items = []
    for k, v in results.items():
        if v is not None:
            result_items.append(f"{k}={v}")
    
    result_str = " | ".join(result_items) if result_items else "Sin datos adicionales"
    
    logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"â”‚ âœ… Ã‰XITO: {method_name} | {result_str}")
    logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

def log_method_error(method_name: str, error: str, **params):
    """Logs method error with parameters and error details"""
    param_str = " | ".join([f"{k}={v}" for k, v in params.items() if v is not None])
    if param_str:
        param_str = f" | {param_str}"
    
    logger.error("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.error(f"â”‚ âŒ ERROR: {method_name}{param_str}")
    logger.error(f"â”‚ ğŸ’¥ Detalle: {error}")
    logger.error("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

def log_scraper_summary(method_name: str, scrapers_called: list, events_by_scraper: dict, total_events: int):
    """Logs scraper summary with detailed breakdown"""
    logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"â”‚ ğŸ•·ï¸  RESUMEN SCRAPERS: {method_name}")
    logger.info(f"â”‚ ğŸ“Š Total scrapers llamados: {len(scrapers_called)}")
    logger.info("â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    for scraper in scrapers_called:
        events_count = events_by_scraper.get(scraper, 0)
        status = "âœ… Ã‰XITO" if events_count > 0 else "âš ï¸  SIN DATOS"
        logger.info(f"â”‚ {scraper}: {status} ({events_count} eventos)")
        
        if events_count > 0 and scraper in events_by_scraper:
            # Mostrar nombres de eventos si estÃ¡n disponibles
            event_names = events_by_scraper.get(f"{scraper}_names", [])[:3]  # Primeros 3
            if event_names:
                for name in event_names:
                    logger.info(f"â”‚   â”œâ”€ {name[:50]}{'...' if len(name) > 50 else ''}")
    
    logger.info("â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"â”‚ ğŸ¯ TOTAL EVENTOS OBTENIDOS: {total_events}")
    logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

# Get configuration from environment - Production ready
# New approach: Use single BACKEND_URL instead of separate HOST+PORT
IS_PRODUCTION = os.getenv("ENVIRONMENT", "development") == "production"

# Parse BACKEND_URL to extract host and port for uvicorn
BACKEND_URL = os.getenv("BACKEND_URL", "http://172.29.228.80:8001" if not IS_PRODUCTION else "https://funaroundyou-f21e91cae36c.herokuapp.com")

# Extract host and port from URL for uvicorn.run()
from urllib.parse import urlparse
parsed_url = urlparse(BACKEND_URL)
HOST = parsed_url.hostname or "172.29.228.80"
BACKEND_PORT = parsed_url.port or (443 if parsed_url.scheme == "https" else 8001)

# Override with Heroku's PORT if available (Heroku deployment)
if os.getenv("PORT"):
    BACKEND_PORT = int(os.getenv("PORT"))
    HOST = "0.0.0.0"  # Heroku requires binding to all interfaces
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/eventos_db")
# âŒ REMOVED: No more hardcoded "Buenos Aires"
# Location will always come from user geolocation or parameter

# Connection pool for PostgreSQL
pool = None

async def update_facebook_cache_background():
    """
    Background job que actualiza el cache de Facebook cada 6 horas
    ğŸ•°ï¸ Windows Service pattern en Python - Timer automÃ¡tico
    """
    try:
        logger.info("ğŸ”„ BACKGROUND JOB: Iniciando actualizaciÃ³n de cache Facebook...")
        
        facebook_scraper = RapidApiFacebookScraper()
        
        # Hacer el request pesado (20s) - nadie espera aquÃ­
        events = await facebook_scraper.scrape_facebook_events_rapidapi(
            city_name="Buenos Aires",  # Temporary fallback for chat context 
            limit=50, 
            max_time_seconds=30.0  # MÃ¡s tiempo para background
        )
        
        if events:
            # Guardar en cache - el scraper ya lo hace internamente
            logger.info(f"âœ… BACKGROUND JOB: Cache actualizado con {len(events)} eventos")
        else:
            logger.warning("âš ï¸ BACKGROUND JOB: No se obtuvieron eventos")
            
    except Exception as e:
        logger.error(f"âŒ BACKGROUND JOB ERROR: {e}")
        # No crashea el servidor, solo logea el error

# WebSocket connections manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                pass

manager = ConnectionManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    global pool
    try:
        # Create connection pool on startup
        logger.info("ğŸš€ Starting Eventos Visualizer Backend...")
        # Try PostgreSQL connection
        try:
            pool = await asyncpg.create_pool(
                DATABASE_URL,
                min_size=5,
                max_size=20,
                command_timeout=60
            )
            logger.info("âœ… Database pool created successfully")
        except:
            logger.warning("âš ï¸ PostgreSQL not available, running without database")
            pool = None
        
        # Initialize database schema only if pool exists
        if pool:
            async with pool.acquire() as conn:
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS events (
                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                        title VARCHAR(255) NOT NULL,
                        description TEXT,
                        start_datetime TIMESTAMP WITH TIME ZONE,
                        end_datetime TIMESTAMP WITH TIME ZONE,
                        venue_name VARCHAR(255),
                        venue_address TEXT,
                        latitude DECIMAL(10, 8),
                        longitude DECIMAL(11, 8),
                        category VARCHAR(100),
                        subcategory VARCHAR(100),
                        price DECIMAL(10,2),
                        currency CHAR(3) DEFAULT 'ARS',
                        is_free BOOLEAN DEFAULT false,
                        external_id VARCHAR(255),
                        source_api VARCHAR(50),
                        image_url TEXT,
                        event_url TEXT,
                        created_at TIMESTAMP DEFAULT NOW(),
                        updated_at TIMESTAMP DEFAULT NOW()
                    );
                    
                    CREATE TABLE IF NOT EXISTS users (
                        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                        email VARCHAR(255) UNIQUE NOT NULL,
                        name VARCHAR(255) NOT NULL,
                        avatar_url TEXT,
                        google_id VARCHAR(255),
                        latitude DECIMAL(10, 8),
                        longitude DECIMAL(11, 8),
                        radius_km INTEGER DEFAULT 25,
                        timezone VARCHAR(100) DEFAULT 'America/Argentina/Buenos_Aires',
                        locale VARCHAR(10) DEFAULT 'es-AR',
                        push_enabled BOOLEAN DEFAULT false,
                        created_at TIMESTAMP DEFAULT NOW()
                    );
                    
                    CREATE TABLE IF NOT EXISTS user_events (
                        user_id UUID REFERENCES users(id),
                        event_id UUID REFERENCES events(id),
                        status VARCHAR(50) DEFAULT 'interested',
                        notification_24h_sent BOOLEAN DEFAULT false,
                        notification_1h_sent BOOLEAN DEFAULT false,
                        calendar_synced BOOLEAN DEFAULT false,
                        created_at TIMESTAMP DEFAULT NOW(),
                        PRIMARY KEY (user_id, event_id)
                    );
                    
                    CREATE INDEX IF NOT EXISTS idx_events_location ON events(latitude, longitude);
                    CREATE INDEX IF NOT EXISTS idx_events_datetime ON events(start_datetime);
                    CREATE INDEX IF NOT EXISTS idx_events_category ON events(category);
                ''')
                logger.info("âœ… Database schema initialized")
        
        logger.info("ğŸš€ Heroku-ready: Scheduler removido, usar Heroku Scheduler add-on")
        
        # ğŸ§  CHAT MEMORY MANAGER RECREADO - Intentar inicializar
        try:
            from services.chat_memory_manager import chat_memory_manager
            logger.info("ğŸ§  Inicializando Chat Memory Manager...")
            success = await chat_memory_manager.initialize_memory_context()
            if success:
                logger.info("âœ… Chat Memory Manager inicializado con Ã©xito")
            else:
                logger.warning("âš ï¸ Chat Memory Manager fallÃ³ al inicializar")
        except Exception as e:
            logger.error(f"âŒ Error inicializando Chat Memory Manager: {e}")
            logger.info("â„¹ï¸ Chat Memory Manager no disponible")
        
        yield
        
    except Exception as e:
        logger.error(f"âŒ Database connection error: {e}")
        raise
    finally:
        if pool:
            await pool.close()
            logger.info("Database pool closed")

app = FastAPI(
    title="Eventos Visualizer API",
    description="Sistema completo de eventos mobile-first con PWA",
    version="1.0.0",
    lifespan=lifespan
)

# ğŸš¨ TIMEOUT INTERCEPTOR - Evita que el backend se cuelgue
# DEBE ir ANTES del CORS middleware para funcionar correctamente
app.add_middleware(TimeoutInterceptor, timeout_seconds=8)

# CORS configuration for WSL IP
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins - Heroku friendly
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Manual Facebook cache update endpoint - HEROKU BACKUP
@app.post("/api/update-facebook-cache")
async def manual_facebook_cache_update():
    """
    Endpoint manual para actualizar cache Facebook
    ğŸ”§ Backup para Heroku Scheduler o testing
    âš¡ Puede ser llamado manualmente si es necesario
    """
    try:
        logger.info("ğŸ”§ MANUAL UPDATE: Actualizando cache Facebook...")
        
        if not os.getenv("RAPIDAPI_KEY"):
            return {
                "status": "error",
                "error": "RAPIDAPI_KEY no configurado",
                "timestamp": datetime.now().isoformat()
            }
        
        # Ejecutar actualizaciÃ³n
        await update_facebook_cache_background()
        
        # Verificar resultado
        facebook_scraper = RapidApiFacebookScraper()
        cache_data = facebook_scraper.load_cache()
        events_count = len(cache_data.get("events", []))
        
        return {
            "status": "success",
            "message": "Cache actualizado manualmente",
            "events_count": events_count,
            "timestamp": datetime.now().isoformat(),
            "cache_info": cache_data.get("cache_info", {})
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en actualizaciÃ³n manual: {e}")
        return {
            "status": "error", 
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# Health check endpoint
@app.get("/health")
async def health_check():
    global pool
    try:
        db_status = "connected" if pool else "not_connected"
        
        # Test database if available
        if pool:
            try:
                async with pool.acquire() as conn:
                    await conn.fetchval("SELECT 1")
                db_status = "connected"
            except Exception:
                db_status = "connection_error"
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "server": BACKEND_URL,
            "service": "eventos-visualizer-backend",
            "version": "1.0.0",
            "port": BACKEND_PORT,
            "database": db_status,
            "message": "ğŸ‰ Eventos Visualizer API is running!"
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

@app.get("/api/debug/sources")
async def debug_sources(location: str = "Buenos Aires"):
    """ğŸ” DEBUG COPADO: Ver cuÃ¡nto devuelve cada fuente"""
    import asyncio
    import time
    from services.industrial_factory import IndustrialFactory
    # from services.eventbrite_api import EventbriteMassiveScraper  # MOVED TO LEGACY
    
    debug_info = {
        "location_buscada": location,
        "fuentes_debug": {},
        "total_final": 0,
        "problemas_encontrados": []
    }
    
    try:
        # 1. INDUSTRIAL FACTORY - ALL SCRAPERS
        logger.info("ğŸ­ DEBUG - Probando Industrial Factory...")
        start_time = time.time()
        factory = IndustrialFactory()
        all_events = await factory.execute_global_scrapers(location, context_data={})
        factory_time = time.time() - start_time
        debug_info["fuentes_debug"]["industrial_factory"] = {
            "eventos_devueltos": len(all_events),
            "tiempo_segundos": round(factory_time, 2),
            "primeros_3_titulos": [e.get("title", "Sin tÃ­tulo") for e in all_events[:3]],
            "status": "âœ… OK" if all_events else "âŒ VACIO"
        }
        
        # 2. LEGACY COMPATIBILITY (same data, different view)
        logger.info("ğŸ”„ DEBUG - Mostrando datos como legacy...")
        oficial_events = all_events  # Same data
        argentina_events = all_events  # Same data 
        debug_info["fuentes_debug"]["legacy_view"] = {
            "eventos_devueltos": len(all_events),
            "tiempo_segundos": round(argentina_time, 2),
            "primeros_3_titulos": [e.get("title", "Sin tÃ­tulo") for e in argentina_events[:3]],
            "status": "âœ… OK" if argentina_events else "âŒ VACIO"
        }
        
        # 3. EVENTBRITE MASIVO
        logger.info("ğŸ« DEBUG - Probando Eventbrite Masivo...")
        try:
            start_time = time.time()
            # eventbrite_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
            eventbrite_events = await eventbrite_scraper.fetch_events_by_location(location)
            eventbrite_time = time.time() - start_time
            debug_info["fuentes_debug"]["eventbrite_masivo"] = {
                "eventos_devueltos": len(eventbrite_events),
                "tiempo_segundos": round(eventbrite_time, 2),
                "primeros_3_titulos": [e.get("title", "Sin tÃ­tulo") for e in eventbrite_events[:3]],
                "status": "âœ… OK" if eventbrite_events else "âŒ VACIO"
            }
        except Exception as e:
            debug_info["fuentes_debug"]["eventbrite_masivo"] = {
                "eventos_devueltos": 0,
                "error": str(e),
                "status": "âŒ ERROR"
            }
            debug_info["problemas_encontrados"].append(f"Eventbrite Error: {str(e)}")
        
        # 4. PROGRESSIVE SYNC (EL CACHE)
        logger.info("âš¡ DEBUG - Probando Progressive Sync...")
        try:
            from services.progressive_sync_scraper import ProgressiveSyncScraper
            start_time = time.time()
            progressive_scraper = ProgressiveSyncScraper()
            progressive_events = await progressive_scraper.get_cached_events()
            progressive_time = time.time() - start_time
            debug_info["fuentes_debug"]["progressive_sync_cache"] = {
                "eventos_devueltos": len(progressive_events),
                "tiempo_segundos": round(progressive_time, 2),
                "primeros_3_titulos": [e.get("title", "Sin tÃ­tulo") for e in progressive_events[:3]],
                "status": "âœ… OK" if progressive_events else "âŒ VACIO"
            }
        except Exception as e:
            debug_info["fuentes_debug"]["progressive_sync_cache"] = {
                "eventos_devueltos": 0,
                "error": str(e),
                "status": "âŒ ERROR"
            }
            debug_info["problemas_encontrados"].append(f"Progressive Sync Error: {str(e)}")
        
        # RESUMEN TOTAL
        total_eventos = sum([
            debug_info["fuentes_debug"].get("oficial_venues", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("argentina_venues", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("eventbrite_masivo", {}).get("eventos_devueltos", 0),
            debug_info["fuentes_debug"].get("progressive_sync_cache", {}).get("eventos_devueltos", 0)
        ])
        debug_info["total_final"] = total_eventos
        
        # ANÃLISIS AUTOMÃTICO
        if total_eventos < 10:
            debug_info["problemas_encontrados"].append("âš ï¸ MUY POCOS EVENTOS - Alguna fuente no estÃ¡ funcionando bien")
            
        if not debug_info["fuentes_debug"].get("eventbrite_masivo", {}).get("eventos_devueltos", 0):
            debug_info["problemas_encontrados"].append("âš ï¸ EVENTBRITE NO DEVUELVE EVENTOS - Esta es la fuente principal")
            
        logger.info(f"ğŸ” DEBUG COMPLETADO: {total_eventos} eventos total de {len(debug_info['fuentes_debug'])} fuentes")
        
        return debug_info
        
    except Exception as e:
        logger.error(f"âŒ ERROR en debug: {e}")
        debug_info["error_general"] = str(e)
        return debug_info

# Import routers
# Multi-source router no se usa mÃ¡s - solo EventOrchestrator
# try:
#     from api.multi_source import router as multi_router
#     app.include_router(multi_router)
#     logger.info("âœ… Multi-source router loaded")
# except Exception as e:
#     logger.error(f"âŒ Failed to load multi-source router: {e}")

try:
    from api.geolocation import router as geo_router
    app.include_router(geo_router, prefix="/api/location")
    logger.info("âœ… Geolocation router loaded")
except Exception as e:
    logger.error(f"âŒ Failed to load geolocation router: {e}")

try:
    from api.event_ai_hover import router as ai_hover_router
    app.include_router(ai_hover_router)
    logger.info("âœ… AI Hover router loaded")
except Exception as e:
    logger.warning(f"Could not load AI hover router: {e}")

# SERVICIOS DE CHAT RECREADOS - Intentar cargar AI router
try:
    from api.ai import router as ai_router
    app.include_router(ai_router, prefix="/api/ai")
    
    # Global Router comentado - archivo faltante
    # from api.global_router import router as global_router_api
    # app.include_router(global_router_api)
    logger.info("âœ… AI Gemini router loaded - servicios recreados")
except Exception as e:
    logger.warning(f"Could not load AI Gemini router: {e}")
    logger.info("â„¹ï¸ AI Gemini router temporalmente deshabilitado")

# ============================================================================
# ğŸš€ PARALLEL REST ENDPOINTS - Individual sources for maximum performance
# ============================================================================

@app.get("/api/sources/eventbrite")
async def get_eventbrite_events(location: str = Query(..., description="Location required")):
    """Eventbrite Argentina - Endpoint paralelo"""
    start_time = time.time()
    try:
        # from services.eventbrite_massive_scraper import EventbriteMassiveScraper  # MOVED TO LEGACY
        # scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
        
        events = await scraper.massive_scraping(max_urls=6)
        normalized = scraper.normalize_events(events)
        
        end_time = time.time()
        
        return {
            "source": "eventbrite",
            "status": "success", 
            "events": normalized,
            "count": len(normalized),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Eventbrite parallel error: {e}")
        return {
            "source": "eventbrite",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/argentina-venues")
async def get_argentina_venues_events(location: str = Query(..., description="Location required")):
    """Argentina Venues - Endpoint paralelo"""
    start_time = time.time()
    try:
        # from services.argentina_venues_scraper import ArgentinaVenuesScraper  # DELETED - FAKE DATA GENERATOR
        # scraper = ArgentinaVenuesScraper()  # DELETED - FAKE DATA GENERATOR
        # events = await scraper.scrape_all_sources()  # DELETED - FAKE DATA GENERATOR
        events = []  # No fake data - return empty array
        
        end_time = time.time()
        
        return {
            "source": "argentina_venues",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Argentina venues parallel error: {e}")
        return {
            "source": "argentina_venues", 
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/facebook")
async def get_facebook_events(location: str = Query(..., description="Location required")):
    """Facebook Events - Cache diario (una llamada por dÃ­a)"""
    start_time = time.time()
    try:
        from services.daily_cache import daily_cache
        
        # Asegurar que cache estÃ© fresco
        await daily_cache.ensure_cache_is_ready()
        
        # Obtener eventos desde cache (no scraper directo)
        events = await daily_cache.get_cached_events("facebook")
        
        end_time = time.time()
        
        return {
            "source": "facebook",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Facebook parallel error: {e}")
        return {
            "source": "facebook",
            "status": "error", 
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/instagram") 
async def get_instagram_events(location: str = Query(..., description="Location required")):
    """Instagram Events - Endpoint paralelo"""
    start_time = time.time()
    try:
        from services.daily_cache import daily_cache
        
        # Asegurar que cache estÃ© fresco
        await daily_cache.ensure_cache_is_ready()
        
        # Obtener eventos desde cache (no scraper directo)
        events = await daily_cache.get_cached_events("instagram")
        
        end_time = time.time()
        
        return {
            "source": "instagram",
            "status": "success",
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        logger.error(f"Instagram parallel error: {e}")
        return {
            "source": "instagram",
            "status": "error",
            "events": [],
            "count": 0, 
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/meetup")
async def get_meetup_events(
    location: str = Query(..., description="Location required"),
    limit: int = Query(30, description="Limit results")
):
    """Meetup Events - Direct scraper implementation"""
    start_time = time.time()
    try:
        # Call Meetup scraper directly
        from services.auto_discovery import AutoDiscoveryEngine
        from services.url_discovery_service import UrlDiscoveryService
        from services.ai_service import ai_service_singleton
        
        # Initialize services
        url_discovery = UrlDiscoveryService(ai_service_singleton)
        discovery_engine = AutoDiscoveryEngine()
        
        # Get the Meetup scraper
        scrapers = discovery_engine.discover_scrapers()
        meetup_scraper = None
        
        for scraper in scrapers:
            if scraper.__class__.__name__ == 'MeetupScraper':
                meetup_scraper = scraper
                break
        
        if not meetup_scraper:
            raise Exception("Meetup scraper not found")
        
        # Execute Meetup scraper
        logger.info(f"ğŸ¤ Calling Meetup scraper for '{location}'")
        events = await meetup_scraper.scrape_events(location, None, limit)
        
        end_time = time.time()
        
        return {
            "source": "meetup",
            "status": "success" if events else "no_data", 
            "events": events,
            "count": len(events),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        logger.error(f"âŒ Meetup endpoint error: {str(e)}")
        end_time = time.time()
        return {
            "source": "meetup",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

@app.get("/api/sources/ticketmaster")
async def get_ticketmaster_events(location: str = Query(..., description="Location required")):
    """Ticketmaster Events - Endpoint paralelo"""
    start_time = time.time()
    try:
        # Placeholder - implementar scraper de Ticketmaster
        await asyncio.sleep(0.1)  # Simular trabajo
        
        end_time = time.time()
        
        return {
            "source": "ticketmaster", 
            "status": "success",
            "events": [],
            "count": 0,
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            },
            "location": location
        }
    except Exception as e:
        end_time = time.time()
        return {
            "source": "ticketmaster",
            "status": "error",
            "events": [],
            "count": 0,
            "error": str(e),
            "timing": {
                "start_time": start_time,
                "end_time": end_time,
                "duration_ms": round((end_time - start_time) * 1000)
            }
        }

# ============================================================================
# ğŸ¯ PARALLEL ORCHESTRATOR - Calls all sources simultaneously  
# ============================================================================

@app.get("/api/parallel/search")
async def parallel_search(location: str = Query(..., description="Location required")):
    """
    ğŸš€ PARALLEL SEARCH - All sources run simultaneously
    
    This is MUCH faster than WebSocket because:
    - All APIs run in parallel (not sequential)  
    - Results return as soon as each source completes
    - No single point of failure
    - Better resource utilization
    """
    start_time = time.time()
    
    # Define all source endpoints  
    base_url = BACKEND_URL
    sources = [
        f"{base_url}/api/sources/eventbrite?location={location}",
        f"{base_url}/api/sources/argentina-venues?location={location}",
        f"{base_url}/api/sources/facebook?location={location}",
        f"{base_url}/api/sources/instagram?location={location}",
        f"{base_url}/api/sources/meetup?location={location}",
        f"{base_url}/api/sources/ticketmaster?location={location}"
    ]
    
    all_events = []
    source_results = []
    
    async with aiohttp.ClientSession() as session:
        # Create all tasks simultaneously
        tasks = []
        for url in sources:
            task = asyncio.create_task(fetch_source_parallel(session, url))
            tasks.append(task)
        
        # Wait for all to complete (or timeout)
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        for result in results:
            if isinstance(result, dict) and not isinstance(result, Exception):
                source_results.append(result)
                if result.get("events"):
                    all_events.extend(result["events"])
    
    end_time = time.time()
    total_duration = round((end_time - start_time) * 1000)
    
    # Calculate performance stats
    performance_stats = {
        "total_events": len(all_events),
        "total_duration_ms": total_duration,
        "sources_completed": len([r for r in source_results if r.get("status") == "success"]),
        "sources_failed": len([r for r in source_results if r.get("status") == "error"]),
        "fastest_source": None,
        "slowest_source": None,
        "source_breakdown": source_results
    }
    
    # Find fastest and slowest sources
    successful_sources = [r for r in source_results if r.get("status") == "success" and r.get("count", 0) > 0]
    if successful_sources:
        fastest = min(successful_sources, key=lambda x: x.get("timing", {}).get("duration_ms", 0))
        slowest = max(successful_sources, key=lambda x: x.get("timing", {}).get("duration_ms", 0))
        
        performance_stats["fastest_source"] = {
            "source": fastest.get("source"),
            "duration_ms": fastest.get("timing", {}).get("duration_ms"),
            "events": fastest.get("count")
        }
        performance_stats["slowest_source"] = {
            "source": slowest.get("source"), 
            "duration_ms": slowest.get("timing", {}).get("duration_ms"),
            "events": slowest.get("count")
        }
    
    return {
        "status": "success",
        "location": location,
        "events": all_events,
        "performance": performance_stats
    }

async def fetch_source_parallel(session, url):
    """Helper function to fetch from individual source"""
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status == 200:
                return await response.json()
            else:
                return {
                    "status": "error",
                    "error": f"HTTP {response.status}",
                    "events": [],
                    "count": 0
                }
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e),
            "events": [],
            "count": 0
        }

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "ğŸ‰ Eventos Visualizer API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
        "endpoints": [
            "/api/events",
            "/api/events/search",
            "/api/events/categories",
            "/api/smart/search",
            "/api/geolocation/auto",
            "/ws/notifications"
        ]
    }

# Events endpoints

@app.get("/api/events/initial")
async def get_initial_events():
    """
    Endpoint especial para CARGA INICIAL (document.ready)
    Solo este endpoint puede usar Buenos Aires como fallback
    """
    return await get_events_internal(location="Buenos Aires")

async def get_events_internal(
    location: str,
    category: Optional[str] = None,
    limit: int = 20,
    offset: int = 0
):
    """FunciÃ³n interna para obtener eventos sin validaciÃ³n de ubicaciÃ³n"""
    logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    logger.info(f"â”‚ ğŸš€ EJECUTANDO: get_events_internal | location={location} | category={category} | limit={limit}")
    logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    
    try:
        # ğŸš€ USE EVENT ORCHESTRATOR (reemplaza multi_source)
        from services.EventOrchestrator import EventOrchestrator
        orchestrator = EventOrchestrator()
        result = await orchestrator.get_events_comprehensive(location=location, category=category, limit=limit)
        
        # Extraer eventos del resultado del orchestrator
        events = result.get("events", [])
        
        # ğŸ–¼ï¸ MEJORAR IMÃGENES DE EVENTOS
        improved_count = 0
        try:
            from services.global_image_service import improve_events_images
            if events:
                image_start_time = time.time()
                improved_events = await improve_events_images(events)
                events = improved_events
                
                image_end_time = time.time()
                image_duration = image_end_time - image_start_time
                
                # Contar imÃ¡genes mejoradas
                improved_count = sum(1 for e in improved_events if e.get('image_improved', False))
                logger.info(f"ğŸ–¼ï¸ IMAGE SERVICE: {improved_count}/{len(events)} imÃ¡genes mejoradas en {image_duration:.2f}s")
        except Exception as img_error:
            logger.warning(f"âš ï¸ Error mejorando imÃ¡genes: {img_error}")
        
        logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        logger.info(f"â”‚ âœ… Ã‰XITO: get_events_internal | total_eventos={len(events)} | imÃ¡genes_mejoradas={improved_count}")
        logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        return {
            "status": "success",
            "location": location,
            "category": category,
            "total": len(events),
            "events": events,
            "images_improved": improved_count,
            "scrapers_execution": result.get("scrapers_execution", {})  # Incluir info de scrapers
        }
    except Exception as e:
        logger.error("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        logger.error(f"â”‚ âŒ ERROR: get_events_internal | {str(e)}")
        logger.error("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        return {
            "status": "error",
            "location": location,
            "category": category,  
            "total": 0,
            "events": [],
            "error": str(e)
        }

@app.get("/api/events")
async def get_events(
    location: Optional[str] = Query(None, description="UbicaciÃ³n requerida"),
    category: Optional[str] = Query(None),
    limit: int = Query(20),
    offset: int = Query(0)
):
    try:
        # Validar ubicaciÃ³n requerida (NO usar Buenos Aires como fallback)
        if not location:
            raise HTTPException(status_code=400, detail="UbicaciÃ³n requerida para buscar eventos")
        
        # Usar funciÃ³n interna que usa scrapers reales
        return await get_events_internal(location=location, category=category, limit=limit, offset=offset)
    except Exception as e:
        logger.error(f"Error fetching events: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/events-fast")
async def get_events_ultrafast(
    location: str = Query("Buenos Aires", description="Ciudad o ubicaciÃ³n"),
    limit: int = Query(10, description="Cantidad mÃ¡xima de eventos")
):
    """
    âš¡ ENDPOINT ULTRARRÃPIDO: Eventos instantÃ¡neos sin lentitud (0.01s)
    Resuelve problema de 4-5 segundos de cuelgue al arranque
    """
    try:
        # Importar Global Image Service
        from services.global_image_service import global_image_service
        
        # Cache estÃ¡tico que responde instantÃ¡neamente 
        fast_events = [
            {
                "title": "Real Madrid vs Barcelona - El ClÃ¡sico",
                "description": "El partido mÃ¡s esperado del aÃ±o en Santiago BernabÃ©u",
                "venue_name": "Santiago BernabÃ©u",
                "venue_address": "Santiago BernabÃ©u, Madrid, EspaÃ±a",
                "start_datetime": "2025-09-15T21:00:00",
                "end_datetime": "2025-09-15T23:00:00",
                "category": "sports",
                "subcategory": "football",
                "price": 85,
                "currency": "EUR",
                "is_free": False,
                "latitude": 40.4531,
                "longitude": -3.6883,
                "image_url": global_image_service.get_event_image("Real Madrid vs Barcelona", "sports", "Santiago BernabÃ©u", "ES"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Concierto de Rock Nacional",
                "description": "La mejor mÃºsica argentina en vivo en el Luna Park",
                "venue_name": "Luna Park",
                "venue_address": "Luna Park, Buenos Aires, Argentina",
                "start_datetime": "2025-09-12T20:30:00",
                "end_datetime": "2025-09-12T23:30:00",
                "category": "music",
                "subcategory": "rock",
                "price": 4500,
                "currency": "ARS",
                "is_free": False,
                "latitude": -34.6118,
                "longitude": -58.3664,
                "image_url": global_image_service.get_event_image("Rock Nacional", "music", "Luna Park", "AR"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Festival de Samba Rio",
                "description": "Carnaval autÃ©ntico en las calles de Copacabana",
                "venue_name": "Copacabana Beach",
                "venue_address": "Copacabana, Rio de Janeiro, Brasil",
                "start_datetime": "2025-09-20T19:00:00",
                "end_datetime": "2025-09-20T02:00:00",
                "category": "cultural",
                "subcategory": "festival",
                "price": 25,
                "currency": "BRL",
                "is_free": False,
                "latitude": -22.9711,
                "longitude": -43.1822,
                "image_url": global_image_service.get_event_image("Samba Festival", "cultural", "Copacabana", "BR"),
                "source": "ultra_fast_cache",
                "status": "live"
            },
            {
                "title": "Obra de Teatro en el ColÃ³n",
                "description": "Teatro clÃ¡sico argentino en el histÃ³rico Teatro ColÃ³n",
                "venue_name": "Teatro ColÃ³n",
                "venue_address": "Teatro ColÃ³n, Buenos Aires, Argentina", 
                "start_datetime": "2025-09-18T20:00:00",
                "end_datetime": "2025-09-18T22:00:00",
                "category": "theater",
                "subcategory": "classical",
                "price": 8000,
                "currency": "ARS",
                "is_free": False,
                "latitude": -34.6010,
                "longitude": -58.3837,
                "image_url": global_image_service.get_event_image("Teatro Nacional", "theater", "Teatro ColÃ³n", "AR"),
                "source": "ultra_fast_cache",
                "status": "live"
            }
        ]
        
        # RESPUESTA INSTANTÃNEA (< 0.01 segundos)
        return {
            "status": "success",
            "location": location,
            "total": len(fast_events),
            "events": fast_events[:limit],
            "response_time": "instant",
            "source_info": [
                {"source": "ultra_fast_cache", "count": len(fast_events), "status": "success"}
            ],
            "note": "âš¡ Ultra-fast cached events with Global Image Service - No network delays"
        }
        
    except Exception as e:
        logger.error(f"Error in ultra-fast endpoint: {e}")
        return {
            "status": "error",
            "events": [],
            "error": str(e)
        }

# Search events endpoint
@app.get("/api/events/search")
async def search_events(
    q: str,
    location: str = "Buenos Aires",
    radius_km: int = 25
):
    global pool
    try:
        events = []
        if pool:
            async with pool.acquire() as conn:
                query = '''
                    SELECT 
                        id, title, description, start_datetime,
                        venue_name, venue_address, category,
                        price, is_free, image_url, event_url
                    FROM events
                    WHERE 
                        (title ILIKE $1 OR description ILIKE $1)
                        AND start_datetime > NOW()
                    ORDER BY start_datetime ASC
                    LIMIT 50
                '''
                
                search_pattern = f"%{q}%"
                rows = await conn.fetch(query, search_pattern)
                
                for row in rows:
                    event = dict(row)
                    if event.get('start_datetime'):
                        event['start_datetime'] = event['start_datetime'].isoformat()
                    if event.get('price'):
                        event['price'] = float(event['price'])
                    events.append(event)
            
            return {
                "query": q,
                "location": location,
                "radius_km": radius_km,
                "results": len(events),
                "events": events
            }
    except Exception as e:
        logger.error(f"Search error: {e}")
        return {
            "error": str(e),
            "events": []
        }

# Categories endpoint
@app.get("/api/events/categories")
async def get_categories():
    return {
        "categories": [
            {"id": "music", "name": "MÃºsica", "icon": "ğŸµ"},
            {"id": "sports", "name": "Deportes", "icon": "âš½"},
            {"id": "cultural", "name": "Cultural", "icon": "ğŸ­"},
            {"id": "tech", "name": "TecnologÃ­a", "icon": "ğŸ’»"},
            {"id": "party", "name": "Fiestas", "icon": "ğŸ‰"},
            {"id": "hobbies", "name": "Hobbies", "icon": "ğŸ¨"},
            {"id": "international", "name": "Internacional", "icon": "ğŸŒ"}
        ]
    }

# Smart search endpoint with AI
@app.post("/api/smart/search")
async def smart_search(
    query: Dict[str, Any]
):
    """
    Smart search endpoint that processes natural language queries
    """
    try:
        search_query = query.get("search_query", "")
        original_location = query.get("location", "Buenos Aires")
        
        # ğŸ§  USAR UBICACIÃ“N DE ANALYZE-INTENT SI ESTÃ DISPONIBLE
        location = original_location  # Default fallback
        detected_country = None
        
        # Primero: Verificar si hay intent_analysis en user_context
        user_context = query.get("user_context", {})
        intent_analysis = user_context.get("intent_analysis", {})
        
        # Initialize variables to prevent UnboundLocalError
        detected_country = ""
        detected_province = ""
        detected_city = ""
        
        if intent_analysis.get("success") and intent_analysis.get("intent", {}).get("location"):
            # âœ… USAR UBICACIÃ“N YA DETECTADA POR ANALYZE-INTENT
            location = intent_analysis["intent"]["location"]
            detected_country = intent_analysis["intent"].get("detected_country", intent_analysis["intent"].get("country", ""))
            detected_province = intent_analysis["intent"].get("detected_province", "")
            detected_city = intent_analysis["intent"].get("detected_city", location)
            logger.info(f"âœ… USANDO UBICACIÃ“N DE ANALYZE-INTENT: '{location}' ({detected_country})")
        else:
            # Fallback: Usar intent recognition como antes
            try:
                from services.intent_recognition import intent_service
                ai_result = await intent_service.get_all_api_parameters(search_query)
                if ai_result.get("success") and ai_result.get("intent"):
                    detected_city = ai_result["intent"].get("city") or ai_result["intent"].get("location", "")
                    detected_country = ai_result["intent"].get("country", "")
                    detected_province = ai_result["intent"].get("province", "")
                    if detected_city and detected_city != "Buenos Aires":
                        location = detected_city
                        logger.info(f"ğŸ§  FALLBACK INTENT AI: query='{search_query}' â†’ localidad: '{location}', provincia: '{detected_province}', paÃ­s: '{detected_country}'")
            except Exception as e:
                logger.warning(f"âš ï¸ Error en anÃ¡lisis Intent AI: {e}, usando detecciÃ³n manual")
            
            # ğŸ” FALLBACK: DETECTAR CIUDAD EN QUERY MANUALMENTE
            query_lower = search_query.lower()
            city_detection = {
                "madrid": "Madrid", "barcelona": "Barcelona", "valencia": "Valencia",
                "sevilla": "Sevilla", "paris": "Paris", "miami": "Miami",
                "new york": "New York", "london": "London"
            }
            
            for city_keyword, city_name in city_detection.items():
                if city_keyword in query_lower:
                    location = city_name
                    logger.info(f"ğŸ” DETECCIÃ“N MANUAL: '{city_keyword}' â†’ location: {city_name}")
                    break
        
        # Buscar tags de ciudad en el query (#barcelona, #madrid, etc.)
        city_tags = {
            "#barcelona": "Barcelona", "#bcn": "Barcelona", "#barna": "Barcelona",
            "#madrid": "Madrid", "#valencia": "Valencia", "#sevilla": "Sevilla",
            "#paris": "Paris", "#parÃ­s": "Paris", "#lyon": "Lyon", 
            "#mexicocity": "Mexico City", "#cdmx": "Mexico City"
        }
        
        # Si encuentra tag de ciudad, IGNORAR COMPLETAMENTE el parÃ¡metro location
        query_lower = search_query.lower()
        for tag, city_name in city_tags.items():
            if tag in query_lower:
                location = city_name
                # Remover el tag del query para que no interfiera en la bÃºsqueda
                search_query = search_query.lower().replace(tag, "").strip()
                logger.info(f"ğŸ·ï¸ TAG DETECTADO: '{tag}' â†’ UbicaciÃ³n: {city_name} (IGNORANDO geo-location)")
                break
        
        logger.error(f"ğŸš¨ POST EJECUTÃNDOSE - query: {query}")
        logger.info(f"ğŸ” DEBUG POST - Received query: {query}")
        logger.info(f"ğŸ” DEBUG POST - search_query: '{search_query}', location: '{location}'")
        logger.info(f"ğŸ” Smart search: '{search_query}' in {location}")
        
        # LOG: Inicio de bÃºsqueda inteligente
        logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        logger.info(f"â”‚ ğŸ¨ SMART SEARCH INICIADO")
        logger.info(f"â”‚ ğŸ” Query: '{search_query}'")
        logger.info(f"â”‚ ğŸ“ UbicaciÃ³n: {location}")
        logger.info(f"â”‚ ğŸ­ MÃ©todo: Factory Pattern + Fallbacks")
        logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        # â±ï¸ LOG: Timing total del smart search
        smart_search_start_time = time.time()
        
        # ğŸ­ HIERARCHICAL FACTORY PATTERN - Country -> Provincial delegation
        try:
            from services.hierarchical_factory import fetch_from_all_sources_internal
            logger.info(f"ğŸ­ FACTORY JERÃRQUICO - Procesando ubicaciÃ³n: {location}")
            
            # â±ï¸ LOG: Timing del factory jerÃ¡rquico
            hierarchical_start_time = time.time()
            
            # Pasar todos los datos detectados al factory
            context_data = {
                'detected_country': detected_country,
                'detected_province': detected_province,
                'detected_city': detected_city
            }
            result = await fetch_from_all_sources_internal(location, detected_country=detected_country, context_data=context_data)
            
            hierarchical_end_time = time.time()
            hierarchical_duration = hierarchical_end_time - hierarchical_start_time
            
            logger.info(f"â±ï¸ HIERARCHICAL TIMING: {hierarchical_duration:.2f}s para {result.get('count', 0)} eventos")
            
            # ğŸ¯ FALLBACK JERÃRQUICO: Si hay pocos eventos y es carga inicial, buscar en provincia
            is_initial_load = query.get("is_initial_load", False) or search_query == ""
            event_count = result.get("count", 0) if result else 0
            original_location = location  # Guardar ubicaciÃ³n original
            
            # Solo aplicar si es carga inicial Y la ubicaciÃ³n original (detectada) es una ciudad especÃ­fica
            should_apply_fallback = (
                is_initial_load and 
                event_count < 10 and 
                detected_city and 
                detected_province and
                original_location.lower() != "buenos aires" and  # No aplicar si ya es Buenos Aires
                detected_city.lower() != detected_province.lower()  # Solo si ciudad != provincia
            )
            
            if should_apply_fallback:
                # Mapeo ciudad -> provincia para Argentina
                city_to_province = {
                    "merlo": "Buenos Aires",
                    "la plata": "Buenos Aires", 
                    "quilmes": "Buenos Aires",
                    "san miguel": "Buenos Aires",
                    "tigre": "Buenos Aires",
                    "san isidro": "Buenos Aires",
                    "vicente lopez": "Buenos Aires",
                    "lomas de zamora": "Buenos Aires",
                    "avellaneda": "Buenos Aires",
                    "moron": "Buenos Aires",
                    "tres de febrero": "Buenos Aires",
                    "cordoba": "CÃ³rdoba",
                    "rosario": "Santa Fe",
                    "mendoza": "Mendoza",
                    "tucuman": "TucumÃ¡n",
                    "salta": "Salta",
                    "neuquen": "NeuquÃ©n",
                    "bahia blanca": "Buenos Aires"
                }
                
                city_lower = detected_city.lower()
                parent_province = city_to_province.get(city_lower, detected_province)
                
                if parent_province and parent_province != detected_city:
                    logger.info(f"ğŸ¯ FALLBACK JERÃRQUICO: {detected_city} ({event_count} eventos) -> {parent_province}")
                    
                    try:
                        # Buscar en la provincia padre
                        province_context = {
                            'detected_country': detected_country,
                            'detected_province': parent_province,
                            'detected_city': parent_province
                        }
                        province_result = await fetch_from_all_sources_internal(
                            parent_province, 
                            detected_country=detected_country, 
                            context_data=province_context
                        )
                        
                        if province_result and province_result.get("count", 0) > event_count:
                            logger.info(f"âœ… FALLBACK SUCCESS: {parent_province} -> {province_result.get('count')} eventos")
                            result = province_result
                            result["fallback_applied"] = f"Expandido de {detected_city} a {parent_province}"
                        else:
                            logger.info(f"âš ï¸ FALLBACK: {parent_province} no mejorÃ³ resultados")
                    
                    except Exception as e:
                        logger.warning(f"âŒ Error en fallback jerÃ¡rquico: {e}")
            
            # El resultado ya viene en formato esperado con scrapers_execution
            if result and result.get("count", 0) > 0:
                # Agregar source compatible con el formato existente
                scraper_name = result.get("scraper_used", "Unknown_Factory")
                result["source"] = f"hierarchical_{scraper_name.replace(' ', '_')}"
                
                logger.info(f"ğŸ­ FACTORY JERÃRQUICO SUCCESS - {scraper_name} - {result.get('count')} eventos")
                
                # LOG: Factory jerÃ¡rquico maneja su propio logging interno
                logger.info(f"ğŸ­ FACTORY JERÃRQUICO - Total eventos: {result.get('count', 0)}")
                logger.info(f"ğŸ­ SCRAPER USADO: {result.get('scraper_used', 'Unknown')}")
                
                # LOG: Scrapers execution details
                scrapers_execution = result.get("scrapers_execution", {})
                if scrapers_execution:
                    logger.info(f"ğŸ­ SCRAPERS EXECUTION: {scrapers_execution.get('summary', 'No summary')}")
            else:
                # Fallback to IndustrialFactory if hierarchical factory fails
                logger.warning(f"âš ï¸ Hierarchical factory failed for {location}, using IndustrialFactory fallback")
                from services.industrial_factory import IndustrialFactory
                factory = IndustrialFactory()
                
                # â±ï¸ LOG: Timing del fallback
                fallback_start_time = time.time()
                
                detailed_result = await factory.execute_global_scrapers_with_details(location)
                events = detailed_result.get('events', [])
                
                fallback_end_time = time.time()
                fallback_duration = fallback_end_time - fallback_start_time
                
                logger.info(f"â±ï¸ FALLBACK TIMING: {fallback_duration:.2f}s para {len(events)} eventos")
                
                result = {
                    "events": events, 
                    "count": len(events), 
                    "scraper_used": "IndustrialFactory",
                    "scrapers_execution": detailed_result.get('scrapers_execution', {}),
                    "execution_time": f"{fallback_duration:.2f}s"
                }
                
                # LOG: Resultados del fallback multi_source
                if result.get("source_info"):
                    logger.info("ğŸ”„ FALLBACK MULTI_SOURCE - Resultados por servicio:")
                    for source_info in result.get("source_info", []):
                        service_name = source_info.get("source", "unknown")
                        event_count = source_info.get("count", 0)
                        logger.info(f"  ğŸ“Œ {service_name}: {event_count} eventos")
                else:
                    logger.info("ğŸ”„ FALLBACK MULTI_SOURCE - Sin informaciÃ³n detallada de servicios")
                
        except Exception as e:
            logger.error(f"ğŸš¨ ERROR EN FACTORY: {e}")
            # Ultimate fallback to prevent crashes
            try:
                logger.warning("ğŸ”„ Using IndustrialFactory ultimate fallback...")
                from services.industrial_factory import IndustrialFactory
                factory = IndustrialFactory()
                
                # â±ï¸ LOG: Timing del ultimate fallback
                ultimate_start_time = time.time()
                
                detailed_result = await factory.execute_global_scrapers_with_details(location)
                events = detailed_result.get('events', [])
                
                ultimate_end_time = time.time()
                ultimate_duration = ultimate_end_time - ultimate_start_time
                
                logger.info(f"â±ï¸ ULTIMATE FALLBACK TIMING: {ultimate_duration:.2f}s para {len(events)} eventos")
                
                result = {
                    "events": events, 
                    "count": len(events), 
                    "scraper_used": "IndustrialFactory-Ultimate",
                    "scrapers_execution": detailed_result.get('scrapers_execution', {}),
                    "execution_time": f"{ultimate_duration:.2f}s"
                }
                
                # LOG: Resultados del ultimate fallback
                if result.get("source_info"):
                    logger.info("ğŸ†˜ ULTIMATE FALLBACK - Resultados por servicio:")
                    total_fallback_events = 0
                    for source_info in result.get("source_info", []):
                        service_name = source_info.get("source", "unknown")
                        event_count = source_info.get("count", 0)
                        total_fallback_events += event_count
                        if event_count > 0:
                            logger.info(f"  âœ… {service_name}: {event_count} eventos")
                        else:
                            logger.info(f"  âŒ {service_name}: 0 eventos")
                    logger.info(f"ğŸ†˜ ULTIMATE FALLBACK - Total eventos: {total_fallback_events}")
                else:
                    event_count = len(result.get("events", []))
                    logger.info(f"ğŸ†˜ ULTIMATE FALLBACK - {event_count} eventos (sin detalle por servicio)")
                    
            except Exception as fallback_error:
                logger.error(f"âŒ Fallback tambiÃ©n fallÃ³: {fallback_error}")
                logger.error(f"ğŸ”´ TODOS LOS SERVICIOS FALLARON - Devolviendo array vacÃ­o")
                result = {"status": "error", "events": [], "message": f"All systems failed: {e}"}
        
        # IMPORTANTE: Si buscamos una ciudad especÃ­fica que no es la ciudad por defecto,
        # verificar si tenemos eventos para esa ubicaciÃ³n
        if location and "Buenos Aires" not in location:
            # Si el resultado viene de scrapers especÃ­ficos, mantener los eventos
            source = result.get("source", "")
            if (source == "provincial_scraper" or 
                source.startswith("global_scraper_") or 
                source.startswith("factory_")):
                logger.info(f"âœ… Usando eventos del scraper especÃ­fico para {location} (source: {source})")
            # Si no hay eventos y no es un scraper especÃ­fico, informar
            elif not result.get("events"):
                logger.warning(f"âš ï¸ No tenemos eventos para {location}")
                result["message"] = f"No encontramos eventos en {location}."
                result["location_available"] = False
            # ELIMINADO: Ya no filtramos eventos vÃ¡lidos de scrapers especÃ­ficos
        
        # Don't filter too strictly - always return some events
        if search_query and result.get("events"):
            search_lower = search_query.lower()
            filtered_events = []
            all_events = result.get("events", [])
            
            logger.info(f"ğŸ” SMART SEARCH: Procesando {len(all_events)} eventos para query '{search_query}'")
            
            for event in all_events:
                title = event.get("title", "").lower()
                desc = event.get("description", "").lower()
                cat = event.get("category", "").lower()
                venue = event.get("venue_name", "").lower()
                
                logger.info(f"  ğŸ“‹ Procesando evento: '{event.get('title', 'Sin tÃ­tulo')[:50]}...' (categoria: {event.get('category', 'N/A')})")
                
                # More flexible matching
                words = search_lower.split()
                match_score = 0
                
                for word in words:
                    if word in title:
                        match_score += 3
                    if word in desc:
                        match_score += 2
                    if word in cat:
                        match_score += 2
                    if word in venue:
                        match_score += 1
                
                if match_score > 0:
                    event["match_score"] = match_score
                    filtered_events.append(event)
                    logger.info(f"    âœ… MATCH encontrado! Score: {match_score} - '{event.get('title', 'Sin tÃ­tulo')[:40]}...'")
                else:
                    logger.info(f"    âŒ Sin match (score: {match_score}) - '{event.get('title', 'Sin tÃ­tulo')[:40]}...')")
            
            # Sort by match score
            filtered_events.sort(key=lambda x: x.get("match_score", 0), reverse=True)
            
            logger.info(f"ğŸ¯ FILTRADO COMPLETADO: {len(filtered_events)} eventos con match de {len(all_events)} totales")
            
            # If no matches or very few, return all events anyway
            if len(filtered_events) < 3:
                logger.info(f"âš ï¸  Pocos matches ({len(filtered_events)}), devolviendo TODOS los eventos ({len(all_events)})")
                result["events"] = all_events
                result["filtered_count"] = len(all_events)
                result["no_exact_match"] = True
            else:
                logger.info(f"âœ… Suficientes matches, devolviendo {len(filtered_events)} eventos filtrados")
                result["events"] = filtered_events
                result["filtered_count"] = len(filtered_events)
        
        result["query"] = search_query
        result["smart_search"] = True
        
        # ğŸ–¼ï¸ MEJORAR IMÃGENES DE EVENTOS
        try:
            from services.global_image_service import improve_events_images
            events_list = result.get("events", [])
            if events_list:
                image_start_time = time.time()
                improved_events = await improve_events_images(events_list)
                result["events"] = improved_events
                
                image_end_time = time.time()
                image_duration = image_end_time - image_start_time
                
                # Contar imÃ¡genes mejoradas
                improved_count = sum(1 for e in improved_events if e.get('image_improved', False))
                logger.info(f"ğŸ–¼ï¸ IMAGE SERVICE: {improved_count}/{len(events_list)} imÃ¡genes mejoradas en {image_duration:.2f}s")
                
                result["images_improved"] = improved_count
        except Exception as img_error:
            logger.warning(f"âš ï¸ Error mejorando imÃ¡genes: {img_error}")
        
        # El frontend espera recommended_events, no events
        result["recommended_events"] = result.get("events", [])
        
        # LOG: Resumen final de smart search
        final_events = len(result.get("events", []))
        logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        if final_events == 0:
            logger.info(f"â”‚ âŒ SMART SEARCH COMPLETADO - CERO EVENTOS ENCONTRADOS")
            logger.info(f"â”‚ ğŸ”´ NINGÃšN SERVICIO DEVOLVIÃ“ EVENTOS")
        else:
            logger.info(f"â”‚ âœ… SMART SEARCH COMPLETADO")
            logger.info(f"â”‚ ğŸ¯ Total eventos retornados: {final_events}")
        logger.info(f"â”‚ ğŸ“ UbicaciÃ³n final: {location}")
        logger.info(f"â”‚ ğŸ­ Fuente: {result.get('source', 'unknown')}")
        if result.get("filtered_count"):
            logger.info(f"â”‚ ğŸ” Eventos filtrados: {result.get('filtered_count')}")
        if result.get("no_exact_match"):
            logger.info(f"â”‚ âš ï¸ Sin coincidencias exactas - retornando todos")
            
        # â±ï¸ LOG: Timing total del smart search
        smart_search_end_time = time.time()
        smart_search_total_duration = smart_search_end_time - smart_search_start_time
        logger.info(f"â”‚ â±ï¸ SMART SEARCH TOTAL: {smart_search_total_duration:.2f}s")
        
        # Add scrapers execution to result if available
        if "scrapers_execution" not in result and hasattr(result, 'scrapers_execution'):
            result["scrapers_execution"] = getattr(result, 'scrapers_execution')
        
        logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        return result
        
    except Exception as e:
        logger.error(f"Smart search error: {e}")
        return {
            "error": str(e),
            "events": [],
            "query": query.get("query", ""),
            "location": query.get("location", "Buenos Aires")
        }

# GET version for frontend compatibility
@app.get("/api/events/smart-search")
async def smart_search_get(
    q: str = Query(..., description="Search query"),
    location: str = Query(..., description="Location required")
):
    """
    GET version of smart search for frontend compatibility
    """
    # Detectar si el query contiene una ubicaciÃ³n
    query_lower = q.lower()
    detected_location = location
    
    # Lista COMPLETA de ciudades argentinas y GLOBALES ğŸŒ
    cities = ["cÃ³rdoba", "cordoba", "mendoza", "rosario", "la plata", "mar del plata", 
              "salta", "tucumÃ¡n", "tucuman", "bariloche", "neuquÃ©n", "neuquen",
              "santa fe", "bahÃ­a blanca", "bahia blanca", "moreno", "moron", "morÃ³n",
              "tigre", "quilmes", "lanÃºs", "lanus", "avellaneda", "san isidro",
              # EspaÃ±a - Global cities
              "barcelona", "bcn", "barna", "madrid", "capital espaÃ±a", "valencia", "sevilla", "seville",
              # Francia - Global cities  
              "paris", "parÃ­s", "parÃ­", "lyon", "marsella", "marseille",
              # MÃ©xico - Global cities
              "mexico city", "cdmx", "guadalajara", "monterrey", "tijuana", "cancÃºn", "cancun",
              # Otros paÃ­ses
              "china", "chile", "uruguay", "brasil", "peru", "colombia", "mexico"]
    
    # Detectar ciudad en el query - SI ENCUENTRA ALGO, IGNORAR COMPLETAMENTE location
    city_found = False
    global_cities = ["barcelona", "bcn", "barna", "madrid", "capital espaÃ±a", "valencia", "sevilla", "seville",
                     "paris", "parÃ­s", "parÃ­", "lyon", "marsella", "marseille",
                     "mexico city", "cdmx", "guadalajara", "monterrey", "tijuana", "cancÃºn", "cancun"]
    
    logger.error(f"ğŸš¨ğŸš¨ğŸš¨ GET ENDPOINT EJECUTÃNDOSE - query: '{q}', location param: '{location}'")
    logger.info(f"ğŸ” DEBUG - Buscando ciudades en: '{query_lower}'")
    logger.info(f"ğŸ” DEBUG - Lista cities: {cities}")
    
    for city in cities:
        if city in query_lower:
            logger.info(f"ğŸ” DEBUG - ENCONTRADA: '{city}' en '{query_lower}'")
            # ğŸŒ CIUDADES GLOBALES: Sin ", Argentina"
            if city in global_cities:
                if city in ["barcelona", "bcn", "barna"]:
                    location = "Barcelona"
                elif city in ["madrid", "capital espaÃ±a"]:
                    location = "Madrid"
                elif city in ["paris", "parÃ­s", "parÃ­"]:
                    location = "Paris"
                elif city in ["valencia"]:
                    location = "Valencia"
                elif city in ["sevilla", "seville"]:
                    location = "Sevilla"
                elif city in ["mexico city", "cdmx"]:
                    location = "Mexico City"
                else:
                    location = city.title()
            else:
                # ğŸ‡¦ğŸ‡· CIUDADES ARGENTINAS: Con ", Argentina"
                location = city.title() + ", Argentina"
            
            city_found = True
            
            # Si el query ES solo la ciudad, buscar "eventos"
            if query_lower.strip() == city:
                q = "eventos"
            else:
                # Remover la parte "en [ciudad]" del query
                q = q.lower().replace(f"en {city}", "").replace(f"en {city.lower()}", "").replace(city, "").strip()
            
            # Si queda vacÃ­o despuÃ©s de remover la ciudad, buscar "eventos"
            if not q or q == "":
                q = "eventos"
            
            logger.info(f"ğŸ“ Ciudad detectada en query: {city.title()} - IGNORANDO location parameter")
            break
    
    # Si no se detectÃ³ ciudad en el query, NO BUSCAR NADA
    if not city_found and q.lower().strip() in cities:
        # Si el query completo es una ciudad, usarla
        location = q.title() + ", Argentina"
        q = "eventos"
        city_found = True
        logger.info(f"ğŸ“ Query ES una ciudad: {location}")
    elif not city_found:
        # Si no hay ciudad, intentar detectar la ubicaciÃ³n automÃ¡ticamente
        try:
            from api.geolocation import detect_location
            detected_location = await detect_location(request=None)
            if detected_location and detected_location.get('city'):
                location = f"{detected_location['city']}, {detected_location.get('country', 'Argentina')}"
                logger.info(f"ğŸ“ UbicaciÃ³n autodetectada: {location}")
            else:
                logger.warning(f"ğŸ“ No se pudo detectar ubicaciÃ³n, eventos limitados")
                location = None  # No usar fallback hardcodeado
        except Exception as e:
            logger.warning(f"âš ï¸ Error en detecciÃ³n automÃ¡tica: {e}")
            location = None  # No usar fallback hardcodeado
    
    # Redirect to POST version with proper format
    return await smart_search({
        "query": q,
        "location": location
    })

# ğŸ‡ªğŸ‡¸ BARCELONA DIRECT ENDPOINT - Bypass routing issues
@app.get("/api/barcelona")
async def get_barcelona_events():
    """Direct Barcelona endpoint that bypasses all routing complexity"""
    try:
        logger.error(f"ğŸ‡ªğŸ‡¸ BARCELONA DIRECTO - Ejecutando scraper")
        
        from services.barcelona_scraper import BarcelonaScraper
        scraper = BarcelonaScraper()
        events = await scraper.scrape_all_sources()
        
        logger.error(f"ğŸ‡ªğŸ‡¸ BARCELONA DIRECTO - Obtenidos {len(events)} eventos")
        
        return {
            "success": True,
            "location": "Barcelona",
            "events": events[:50],
            "recommended_events": events[:50], 
            "source": "direct_barcelona_scraper",
            "total_events": len(events),
            "message": f"Eventos de Barcelona (directo)",
            "strategy": "direct_scraper_bypass"
        }
    except Exception as e:
        logger.error(f"ğŸ‡ªğŸ‡¸ ERROR Barcelona directo: {e}")
        return {
            "success": False,
            "location": "Barcelona", 
            "events": [],
            "error": str(e),
            "source": "direct_barcelona_scraper"
        }

# Facebook RapidAPI endpoint - LA JOYITA ğŸ’ (DEBE ir ANTES de {event_id})
@app.get("/api/events/facebook")
async def get_facebook_events(
    location: str = Query(..., description="Location required"),
    limit: int = Query(30, description="Maximum number of events to return")
):
    """
    Endpoint ultrarrÃ¡pido de Facebook - SOLO usa cache
    ğŸ•°ï¸ Background job (Windows Service pattern) actualiza cada 6 horas
    âš¡ Usuario SIEMPRE ve respuesta instantÃ¡nea
    """
    try:
        facebook_scraper = RapidApiFacebookScraper()
        
        logger.info(f"âš¡ CACHE-ONLY: Cargando eventos de Facebook para {location}")
        
        # SOLO CACHE - nunca mÃ¡s esperas de 20s
        cache_data = facebook_scraper.load_cache()
        cached_events = cache_data.get("events", [])
        
        if cached_events:
            # Normalizar eventos desde cache
            normalized_events = facebook_scraper.normalize_facebook_events(cached_events)
            
            # Limitar resultados
            limited_events = normalized_events[:limit]
            
            cache_info = cache_data.get("cache_info", {})
            last_updated = cache_info.get("last_updated", "unknown")
            
            logger.info(f"âš¡ CACHE HIT: {len(limited_events)} eventos (cache: {last_updated})")
        
            return {
                "status": "success",
                "source": "rapidapi_facebook_cache",
                "location": location,
                "category": "facebook_events",
                "total": len(limited_events),
                "events": limited_events,
                "cache_info": {
                    "cached": True,
                    "last_updated": last_updated,
                    "background_job": "every_6_hours",
                    "method": "windows_service_pattern"
                }
            }
        else:
            # Cache vacÃ­o - el background job aÃºn no corriÃ³
            logger.warning(f"âš ï¸ CACHE VACÃO: Background job aÃºn no actualizÃ³ el cache")
            return {
                "status": "success", 
                "source": "rapidapi_facebook_cache",
                "location": location,
                "category": "facebook_events", 
                "total": 0,
                "events": [],
                "cache_info": {
                    "cached": False,
                    "message": "Background job actualizarÃ¡ cache pronto",
                    "background_job": "every_6_hours"
                }
            }
        
    except Exception as e:
        logger.error(f"âŒ Error en Facebook endpoint: {e}")
        return {
            "status": "error",
            "error": str(e),
            "location": location,
            "total": 0,
            "events": []
        }

# Teatro endpoint - especializado en obras de teatro (DEBE ir ANTES de {event_id})
@app.get("/api/events/teatro")
async def get_teatro_events(
    location: str = Query(..., description="Location required"),
    limit: int = Query(20, description="Maximum number of events to return")
):
    """
    Endpoint especializado en obras de teatro argentinas
    Fuentes: teatros oficiales, carteleras, medios culturales
    """
    try:
        teatro_scraper = TeatroOptimizadoScraper()
        
        logger.info(f"ğŸ­ Buscando obras de teatro en {location}")
        
        # Scraping de teatro con timeout optimizado
        raw_events = await teatro_scraper.scrape_teatro_optimizado(max_time_seconds=8.0)
        
        # Normalizar eventos para el sistema
        normalized_events = teatro_scraper.normalize_theater_events_optimizado(raw_events)
        
        # Aplicar lÃ­mite
        events = normalized_events[:limit]
        
        logger.info(f"ğŸ­ Retornando {len(events)} obras de teatro para {location}")
        
        return {
            "status": "success",
            "source": "teatro_optimizado",
            "location": location,
            "category": "theater",
            "total": len(events),
            "events": events,
            "scraping_info": {
                "sources_found": len(raw_events),
                "sources_normalized": len(normalized_events),
                "method": "teatro_optimizado_scraper"
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en teatro endpoint: {e}")
        return {
            "status": "error",
            "error": str(e),
            "location": location,
            "total": 0,
            "events": []
        }

# Single event endpoint
@app.get("/api/events/{event_id}")
async def get_event_by_id(event_id: str):
    """
    Obtener un evento especÃ­fico por ID/tÃ­tulo
    """
    try:
        # Buscar en eventos recientes de la ciudad por defecto
        from services.hierarchical_factory import fetch_from_all_sources_internal
        result = await fetch_from_all_sources_internal("Buenos Aires")
        
        events = result.get("events", [])
        
        # Buscar por tÃ­tulo de manera flexible
        for event in events:
            # Generar diferentes versiones del ID para comparar
            title_lower = event['title'].lower()
            simple_id = title_lower.replace(" ", "-").replace(",", "").replace(".", "").replace("(", "").replace(")", "")
            title_match = title_lower.replace(" ", "-")
            event_id_lower = event_id.lower()
            event_id_spaces = event_id.replace("-", " ").lower()
            
            # MÃºltiples formas de hacer match
            if (simple_id == event_id_lower or 
                title_match == event_id_lower or 
                title_lower == event_id_spaces or
                title_lower.startswith(event_id_spaces) or
                event_id_spaces in title_lower):
                
                return {
                    "status": "success",
                    "event": event
                }
        
        # Si no se encuentra
        raise HTTPException(
            status_code=404, 
            detail=f"Event '{event_id}' not found"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting event {event_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))



# AI recommendation endpoint
async def get_nearby_cities_from_ai(location: str, country: str) -> List[str]:
    """ğŸŒ Obtiene 3 ciudades cercanas usando Gemini AI"""
    try:
        from services.ai_service import GeminiAIService
        ai_service = GeminiAIService()
        
        prompt = f"""
        Dame exactamente 3 ciudades cercanas a {location}, {country}.
        Responde SOLO con los nombres de las ciudades separados por comas, sin explicaciones.
        Ejemplo: CÃ³rdoba, Santa Fe, Entre RÃ­os
        """
        
        response = await ai_service.process_prompt(prompt)
        cities = [city.strip() for city in response.split(',')]
        return cities[:3]  # Asegurar mÃ¡ximo 3
        
    except Exception as e:
        logger.warning(f"âš ï¸ Error obteniendo ciudades cercanas: {e}")
        # Fallback segÃºn el paÃ­s
        fallbacks = {
            "Argentina": ["CÃ³rdoba", "Santa Fe", "Entre RÃ­os"],
            "Spain": ["Barcelona", "Valencia", "Sevilla"],
            "France": ["Lyon", "Marseille", "Toulouse"],
            "default": ["City A", "City B", "City C"]
        }
        return fallbacks.get(country, fallbacks["default"])

# FunciÃ³n eliminada - ahora se usa factory.execute_recommend()

@app.post("/api/ai/recommend")
async def ai_recommend(
    data: Dict[str, Any]
):
    """
    ğŸŒ RECOMENDACIONES CON CIUDADES CERCANAS
    Usa solo Eventbrite + Meetup para ciudades cercanas detectadas por IA
    """
    try:
        location = data.get("location", "Buenos Aires")
        country = data.get("detected_country", "Argentina")
        preferences = data.get("preferences", {})
        
        logger.info(f"ğŸŒ AI Recommend: Buscando ciudades cercanas a {location}, {country}")
        
        # 1. Pedir ciudades cercanas a Gemini
        nearby_cities = await get_nearby_cities_from_ai(location, country)
        logger.info(f"ğŸ™ï¸ Ciudades cercanas encontradas: {nearby_cities}")
        
        # 2. Usar Industrial Factory para scrape con nearby_cities
        from services.industrial_factory import IndustrialFactory
        factory = IndustrialFactory()
        
        result = await factory.execute_recommend(
            location=location,
            nearby_cities=nearby_cities,
            category=preferences.get('category'),
            limit=15,
            detected_country=country,
            context_data={"preferences": preferences}
        )
        
        logger.info(f"âœ… Factory Recommend: {result.get('total_events', 0)} eventos de {len(result.get('scrapers_used', []))} scrapers")
        
        return {
            "success": True,
            "location": location,
            "nearby_cities": result.get('cities_processed', nearby_cities),
            "city_buttons": result.get('city_buttons', []),
            "nearby_events": result.get('nearby_events', [])[:30],
            "total_nearby": result.get('total_events', 0),
            "recommendations": result.get('nearby_events', [])[:10],  # Para compatibilidad
            "recommended_events": result.get('nearby_events', [])[:10],  # Para compatibilidad
            "scrapers_used": result.get('scrapers_used', []),
            "city_stats": result.get('city_stats', {})
        }
        
    except Exception as e:
        logger.error(f"âŒ Error in AI recommend: {e}")
        return {
            "success": False,
            "error": str(e),
            "recommendations": [],
            "city_buttons": []
        }

# AI chat endpoint
@app.post("/api/ai/chat")
async def ai_chat(data: Dict[str, Any]):
    """AI chat endpoint for interactive recommendations"""
    message = data.get("message", "")
    context = data.get("context", {})
    
    # ğŸ” PARSEO SIMPLE DE UBICACIONES
    detected_location = "Buenos Aires"
    message_lower = message.lower()
    
    # Mapeo de ciudades GLOBALES (traveler app)
    location_mapping = {
        # Argentina
        "cÃ³rdoba": "CÃ³rdoba", "cordoba": "CÃ³rdoba", "la docta": "CÃ³rdoba",
        "mendoza": "Mendoza", "mza": "Mendoza",
        "rosario": "Rosario",
        "la plata": "La Plata",
        "mar del plata": "Mar del Plata", "mardel": "Mar del Plata",
        "salta": "Salta",
        "tucumÃ¡n": "TucumÃ¡n", "tucuman": "TucumÃ¡n",
        "bariloche": "Bariloche",
        "neuquÃ©n": "NeuquÃ©n", "neuquen": "NeuquÃ©n",
        "buenos aires": "Buenos Aires", "caba": "Buenos Aires", "capital": "Buenos Aires",
        
        # EspaÃ±a - Proof of concept global
        "barcelona": "Barcelona", "bcn": "Barcelona", "barna": "Barcelona",
        "madrid": "Madrid", "capital espaÃ±a": "Madrid",
        "valencia": "Valencia",
        "sevilla": "Sevilla", "seville": "Sevilla",
        
        # Francia  
        "paris": "Paris", "parÃ­s": "Paris",
        "lyon": "Lyon",
        "marseille": "Marseille", "marsella": "Marseille",
        
        # MÃ©xico
        "cdmx": "Mexico City", "ciudad de mÃ©xico": "Mexico City", "df": "Mexico City",
        "guadalajara": "Guadalajara", "gdl": "Guadalajara",
        "monterrey": "Monterrey",
        
        # Colombia
        "bogotÃ¡": "BogotÃ¡", "bogota": "BogotÃ¡",
        "medellÃ­n": "MedellÃ­n", "medellin": "MedellÃ­n",
        
        # Chile
        "santiago": "Santiago", "santiago chile": "Santiago"
    }
    
    # Buscar ciudad en el mensaje
    for city_key, city_name in location_mapping.items():
        if city_key in message_lower:
            detected_location = city_name
            logger.info(f"ğŸ™ï¸ UbicaciÃ³n detectada en texto: '{city_key}' â†’ {city_name}")
            break
    
    # Si no detectÃ³ ciudad en el texto, usar geolocalizaciÃ³n automÃ¡tica
    if detected_location == "Buenos Aires":
        try:
            from api.geolocation import detect_location
            from fastapi import Request
            
            # Crear un request mock para obtener la IP del usuario
            # En una implementaciÃ³n real, deberÃ­as pasar el request real
            logger.info("ğŸ“ No se detectÃ³ ciudad, usando geolocalizaciÃ³n automÃ¡tica...")
            detected_location_data = await detect_location(request=None)
            if detected_location_data and detected_location_data.get('city'):
                detected_location = detected_location_data['city']
                logger.info(f"ğŸ“ Ciudad autodetectada: {detected_location}")
            else:
                logger.warning("ğŸ“ No se pudo autodetectar ubicaciÃ³n")
                detected_location = None  # No usar fallback hardcodeado
        except Exception as e:
            logger.warning(f"âš ï¸ Error en geolocalizaciÃ³n automÃ¡tica: {e}")
            detected_location = "Buenos Aires"
    
    # ğŸš€ USE PROVINCIAL/GLOBAL SCRAPERS (no database)
    if detected_location == "Mendoza":
        logger.info(f"âœ… Usando eventos del scraper provincial para {detected_location}")
        from services.provincial_scrapers import MendozaScraper
        mendoza_scraper = MendozaScraper()
        events = await mendoza_scraper.scrape_all()
    elif detected_location == "Barcelona":
        logger.info(f"âœ… Usando eventos del scraper global para {detected_location}")
        from services.barcelona_scraper import BarcelonaScraper
        barcelona_scraper = BarcelonaScraper()
        events = await barcelona_scraper.scrape_all_sources()
    else:
        # Buenos Aires y otros - usar multi-source
        from services.hierarchical_factory import fetch_from_all_sources
        result = await fetch_from_all_sources(location=detected_location)
        events = result.get("events", [])
    
    # Score events by relevance to query
    scored_events = []
    query_words = message.lower().split()
    
    for event in events:
        score = 0
        title_lower = event.get('title', '').lower()
        desc_lower = event.get('description', '').lower()
        category_lower = event.get('category', '').lower()
        
        # Basic scoring
        for word in query_words:
            if word in title_lower:
                score += 3
            if word in desc_lower:
                score += 2
            if word in category_lower:
                score += 2
        
        if score > 0 or len(query_words) <= 1:  # Include all if generic query
            event['match_score'] = score
            scored_events.append(event)
    
    # Sort by score
    scored_events.sort(key=lambda x: x.get('match_score', 0), reverse=True)
    
    return {
        "success": True,
        "location": detected_location,
        "events": scored_events[:10],
        "recommended_events": scored_events[:5],
        "total_events": len(events),
        "sources_completed": [{"source": f"{detected_location} Scrapers", "count": len(events), "status": "success"}],
        "message": f"Eventos especÃ­ficos de {detected_location}" if scored_events else f"No encontramos eventos especÃ­ficos de {detected_location}.",
        "query": message,
        "smart_search": True,
        "filtered_count": len(scored_events)
    }

# AI recommendations endpoint (alias)
@app.post("/api/ai/recommendations")
async def ai_recommendations(data: Dict[str, Any]):
    """Get AI recommendations"""
    return await ai_recommend(data)

# AI plan weekend endpoint
@app.post("/api/ai/plan-weekend")
async def ai_plan_weekend(data: Dict[str, Any]):
    """Plan weekend with AI"""
    location = data.get("location", "Buenos Aires")
    from services.hierarchical_factory import fetch_from_all_sources
    result = await fetch_from_all_sources(location=location)
    
    # Filter for weekend events
    weekend_events = result.get("events", [])[:10]
    
    return {
        "plan": "Weekend plan generated",
        "events": weekend_events,
        "location": location
    }

# AI trending now endpoint
@app.get("/api/ai/trending-now")
async def ai_trending_now():
    """Get trending events"""
    from services.hierarchical_factory import fetch_from_all_sources
    result = await fetch_from_all_sources(location="Buenos Aires")
    
    return {
        "trending": result.get("events", [])[:5],
        "timestamp": datetime.utcnow().isoformat()
    }

# AI nearby cities endpoint
@app.get("/api/ai/nearby-cities")
async def ai_nearby_cities(city: str):
    """Get 3 nearby cities for a given location"""
    # Hardcoded nearby cities for common locations (to avoid AI token usage)
    nearby_cities_map = {
        'miami': ['Fort Lauderdale', 'West Palm Beach', 'Hollywood'],
        'buenos aires': ['La Plata', 'San Isidro', 'Tigre'],
        'madrid': ['AlcalÃ¡ de Henares', 'Getafe', 'LeganÃ©s'],
        'barcelona': ['Badalona', 'Hospitalet', 'Sabadell'],
        'mexico city': ['Ecatepec', 'Guadalajara', 'Puebla'],
        'sÃ£o paulo': ['Guarulhos', 'Campinas', 'Santo AndrÃ©'],
        'sao paulo': ['Guarulhos', 'Campinas', 'Santo AndrÃ©'],
        'new york': ['Newark', 'Jersey City', 'Yonkers'],
        'los angeles': ['Long Beach', 'Santa Monica', 'Pasadena'],
        'chicago': ['Evanston', 'Oak Park', 'Schaumburg'],
        'london': ['Westminster', 'Camden', 'Greenwich'],
        'paris': ['Versailles', 'Boulogne-Billancourt', 'Saint-Denis'],
        'berlin': ['Potsdam', 'Spandau', 'Charlottenburg'],
        'tokyo': ['Yokohama', 'Kawasaki', 'Saitama'],
        'sydney': ['Parramatta', 'Bondi', 'Manly'],
        'melbourne': ['St Kilda', 'Richmond', 'Fitzroy'],
        'toronto': ['Mississauga', 'Brampton', 'Markham'],
        'vancouver': ['Burnaby', 'Richmond', 'Surrey'],
        'montreal': ['Laval', 'Longueuil', 'Gatineau']
    }
    
    city_lower = city.lower().strip()
    nearby = nearby_cities_map.get(city_lower, [])
    
    return {
        "city": city,
        "nearby_cities": nearby,
        "cached": True
    }

# Scraper performance metrics endpoint
@app.get("/api/scrapers/performance")
async def get_scrapers_performance():
    """Get real-time performance metrics for all scrapers"""
    from services.auto_discovery import AutoDiscoveryEngine
    from datetime import datetime
    
    discovery = AutoDiscoveryEngine()
    scrapers_map = discovery.discover_all_scrapers()
    global_scrapers = scrapers_map.get('global', {})
    
    # Build performance metrics
    performance_data = []
    for name, scraper in global_scrapers.items():
        enabled = getattr(scraper, 'enabled_by_default', False)
        priority = getattr(scraper, 'priority', 99)
        nearby = getattr(scraper, 'nearby_cities', False)
        
        # Estimate response times based on scraper type
        if 'playwright' in name.lower() or 'fever' in name.lower():
            avg_response_ms = 5000
            scraper_type = 'playwright'
        elif 'meetup' in name.lower():
            avg_response_ms = 4500
            scraper_type = 'playwright'
        elif 'api' in name.lower() or 'facebook' in name.lower():
            avg_response_ms = 800
            scraper_type = 'api'
        elif 'eventbrite' in name.lower():
            avg_response_ms = 1200
            scraper_type = 'web_scraping'
        else:
            avg_response_ms = 2000
            scraper_type = 'web_scraping'
        
        performance_data.append({
            'name': name,
            'enabled': enabled,
            'priority': priority,
            'supports_nearby_cities': nearby,
            'avg_response_ms': avg_response_ms,
            'type': scraper_type,
            'status': 'active' if enabled else 'disabled',
            'last_check': datetime.utcnow().isoformat()
        })
    
    # Sort by priority
    performance_data.sort(key=lambda x: x['priority'])
    
    return {
        'scrapers': performance_data,
        'total_enabled': sum(1 for s in performance_data if s['enabled']),
        'total_disabled': sum(1 for s in performance_data if not s['enabled']),
        'execution_order': [s['name'] for s in performance_data if s['enabled']],
        'timestamp': datetime.utcnow().isoformat()
    }

# Advanced scraping endpoint
@app.get("/api/scraping/multi-technique")
async def scraping_multi_technique():
    """
    Test all advanced scraping techniques
    """
    try:
        logger.info("ğŸš€ Starting advanced multi-technique scraping...")
        
        # Initialize scrapers
        multi_scraper = MultiTechniqueScraper()
        cloudscraper = CloudscraperEvents()
        # massive_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
        
        # Run scraping in parallel
        tasks = [
            multi_scraper.scrape_all_methods(),
            cloudscraper.fetch_all_events("Buenos Aires"),
            massive_scraper.massive_scraping(max_urls=8)
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Extract results
        multi_events = results[0] if isinstance(results[0], list) else []
        cloud_events = results[1] if isinstance(results[1], list) else []
        massive_events = results[2] if isinstance(results[2], list) else []
        
        # Combine results
        all_events = multi_events + cloud_events + massive_events
        
        # Normalize events
        if multi_events:
            normalized_multi = multi_scraper.normalize_events(multi_events)
        else:
            normalized_multi = []
            
        if cloud_events:
            normalized_cloud = cloud_events  # Already normalized
        else:
            normalized_cloud = []
            
        if massive_events:
            normalized_massive = massive_scraper.normalize_events(massive_events)
        else:
            normalized_massive = []
        
        all_normalized = normalized_multi + normalized_cloud + normalized_massive
        
        logger.info(f"âœ… Advanced scraping completed: {len(all_normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(all_normalized),
            "events": all_normalized,
            "sources": {
                "multi_technique": len(normalized_multi),
                "cloudscraper": len(normalized_cloud),
                "massive_eventbrite": len(normalized_massive)
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Advanced scraping error: {e}")
        return {
            "status": "error", 
            "message": str(e),
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Facebook human session scraping endpoint
@app.get("/api/scraping/facebook-human")
async def scraping_facebook_human():
    """
    Facebook scraping using human session (requires setup)
    """
    try:
        logger.info("ğŸš€ Starting Facebook human session scraping...")
        
        scraper = FacebookHumanSessionScraper()
        
        # Check if session exists
        if not scraper.session_file.exists():
            return {
                "status": "setup_required",
                "message": "Human session not configured. Run setup first.",
                "events": [],
                "setup_instructions": "Run: python -m backend.services.facebook_human_session_scraper setup"
            }
        
        # Run scraping
        events = await scraper.scrape_all_events(venues_limit=4, searches_limit=2)
        
        # Normalize events
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"âœ… Facebook human scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "events": normalized,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Facebook human scraping error: {e}")
        return {
            "status": "error",
            "message": str(e),
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Facebook hybrid removed - only real data sources allowed

# Instagram hybrid removed - only real data sources allowed

# Bright Data scraping endpoint  
@app.get("/api/scraping/bright-data")
async def scraping_bright_data():
    """
    Facebook scraping using Bright Data proxies
    """
    try:
        logger.info("ğŸš€ Starting Bright Data scraping...")
        
        # Note: Bright Data config would need to be set up
        # scraper = FacebookBrightDataScraper()  # MOVED TO LEGACY
        
        events = await scraper.scrape_all_venues(limit_per_venue=3)
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"âœ… Bright Data scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "events": normalized,
            "note": "Bright Data config not set - using fallback methods",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Bright Data scraping error: {e}")
        return {
            "status": "error",
            "message": str(e), 
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# Massive Eventbrite scraper endpoint  
@app.get("/api/scraping/eventbrite-massive")
async def scraping_eventbrite_massive():
    """
    Massive Eventbrite scraping - obtiene MUCHOS mÃ¡s eventos
    """
    try:
        logger.info("ğŸš€ Starting massive Eventbrite scraping...")
        
        scraper = EventbriteMassiveScraper()
        
        # Scraping masivo
        events = await scraper.massive_scraping(max_urls=12)
        
        # Normalizar eventos
        normalized = scraper.normalize_events(events) if events else []
        
        logger.info(f"âœ… Massive Eventbrite scraping completed: {len(normalized)} events")
        
        return {
            "status": "success",
            "total_events": len(normalized),
            "raw_events": len(events),
            "events": normalized,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Massive Eventbrite scraping error: {e}")
        return {
            "status": "error",
            "message": str(e), 
            "events": [],
            "timestamp": datetime.utcnow().isoformat()
        }

# ğŸ§  AI intent analysis endpoint - POWERED BY GEMINI
@app.post("/api/ai/analyze-intent")
async def analyze_intent(
    data: Dict[str, Any]
):
    """
    ğŸ§  Analyze user intent from natural language using GEMINI AI
    """
    try:
        query = data.get("query", "")
        current_location = data.get("current_location", "Buenos Aires")
        
        # Usar IntentRecognitionService para detectar ciudad, provincia y paÃ­s
        from services.intent_recognition import IntentRecognitionService
        intent_service = IntentRecognitionService()
        
        # Analizar con IA
        result = await intent_service.get_all_api_parameters(query)
        
        if result.get('success'):
            intent_data = result.get('intent', {})
            return {
                "success": True,
                "intent": {
                    "query": query,
                    "location": intent_data.get('city', query),
                    "city": intent_data.get('city', ''),
                    "province": intent_data.get('province', ''),
                    "country": intent_data.get('country', ''),
                    "categories": [intent_data.get('category', 'general')],
                    "confidence": intent_data.get('confidence', 0.5),
                    "type": "location_search"
                },
                "user_context": {
                    "location": intent_data.get('city', current_location),
                    "detected_country": intent_data.get('country', 'Argentina'),
                    "detected_province": intent_data.get('province', '')
                },
                "api_parameters": {
                    "location": intent_data.get('city', query),
                    "category": intent_data.get('category')
                }
            }
        else:
            # Fallback si falla la IA
            return {
                "success": False,
                "intent": {
                    "query": query,
                    "location": query,
                    "categories": [],
                    "confidence": 0.1,
                    "type": "location_search"
                },
                "user_context": {
                    "location": current_location,
                    "detected_country": "Argentina"
                },
                "api_parameters": {
                    "location": query,
                    "category": None
                }
            }
#         
#         logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         logger.info(f"â”‚ ğŸš€ EJECUTANDO: analyze_intent | query='{query[:40]}...' | location={current_location}")
#         logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         
#         if not query.strip():
#             logger.error("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#             logger.error(f"â”‚ âŒ ERROR: analyze_intent | Query vacÃ­o requerido")
#             logger.error("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#             return {
#                 "success": False,
#                 "error": "Query is required",
#                 "intent": {}
#             }
#         
#         # Usar servicio unificado de reconocimiento de intenciones
#         from services.intent_recognition import intent_service
#         
#         result = await intent_service.get_all_api_parameters(query)
#         
#         # Formatear respuesta para compatibilidad con frontend existente
#         intent = {
#             "query": query,
#             "categories": [result['intent']['category']] if result['intent']['category'] != 'Todos' else [],
#             "location": result['intent']['location'],
#             "time_preference": None,  # Se puede agregar despuÃ©s
#             "price_preference": None,
#             
#             # InformaciÃ³n adicional de Gemini - ESTRUCTURA CORREGIDA
#             "confidence": result['intent']['confidence'],
#             "detected_country": result['intent']['country'],  # Usar 'country' en lugar de 'detected_country'
#             "detected_city": result['intent']['city'],        # Usar 'city' en lugar de 'location'
#             "detected_province": result['intent'].get('province', ''),
#             "keywords": result['intent'].get('keywords', []),
#             "intent_type": result['intent']['type'],
#             
#             # Agregar jerarquÃ­a geogrÃ¡fica completa
#             "geographic_hierarchy": result['intent'].get('geographic_hierarchy', {}),
#             "scraper_config": result['intent'].get('scraper_config', {})
#         }
#         
#         # Crear user_context con lÃ³gica de prioridades
#         # 1. PRIORIDAD: UbicaciÃ³n detectada por IA (override)
#         # 2. FALLBACK: current_location del frontend
#         detected_location = result['intent']['location']
#         final_location = detected_location if detected_location else current_location
#         
#         user_context = {
#             "location": final_location,
#             "coordinates": None,  # Se podrÃ­a agregar geocoding despuÃ©s
#             "detected_country": result['intent']['country']  # Usar 'country' en lugar de 'detected_country'
#         }
#         
#         logger.info("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         logger.info(f"â”‚ âœ… Ã‰XITO: analyze_intent | categorÃ­a={result['intent']['category']} | ubicaciÃ³n={detected_location}")
#         logger.info(f"â”‚ confianza={result['intent']['confidence']:.2f} | paÃ­s={result['intent']['country']} | tipo={result['intent']['type']}")
#         logger.info(f"â”‚ ğŸ—ºï¸ JERARQUÃA: {result['intent'].get('geographic_hierarchy', {}).get('full_location', 'N/A')}")
#         logger.info("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         
#         return {
#             "success": True,
#             "intent": intent,
#             "user_context": user_context,  # â† Nuevo: contexto actualizado
#             "apis": {
#                 "location": detected_location,
#                 "category": result['intent']['category']
#             }
#         }
#         
    except Exception as e:
        pass  # Comentado temporalmente
#         logger.error("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         logger.error(f"â”‚ âŒ ERROR: analyze_intent | {str(e)}")
#         logger.error("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
#         return {
#             "success": False,
#             "error": str(e),
#             "intent": {
#                 "query": query,
#                 "categories": [],
#                 "location": None,
#                 "fallback": True
#             },
#             "user_context": {
#                 "location": current_location,  # Usar current_location si falla
#                 "coordinates": None,
#                 "detected_country": None
#             }
#         }

# WebSocket endpoint for notifications
@app.websocket("/ws/notifications")
async def websocket_notifications(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # Process received message
            message = json.loads(data)
            
            # Broadcast notification to all connected clients
            notification = {
                "type": "event_reminder",
                "message": message.get("message", "New event notification"),
                "timestamp": datetime.utcnow().isoformat()
            }
            
            await manager.broadcast(json.dumps(notification))
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket)

# WebSocket streaming scrapers progress endpoint
@app.websocket("/ws/scrapers-progress")
async def websocket_scrapers_progress(websocket: WebSocket):
    """ğŸ”„ WebSocket para mostrar progreso de scrapers en tiempo real"""
    await manager.connect(websocket)
    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "connection",
            "status": "connected",
            "message": "ğŸ”„ ConexiÃ³n establecida - Streaming de scrapers activado"
        })
        
        while True:
            # Wait for scraping request
            data = await websocket.receive_json()
            
            if data.get("action") == "start_scraping":
                location = data.get("location", "Buenos Aires")
                category = data.get("category")
                limit = data.get("limit", 30)
                
                # Stream scrapers with real-time progress
                await stream_scrapers_with_progress(websocket, location, category, limit)
                
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket scrapers progress error: {e}")
        manager.disconnect(websocket)

# WebSocket streaming search endpoint
@app.websocket("/ws/search-events")
async def websocket_search_events(websocket: WebSocket):
    await manager.connect(websocket)
    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "connection",
            "status": "connected",
            "message": "ğŸ”¥ ConexiÃ³n establecida - Listo para bÃºsqueda streaming"
        })
        
        while True:
            # Wait for search request
            data = await websocket.receive_json()
            
            if data.get("action") == "search":
                # ğŸ” PARSEO DE UBICACIÃ“N EN WEBSOCKET (como IA-First)
                message = data.get("message", "")
                base_location = data.get("location", "Buenos Aires")
                detected_location = base_location
                
                # Si hay mensaje, intentar detectar ubicaciÃ³n
                if message:
                    message_lower = message.lower()
                    
                    # Mapeo de ciudades GLOBALES (traveler app)
                    location_mapping = {
                        # Argentina
                        "cÃ³rdoba": "CÃ³rdoba", "cordoba": "CÃ³rdoba", "la docta": "CÃ³rdoba",
                        "mendoza": "Mendoza", "mza": "Mendoza",
                        "rosario": "Rosario",
                        "la plata": "La Plata",
                        "mar del plata": "Mar del Plata", "mardel": "Mar del Plata",
                        "salta": "Salta",
                        "tucumÃ¡n": "TucumÃ¡n", "tucuman": "TucumÃ¡n",
                        "bariloche": "Bariloche",
                        "neuquÃ©n": "NeuquÃ©n", "neuquen": "NeuquÃ©n",
                        "buenos aires": "Buenos Aires", "caba": "Buenos Aires", "capital": "Buenos Aires",
                        
                        # EspaÃ±a - Proof of concept global
                        "barcelona": "Barcelona", "bcn": "Barcelona", "barna": "Barcelona",
                        "madrid": "Madrid", "capital espaÃ±a": "Madrid",
                        "valencia": "Valencia",
                        "sevilla": "Sevilla", "seville": "Sevilla",
                        
                        # Francia  
                        "paris": "Paris", "parÃ­s": "Paris",
                        "lyon": "Lyon",
                        "marseille": "Marseille", "marsella": "Marseille",
                        
                        # MÃ©xico
                        "cdmx": "Mexico City", "ciudad de mÃ©xico": "Mexico City", "df": "Mexico City",
                        "guadalajara": "Guadalajara", "gdl": "Guadalajara",
                        "monterrey": "Monterrey",
                        
                        # Colombia
                        "bogotÃ¡": "BogotÃ¡", "bogota": "BogotÃ¡",
                        "medellÃ­n": "MedellÃ­n", "medellin": "MedellÃ­n",
                        
                        # Chile
                        "santiago": "Santiago", "santiago chile": "Santiago"
                    }
                    
                    # Buscar ciudad en el mensaje
                    for city_key, city_name in location_mapping.items():
                        if city_key in message_lower:
                            detected_location = city_name
                            logger.info(f"ğŸ™ï¸ WebSocket - UbicaciÃ³n detectada: '{city_key}' â†’ {city_name}")
                            break
                
                # Send search started con ubicaciÃ³n detectada
                await websocket.send_json({
                    "type": "search_started",
                    "status": "searching",
                    "location": detected_location,
                    "message": f"ğŸš€ Iniciando bÃºsqueda en {detected_location}...",
                    "location_source": "detected" if detected_location != base_location else "default"
                })
                
                # Start streaming search con ubicaciÃ³n correcta
                await stream_events_optimized(websocket, detected_location)
                
    except WebSocketDisconnect:
        manager.disconnect(websocket)
    except Exception as e:
        logger.error(f"WebSocket search error: {e}")
        manager.disconnect(websocket)

async def stream_events_search(websocket: WebSocket, location: str):
    """Stream events from multiple Argentine sources via WebSocket"""
    total_events = 0
    batch_size = 3
    
    try:
        # ğŸ¯ SCRAPERS ESPECÃFICOS POR UBICACIÃ“N
        logger.info(f"ğŸ” WebSocket - Iniciando bÃºsqueda especÃ­fica para: {location}")
        
        # 1. Scrapers especÃ­ficos por provincia
        if location in ["CÃ³rdoba", "Cordoba"]:
            await websocket.send_json({
                "type": "source_started", 
                "source": "cordoba_provincial",
                "message": f"ğŸ›ï¸ Fuentes especÃ­ficas de CÃ³rdoba...",
                "progress": 10
            })
            
            try:
                from services.provincial_scrapers import CordobaScraper
                cordoba_scraper = CordobaScraper()
                cordoba_events = await cordoba_scraper.scrape_all()
                
                # Send CÃ³rdoba events in batches
                for i in range(0, len(cordoba_events), batch_size):
                    batch = cordoba_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "cordoba_provincial",
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(cordoba_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de CÃ³rdoba: {e}")
                
        elif location == "Mendoza":
            await websocket.send_json({
                "type": "source_started", 
                "source": "mendoza_provincial",
                "message": f"ğŸ· Fuentes especÃ­ficas de Mendoza...",
                "progress": 10
            })
            
            try:
                from services.provincial_scrapers import MendozaScraper
                mendoza_scraper = MendozaScraper()
                mendoza_events = await mendoza_scraper.scrape_all()
                
                # Send Mendoza events in batches
                for i in range(0, len(mendoza_events), batch_size):
                    batch = mendoza_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "mendoza_provincial", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(mendoza_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de Mendoza: {e}")
                
        elif location == "Barcelona":
            await websocket.send_json({
                "type": "source_started", 
                "source": "barcelona_global",
                "message": f"ğŸ‡ªğŸ‡¸ Fuentes especÃ­ficas de Barcelona...",
                "progress": 10
            })
            
            try:
                from services.barcelona_scraper import BarcelonaScraper
                barcelona_scraper = BarcelonaScraper()
                barcelona_events = await barcelona_scraper.scrape_all_sources()
                
                # Send Barcelona events in batches
                for i in range(0, len(barcelona_events), batch_size):
                    batch = barcelona_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "barcelona_global", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(10 + (i/max(len(barcelona_events), 1)) * 30, 40)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en scrapers de Barcelona: {e}")
        
        # 2. Fuentes generales (solo si es Buenos Aires o para complementar)
        if location == "Buenos Aires" or total_events < 10:
            # Eventbrite Argentina (mÃ¡s rÃ¡pido)
            await websocket.send_json({
                "type": "source_started", 
                "source": "eventbrite",
                "message": "ğŸ« Buscando en Eventbrite Argentina...",
                "progress": 45
            })
            
            try:
                # eventbrite_scraper = EventbriteMassiveScraper()  # MOVED TO LEGACY
                eventbrite_events = await eventbrite_scraper.massive_scraping(max_urls=6)
                eventbrite_normalized = eventbrite_scraper.normalize_events(eventbrite_events)
                
                # Send Eventbrite events in batches
                for i in range(0, len(eventbrite_normalized), batch_size):
                    batch = eventbrite_normalized[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "eventbrite", 
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(45 + (i/max(len(eventbrite_normalized), 1)) * 15, 60)
                    })
                    await asyncio.sleep(0.3)
            except Exception as e:
                logger.error(f"Error en Eventbrite: {e}")
            
            # Venues Argentinos Oficiales
            await websocket.send_json({
                "type": "source_started",
                "source": "argentina_venues", 
                "message": "ğŸ›ï¸ Venues oficiales de Argentina...",
                "progress": 60
            })
            
            try:
                # Import venue scrapers
                from services.oficial_venues_scraper import OficialVenuesScraper
                # from services.argentina_venues_scraper import ArgentinaVenuesScraper  # DELETED
                
                # Argentina Venues Scraper - DELETED - FAKE DATA GENERATOR
                # argentina_scraper = ArgentinaVenuesScraper()  # DELETED - FAKE DATA GENERATOR
                # argentina_events = await argentina_scraper.fetch_all_events()  # DELETED - FAKE DATA GENERATOR
                argentina_events = []  # No fake data - return empty array
                
                # Send Argentina venues events (now empty)
                for i in range(0, len(argentina_events), batch_size):
                    batch = argentina_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "argentina_venues",
                        "events": batch,
                        "batch_count": len(batch), 
                        "total_so_far": total_events,
                        "progress": min(60 + (i/max(len(argentina_events), 1)) * 15, 75)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Error en venues argentinos: {e}")
        
        # 3. Fuentes adicionales (si necesario)
        if total_events < 20:
            # Ticketek Argentina (si existe)
            await websocket.send_json({
                "type": "source_started",
                "source": "ticketmaster",
                "message": "ğŸª Ticketek/Ticketmaster Argentina...", 
                "progress": 80
            })
            
            try:
                # Try to scrape Ticketek if available
                from services.ticketek_scraper import TicketekArgentinaScraper
                ticketek_scraper = TicketekArgentinaScraper()
                ticketek_events = await ticketek_scraper.fetch_all_events()
                
                for i in range(0, len(ticketek_events), batch_size):
                    batch = ticketek_events[i:i+batch_size]
                    total_events += len(batch)
                    
                    await websocket.send_json({
                        "type": "events_batch",
                        "source": "ticketmaster",
                        "events": batch,
                        "batch_count": len(batch),
                        "total_so_far": total_events,
                        "progress": min(80 + (i/max(len(ticketek_events), 1)) * 15, 95)
                    })
                    await asyncio.sleep(0.2)
            except Exception as e:
                logger.error(f"Ticketek no disponible: {e}")
        
        # BÃºsqueda completada
        await websocket.send_json({
            "type": "search_completed",
            "status": "success",
            "total_events": total_events,
            "location": location,
            "sources_completed": 3 if location in ["Mendoza", "CÃ³rdoba", "Cordoba"] else 2,
            "progress": 100,
            "message": f"ğŸ‰ BÃºsqueda completada en {location}! {total_events} eventos encontrados"
        })
        
    except Exception as e:
        logger.error(f"Error en bÃºsqueda streaming: {e}")
        await websocket.send_json({
            "type": "search_error",
            "status": "error", 
            "message": f"Error en la bÃºsqueda: {str(e)}",
            "progress": 100
        })

@app.get("/api/cache/status")
async def get_cache_status():
    """Estado del cache diario"""
    try:
        from services.daily_cache import daily_cache
        stats = await daily_cache.get_cache_stats()
        return {
            "status": "success",
            "cache": stats
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@app.post("/api/cache/refresh")
async def refresh_cache():
    """Forzar actualizaciÃ³n del cache (usar con cuidado)"""
    try:
        from services.daily_cache import daily_cache
        results = await daily_cache.update_cache_from_scrapers()
        return {
            "status": "success",
            "message": "Cache refreshed",
            "results": results
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }

@app.get("/api/multi/fetch-stream") 
async def fetch_stream_progressive(location: str = Query(..., description="Location required")):
    """
    Streaming endpoint que devuelve eventos en fases progresivas
    Para frontend que quiera mostrar datos conforme llegan
    """
    try:
        logger.info(f"ğŸ“¡ Progressive stream iniciado para: {location}")
        
        scraper = ProgressiveSyncScraper()
        
        # Recolectar todas las fases
        phases = []
        async for phase_result in scraper.progressive_fetch_stream(location):
            phases.append(phase_result)
        
        return {
            "status": "success",
            "location": location,
            "phases": phases,
            "strategy": "progressive_stream",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en progressive stream: {e}")
        return {
            "status": "error",
            "error": str(e),
            "phases": [],
            "message": f"âŒ Error durante progressive stream: {str(e)}"
        }

# ğŸ—“ï¸ DAILY SOCIAL SCRAPER ENDPOINTS

@app.get("/api/daily-scraper/run")
async def run_daily_social_scraping():
    """
    Ejecuta el scraping diario de Facebook e Instagram
    Pensado para ser llamado por un cron job una vez por dÃ­a
    """
    try:
        logger.info("ğŸ—“ï¸ Iniciando scraping diario de redes sociales...")
        
        scraper = DailySocialScraper()
        result = await scraper.run_daily_scraping()
        
        # AquÃ­ serÃ­a donde guardas en la base de datos
        # await save_daily_events_to_db(result['events'])
        
        logger.info(f"âœ… Scraping diario completado: {result['total_events_found']} eventos")
        
        return {
            "status": "success", 
            "message": "Daily scraping completed successfully",
            "summary": {
                "total_events": result['total_events_found'],
                "facebook_events": result['facebook_events'],
                "instagram_events": result['instagram_events'],
                "execution_time": f"{result['execution_time_seconds']:.1f}s",
                "scraping_date": result['scraping_date'],
                "next_scraping": result['next_scraping']
            },
            "events_sample": result['events'][:5],  # Solo muestra 5 como ejemplo
            "cron_job_info": {
                "frequency": "Once per day",
                "recommended_time": "03:00 AM local time",
                "endpoint": "/api/daily-scraper/run",
                "storage": "Database storage ready"
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Error en daily scraping: {e}")
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.utcnow().isoformat()
        }

@app.get("/api/daily-scraper/status")
async def daily_scraper_status():
    """
    Estado del sistema de scraping diario
    """
    return {
        "status": "ready",
        "strategy": "Daily Facebook & Instagram scraping with DB storage",
        "features": [
            "ğŸ•’ Runs once per day via cron job",
            "ğŸ“˜ Facebook real event extraction", 
            "ğŸ“¸ Instagram real event extraction",
            "ğŸ’¾ Database storage for retrieved events",
            "ğŸš« No simulated/generated events",
            "ğŸ” Smart deduplication across platforms"
        ],
        "target_venues": {
            "facebook": 8,
            "instagram": 8 
        },
        "implementation": "âœ… DailySocialScraper class ready",
        "database_integration": "âš ï¸ Pending implementation",
        "next_steps": "Set up cron job to call /api/daily-scraper/run"
    }

@app.get("/api/debug/scrapers-status")
async def get_scrapers_debug():
    """
    ğŸ” DEBUG ENDPOINT - Para inspector del browser
    Muestra estado de todos los scrapers: execution_status y results_count
    """
    try:
        from services.country_scraper_factory import get_events_by_location
        
        # Test con Miami para obtener debug info
        result = await get_events_by_location("Miami")
        
        debug_info = []
        if result.get("scrapers_executed"):
            for scraper_name, scraper_info in result["scrapers_executed"].items():
                debug_info.append({
                    "scraper_name": scraper_name,
                    "execution_status": scraper_info.get("status", "unknown"),
                    "results_count": scraper_info.get("events", 0),
                    "execution_time": "N/A",  # Lo agregaremos despuÃ©s
                    "country": "USA" if "Miami" in scraper_name else "Unknown",
                    "city": "Miami" if "Miami" in scraper_name else "Unknown"
                })
        
        return {
            "total_scrapers": len(debug_info),
            "scrapers": debug_info,
            "timestamp": datetime.utcnow().isoformat(),
            "inspector_info": "ğŸ” Use this endpoint to debug scrapers from browser inspector"
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "scrapers": [],
            "total_scrapers": 0
        }

# ğŸ§¹ DAILY CLEANUP ENDPOINTS
@app.post("/api/admin/cleanup")
async def run_manual_cleanup():
    """
    ğŸ§¹ Ejecutar limpieza manual de eventos expirados
    Ãštil para testing o limpieza bajo demanda
    """
    try:
        from services.daily_cleanup import run_manual_cleanup
        result = await run_manual_cleanup()
        return result
    except Exception as e:
        logger.error(f"âŒ Error en limpieza manual: {e}")
        return {
            "status": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/api/admin/cleanup/status")
async def get_cleanup_status():
    """
    ğŸ” Obtener estado del sistema de limpieza
    """
    try:
        from services.daily_cleanup import DailyCleanupManager
        
        cleanup_manager = DailyCleanupManager()
        now = datetime.now()
        
        return {
            "current_time": now.isoformat(),
            "cleanup_hour": cleanup_manager.cleanup_hour,
            "should_run_now": cleanup_manager.should_run_cleanup(),
            "time_until_next_cleanup": f"{(cleanup_manager.cleanup_hour - now.hour) % 24} horas",
            "retention_days": cleanup_manager.retention_days,
            "cache_files_monitored": len(cleanup_manager.cache_files),
            "status": "active"
        }
    except Exception as e:
        return {
            "status": "error", 
            "error": str(e)
        }

# ğŸ§ª ADMIN TESTING ENDPOINTS - Individual Source Testing
@app.post("/api/test/facebook")
async def test_facebook_source(request: Request):
    """Test Facebook scraping individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Barcelona")
        
        # from services.rapidapi_facebook_scraper import RapidApiFacebookScraper
        scraper = RapidApiFacebookScraper()
        events = await scraper.scrape_facebook_events_rapidapi(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Facebook",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Facebook",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/eventbrite")
async def test_eventbrite_source(request: Request):
    """Test Eventbrite scraping individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Barcelona")
        
        from services.eventbrite_web_scraper import EventbriteWebScraper
        scraper = EventbriteWebScraper()
        events = await scraper.fetch_events_by_location(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Eventbrite",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Eventbrite",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/venues")
async def test_venues_source(request: Request):
    """Test Official Venues scraping individual"""
    start_time = time.time()
    try:
        from services.oficial_venues_scraper import OficialVenuesScraper
        scraper = OficialVenuesScraper()
        events = await scraper.scrape_all_oficial_venues()
        
        execution_time = time.time() - start_time
        return {
            "source": "Official Venues",
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Official Venues",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/provincial")
async def test_provincial_source(request: Request):
    """Test Provincial scrapers individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "CÃ³rdoba")
        
        from services.provincial_scrapers import get_events_by_province
        events = await get_events_by_province(location)
        
        execution_time = time.time() - start_time
        return {
            "source": "Provincial",
            "location": location,
            "events": events.get("events", []) if isinstance(events, dict) else events,
            "count": len(events.get("events", [])) if isinstance(events, dict) else len(events) if events else 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Provincial",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/multi-source")
async def test_multi_source(request: Request):
    """Test Multi-Source API individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "Buenos Aires")
        
        from services.hierarchical_factory import fetch_from_all_sources
        result = await fetch_from_all_sources(location=location, fast=True)
        events = result.get("events", []) if result else []
        
        execution_time = time.time() - start_time
        return {
            "source": "Multi-Source",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Multi-Source",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

@app.post("/api/test/factory")
async def test_factory_source(request: Request):
    """Test Hierarchical Factory individual"""
    start_time = time.time()
    try:
        body = await request.json()
        location = body.get("location", "CÃ³rdoba")
        
        from services.hierarchical_factory import get_events_hierarchical
        result = await get_events_hierarchical(location)
        events = result.get("events", []) if result else []
        
        execution_time = time.time() - start_time
        return {
            "source": "Hierarchical Factory",
            "location": location,
            "events": events,
            "count": len(events),
            "execution_time": f"{execution_time:.3f}s",
            "status": "success",
            "scraper_used": result.get("scraper_used", "Unknown") if result else "Unknown"
        }
    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "source": "Hierarchical Factory",
            "events": [],
            "count": 0,
            "execution_time": f"{execution_time:.3f}s",
            "status": "error",
            "error": str(e)
        }

async def stream_scrapers_with_progress(websocket: WebSocket, location: str, category: Optional[str] = None, limit: int = 30):
    """
    ğŸš€ STREAMING REAL-TIME DE SCRAPERS EN PARALELO
    
    NUEVA IMPLEMENTACIÃ“N:
    - Ejecuta scrapers en paralelo (no secuencial)
    - EnvÃ­a eventos inmediatamente cuando cada scraper completa
    - No espera a que todos terminen para mostrar resultados
    - Usa asyncio.as_completed() para streaming real-time
    """
    try:
        # Inicializar contexto para scrapers
        from services.intent_recognition import intent_service
        
        await websocket.send_json({
            "type": "scraping_started",
            "message": f"ğŸ§  Analizando ubicaciÃ³n: {location}",
            "location": location,
            "timestamp": datetime.now().isoformat()
        })
        
        # AnÃ¡lisis de ubicaciÃ³n con IA
        start_ai = time.time()
        ai_result = await intent_service.get_all_api_parameters(location)
        end_ai = time.time()
        
        detected_location = ai_result.get("location", location)
        detected_country = ai_result.get("country", "")
        detected_province = ai_result.get("province", "")
        detected_city = ai_result.get("city", "")
        
        await websocket.send_json({
            "type": "location_analyzed",
            "message": f"âœ… UbicaciÃ³n analizada: {detected_city}, {detected_province}, {detected_country}",
            "location_data": {
                "original": location,
                "detected": detected_location,
                "city": detected_city,
                "province": detected_province,
                "country": detected_country
            },
            "analysis_time": f"{end_ai - start_ai:.2f}s",
            "timestamp": datetime.now().isoformat()
        })
        
        # Preparar contexto para scrapers
        context_data = {
            'detected_country': detected_country,
            'detected_province': detected_province,
            'detected_city': detected_city
        }
        
        # Usar IndustrialFactory con STREAMING REAL-TIME
        from services.industrial_factory import IndustrialFactory
        from services.dependency_container import get_container
        
        # Obtener container con DI
        container = get_container()
        factory = container.resolve(IndustrialFactory)
        
        # EJECUTAR STREAMING EN PARALELO ğŸš€
        await factory.execute_with_websocket_streaming(
            websocket=websocket,
            location=detected_location,
            category=category,
            limit=limit,
            detected_country=detected_country,
            context_data=context_data
        )
        
        
        # STREAMING REAL-TIME IMPLEMENTADO âœ…
        # El nuevo mÃ©todo execute_with_websocket_streaming() ya maneja todo el streaming
        
        """
        # TODO: Limpiar cÃ³digo corrupto (lÃ­neas comentadas)
                events = await asyncio.wait_for(
                    scraper_instance.scrape_events(detected_location, category, limit),
                    timeout=5.0
                )
                
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                events_count = len(events) if isinstance(events, list) else 0
                
                if events_count > 0:
                    total_events.extend(events)
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'success',
                    'events_count': events_count,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_completed",
                    "scraper_name": scraper_name.title(),
                    "message": f"âœ… {scraper_name.title()}: {events_count} eventos en {execution_time:.2f}s",
                    "events_count": events_count,
                    "execution_time": f"{execution_time:.2f}s",
                    "events": events[:3] if events_count > 0 else [],  # Solo primeros 3 como preview
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
                
            except asyncio.TimeoutError:
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'timeout',
                    'events_count': 0,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_timeout",
                    "scraper_name": scraper_name.title(),
                    "message": f"â° {scraper_name.title()}: Timeout despuÃ©s de 5s",
                    "execution_time": f"{execution_time:.2f}s",
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                scraper_end = time.time()
                execution_time = scraper_end - scraper_start
                
                scraper_results.append({
                    'name': scraper_name.title(),
                    'status': 'error',
                    'events_count': 0,
                    'execution_time': f'{execution_time:.2f}s'
                })
                
                await websocket.send_json({
                    "type": "scraper_error",
                    "scraper_name": scraper_name.title(),
                    "message": f"âŒ {scraper_name.title()}: {str(e)}",
                    "error": str(e),
                    "execution_time": f"{execution_time:.2f}s",
                    "progress": current_progress + progress_step,
                    "timestamp": datetime.now().isoformat()
                })
            
            current_progress += progress_step
        
        # Enviar resultado final
        await websocket.send_json({
            "type": "scraping_completed",
            "message": f"ğŸ¯ Scraping completado: {len(total_events)} eventos totales",
            "total_events": len(total_events),
            "scrapers_summary": scraper_results,
            "successful_scrapers": len([s for s in scraper_results if s['status'] == 'success']),
            "total_scrapers": len(global_scrapers),
            "events": total_events,  # Todos los eventos
            "progress": 100,
            "timestamp": datetime.now().isoformat()
        })
        """
        
    except Exception as e:
        logger.error(f"âŒ Error en streaming de scrapers: {str(e)}")
        await websocket.send_json({
            "type": "scraping_error",
            "message": f"âŒ Error general: {str(e)}",
            "error": str(e),
            "progress": 0,
            "timestamp": datetime.now().isoformat()
        })

async def stream_events_optimized(websocket: WebSocket, location: str):
    """
    ğŸš€ STREAMING OPTIMIZADO CON INDUSTRIAL FACTORY
    
    Entrega eventos en tiempo real usando:
    - Sistema de cachÃ© JSON (0 llamadas a IA)
    - Solo scrapers habilitados (2 en lugar de 9) 
    - Streaming por scraper individual (no esperar a todos)
    """
    try:
        await websocket.send_json({
            "type": "search_started",
            "message": f"ğŸš€ BÃºsqueda optimizada iniciada para {location}",
            "location": location,
            "enabled_scrapers": ["Eventbrite", "Meetup"],
            "progress": 0
        })
        
        # Usar IndustrialFactory para obtener scrapers optimizados
        from services.industrial_factory import IndustrialFactory
        factory = IndustrialFactory()
        
        # Auto-discovery con flags - solo scrapers habilitados
        scrapers_map = factory.discovery_engine.discover_all_scrapers()
        global_scrapers = scrapers_map.get('global', {})
        
        await websocket.send_json({
            "type": "scrapers_discovered",
            "message": f"ğŸ”§ {len(global_scrapers)} scrapers habilitados listos",
            "scrapers": list(global_scrapers.keys()),
            "progress": 10
        })
        
        # Ejecutar scrapers INDIVIDUALMENTE con streaming
        total_events = 0
        scraper_count = len(global_scrapers)
        
        for i, (scraper_name, scraper_instance) in enumerate(global_scrapers.items()):
            scraper_progress = 10 + (i * 80 // scraper_count)
            
            await websocket.send_json({
                "type": "scraper_started",
                "scraper": scraper_name.title(),
                "message": f"âš¡ {scraper_name.title()} iniciado...",
                "progress": scraper_progress
            })
            
            # Ejecutar scraper individual con timeout
            start_time = time.time()
            try:
                events = await asyncio.wait_for(
                    scraper_instance.scrape_events(location, None, 30),
                    timeout=5.0
                )
                execution_time = time.time() - start_time
                
                if events and len(events) > 0:
                    total_events += len(events)
                    
                    # ENTREGAR EVENTOS INMEDIATAMENTE
                    await websocket.send_json({
                        "type": "events_batch",
                        "scraper": scraper_name.title(),
                        "events": events,
                        "count": len(events),
                        "execution_time": f"{execution_time:.2f}s",
                        "total_events": total_events,
                        "progress": scraper_progress + 15,
                        "status": "success"
                    })
                    
                    logger.info(f"ğŸš€ WebSocket - {scraper_name.title()}: {len(events)} eventos enviados en {execution_time:.2f}s")
                else:
                    await websocket.send_json({
                        "type": "scraper_completed",
                        "scraper": scraper_name.title(),
                        "count": 0,
                        "execution_time": f"{execution_time:.2f}s",
                        "message": f"âŒ {scraper_name.title()}: 0 eventos",
                        "progress": scraper_progress + 15,
                        "status": "empty"
                    })
                    
            except asyncio.TimeoutError:
                await websocket.send_json({
                    "type": "scraper_timeout",
                    "scraper": scraper_name.title(),
                    "message": f"â° {scraper_name.title()}: Timeout despuÃ©s de 5s",
                    "progress": scraper_progress + 15,
                    "status": "timeout"
                })
                logger.warning(f"â° WebSocket - {scraper_name.title()}: Timeout")
                
            except Exception as e:
                await websocket.send_json({
                    "type": "scraper_error",
                    "scraper": scraper_name.title(),
                    "message": f"âŒ {scraper_name.title()}: {str(e)}",
                    "progress": scraper_progress + 15,
                    "status": "error"
                })
                logger.error(f"âŒ WebSocket - {scraper_name.title()}: {str(e)}")
        
        # Enviar resumen final
        await websocket.send_json({
            "type": "search_completed",
            "total_events": total_events,
            "scrapers_used": len(global_scrapers),
            "message": f"âœ… BÃºsqueda completada: {total_events} eventos totales",
            "progress": 100
        })
        
        logger.info(f"ğŸ¯ WebSocket - BÃºsqueda completada: {total_events} eventos de {len(global_scrapers)} scrapers")
        
    except Exception as e:
        await websocket.send_json({
            "type": "search_error",
            "message": f"âŒ Error en bÃºsqueda: {str(e)}",
            "progress": 100
        })
        logger.error(f"âŒ WebSocket streaming error: {str(e)}")

@app.get("/api/admin")
async def admin_page():
    """Serve admin test page"""
    try:
        with open("/mnt/c/Code/eventos-visualizer/backend/templates/admin.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except Exception as e:
        return HTMLResponse(content=f"<h1>Error loading admin page: {e}</h1>", status_code=500)

if __name__ == "__main__":
    print(f"\nğŸš€ Starting Eventos Visualizer Backend")
    print(f"ğŸ“ Server: {BACKEND_URL}")
    print(f"ğŸ“Š Health: {BACKEND_URL}/health")
    print(f"ğŸ“ Docs: {BACKEND_URL}/docs")
    ws_url = BACKEND_URL.replace('http://', 'ws://').replace('https://', 'wss://')
    print(f"ğŸ”Œ WebSocket: {ws_url}/ws/notifications\n")
    
    uvicorn.run(
        "main:app",
        host=HOST,
        port=BACKEND_PORT,
        reload=False,
        log_level="info"
    )