"""
ðŸŽ¯ EVENT ORCHESTRATOR - Coordinador Principal de Scrapers
Sistema centralizado que orquesta todos los scrapers globales, regionales y locales
"""

import asyncio
import logging
import time
from typing import List, Dict, Any, Optional
from services.industrial_factory import IndustrialFactory
from services.auto_discovery import AutoDiscoveryEngine

logger = logging.getLogger(__name__)

class EventOrchestrator:
    """
    ðŸŽ¯ COORDINADOR PRINCIPAL DE EVENTOS
    
    RESPONSABILIDADES:
    - Coordinar scrapers globales, regionales y locales
    - Ejecutar bÃºsquedas en paralelo optimizado
    - Eliminar duplicados inteligentemente
    - Gestionar timeouts y reintentos
    - Reportar estadÃ­sticas de ejecuciÃ³n
    """
    
    def __init__(self):
        """Inicializa el orchestrador con factory pattern"""
        self.factory = IndustrialFactory()
        
    async def get_events_comprehensive(
        self, 
        location: str, 
        category: Optional[str] = None,
        limit: int = 30
    ) -> Dict[str, Any]:
        """
        ðŸš€ BÃšSQUEDA COMPLETA DE EVENTOS
        
        Args:
            location: Ciudad o regiÃ³n de bÃºsqueda
            category: CategorÃ­a opcional de eventos
            limit: NÃºmero mÃ¡ximo de eventos a retornar
            
        Returns:
            Diccionario con eventos, estadÃ­sticas y metadata
        """
        
        logger.info(f"ðŸŽ¯ EVENT ORCHESTRATOR: Iniciando bÃºsqueda completa para '{location}'")
        
        start_time = time.time()
        all_events = []
        scraper_stats = []
        
        try:
            # ðŸŒ FASE 1: Scrapers Globales (prioritarios)
            logger.info("ðŸŒ Ejecutando scrapers GLOBALES con Industrial Factory...")
            
            global_events = await self.factory.execute_global_scrapers(
                location=location,
                category=category,
                limit=limit,
                context_data={}
            )
            
            logger.info(f"ðŸŒ Industrial Factory completado: {len(global_events)} eventos de 8 scrapers")
            all_events.extend(global_events)
            
            # ðŸ‡¦ðŸ‡· FASE 1.5: Argentina Factory para eventos regionales
            if "argentina" in location.lower() or any(city in location.lower() for city in ["buenos aires", "cÃ³rdoba", "rosario", "mendoza", "salta"]):
                try:
                    from services.regional_factory.argentina.argentina_factory import argentina_factory
                    
                    logger.info(f"ðŸ‡¦ðŸ‡· Ejecutando Argentina Factory para {location}...")
                    argentina_result = await argentina_factory.get_events(location, category, limit)
                    
                    argentina_events = argentina_result.get("events", [])
                    if argentina_events:
                        logger.info(f"ðŸ‡¦ðŸ‡· Argentina Factory: {len(argentina_events)} eventos regionales agregados")
                        all_events.extend(argentina_events)
                    else:
                        logger.info(f"ðŸ‡¦ðŸ‡· Argentina Factory: No se encontraron eventos regionales para {location}")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Error en Argentina Factory: {e}")
            
            # ðŸ”„ FASE 2: Eliminar duplicados
            unique_events = self._remove_duplicates(all_events)
            logger.info(f"ðŸ”„ Duplicados eliminados: {len(all_events)} â†’ {len(unique_events)} eventos Ãºnicos")
            
            # ðŸ“Š Limitar resultados
            final_events = unique_events[:limit] if limit else unique_events
            
            execution_time = time.time() - start_time
            
            return {
                "events": final_events,
                "total_found": len(unique_events),
                "returned": len(final_events),
                "execution_time": f"{execution_time:.2f}s",
                "scrapers_used": "industrial_factory",
                "location_searched": location,
                "category_filter": category
            }
            
        except Exception as e:
            logger.error(f"âŒ Error en Event Orchestrator: {str(e)}")
            return {
                "events": [],
                "total_found": 0,
                "returned": 0,
                "execution_time": f"{time.time() - start_time:.2f}s",
                "error": str(e),
                "location_searched": location,
                "category_filter": category
            }
    
    def _remove_duplicates(self, events: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ðŸ”„ ELIMINACIÃ“N CONSERVADORA DE DUPLICADOS
        
        SOLO elimina duplicados por URL exacta - preserva eventos con tÃ­tulos genÃ©ricos
        """
        
        if not events:
            return []
            
        unique_events = []
        seen_urls = set()
        
        for event in events:
            event_url = event.get('event_url', '').strip()
            
            # Si tiene URL, solo eliminar si la URL ya existe
            if event_url:
                if event_url in seen_urls:
                    continue  # Skip - URL duplicada
                seen_urls.add(event_url)
            
            # Siempre agregar eventos sin URL o con URL nueva
            unique_events.append(event)
        
        return unique_events