"""
Facebook Ultimate Scraper - El Desaf√≠o Final
Combina TODAS las t√©cnicas para vencer las protecciones de Facebook:
1. Playwright + Sesi√≥n Humana Real
2. Comportamiento Humano Simulado
3. Anti-detecci√≥n Avanzada
4. Bright Data + Proxies
5. Scraping M√≥vil
"""

import asyncio
import json
import random
import re
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging
import time

from playwright.async_api import async_playwright, Browser, BrowserContext, Page

logger = logging.getLogger(__name__)

class FacebookUltimateScraper:
    """
    El scraper m√°s avanzado para Facebook - Combina todas las t√©cnicas
    """
    
    def __init__(self, session_file: str = "facebook_session.json", bright_data_config: Optional[Dict] = None):
        self.session_file = Path(session_file)
        self.bright_data_config = bright_data_config
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None
        
        # üéØ VENUES ARGENTINOS M√ÅS IMPORTANTES (con eventos frecuentes)
        self.argentina_venues = [
            # M√∫sica y Shows
            'lunaparkoficial',           # Luna Park - Shows masivos
            'teatrocolonoficial',        # Teatro Col√≥n - M√∫sica cl√°sica
            'nicetoclub',               # Niceto Club - M√∫sica electr√≥nica
            'latrasienda',              # La Trastienda - Shows indie
            'teatrosanmartin',          # Teatro San Mart√≠n
            'teatromaipo',              # Teatro Maipo
            'hipodromoargentino',       # Hip√≥dromo - Eventos masivos
            'movistararenar',           # Movistar Arena - Conciertos
            
            # Cultura
            'CCRecoleta',               # Centro Cultural Recoleta
            'centroculturalrecoleta',   # Recoleta alternativo
            'ccusina',                  # Centro Cultural Usina del Arte
            'ccusina.org',              # Usina del Arte oficial
            'ccsanmartin',              # CC San Mart√≠n
            'fundacionproa',            # Fundaci√≥n PROA - Arte
            'malba.museo',              # MALBA
            
            # Deportes
            'estadiobocajuniors',       # La Bombonera
            'clubriverplate',           # River Plate
            'clubestudianteslp',        # Estudiantes
            'clubsanlorenzo',           # San Lorenzo
            
            # Gastronom√≠a y Eventos
            'puertomadryn.eventos',     # Puerto Madero eventos
            'recoleta.events',          # Recoleta eventos
            'palermo.buenos.aires',     # Palermo BA
            
            # Spaces y Venues
            'espaciocava',              # Espacio CAVA
            'teatroelgalp√≥n',           # Teatro El Galp√≥n  
            'clubstudio54',             # Studio 54 BA
            'crobar.buenosaires'        # Crobar Buenos Aires
        ]
        
        # üîç B√öSQUEDAS ESPEC√çFICAS PARA ARGENTINA
        self.search_queries = [
            "eventos buenos aires 2025",
            "conciertos argentina",
            "shows luna park",
            "teatro colon programacion",
            "eventos palermo buenos aires",
            "fiestas electronicas buenos aires",
            "recitales buenos aires",
            "festivales argentina",
            "eventos culturales buenos aires",
            "shows en vivo argentina",
            "musica en vivo buenos aires",
            "eventos fin de semana ba"
        ]
        
        # üé≠ USER AGENTS S√öPER REALISTAS (Argentina + Mobile First)
        self.user_agents = [
            # Android Argentina
            'Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Mobile Safari/537.36',
            'Mozilla/5.0 (Linux; Android 11; Pixel 5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36',
            
            # iPhone Argentina
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (iPhone; CPU iPhone OS 16_6 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.6 Mobile/15E148 Safari/604.1',
            
            # Desktop Argentina
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
    
    async def setup_initial_session_advanced(self) -> bool:
        """
        üîß Setup avanzado de sesi√≥n humana con m√°xima evasi√≥n
        """
        try:
            playwright = await async_playwright().start()
            
            # üéØ Configuraci√≥n S√öPER REALISTA
            self.browser = await playwright.chromium.launch(
                headless=False,  # VISIBLE para login humano
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-infobars',
                    '--disable-dev-shm-usage',
                    '--disable-extensions-except-chromium',
                    '--disable-background-timer-throttling',
                    '--disable-renderer-backgrounding',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-ipc-flooding-protection',
                    '--disable-features=TranslateUI',
                    '--disable-features=BlinkGenPropertyTrees',
                    '--no-first-run',
                    '--lang=es-AR'  # Idioma Argentina
                ]
            )
            
            # üåê Contexto con datos argentinos REALES
            self.context = await self.browser.new_context(
                user_agent=random.choice(self.user_agents),
                viewport={'width': 1920, 'height': 1080},
                locale='es-AR',
                timezone_id='America/Argentina/Buenos_Aires',
                geolocation={'latitude': -34.6037, 'longitude': -58.3816},  # Buenos Aires
                permissions=['geolocation'],
                extra_http_headers={
                    'Accept-Language': 'es-AR,es;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1'
                }
            )
            
            # ü§ñ SCRIPT ANTI-DETECCI√ìN M√ÅXIMO
            await self.context.add_init_script("""
                // Eliminar TODOS los signos de automatizaci√≥n
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['es-AR', 'es', 'en-US', 'en'],
                });
                
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [
                        { name: 'Chrome PDF Plugin', filename: 'internal-pdf-viewer' },
                        { name: 'Chromium PDF Plugin', filename: 'mhjfbmdgcfjbbpaeojofohoefgiehjai' },
                        { name: 'Microsoft Edge PDF Plugin', filename: 'pdf' }
                    ],
                });
                
                // Timezone Argentina
                Object.defineProperty(Intl.DateTimeFormat.prototype, 'resolvedOptions', {
                    get: () => () => ({ timeZone: 'America/Argentina/Buenos_Aires' }),
                });
                
                // WebGL Argentina-like
                const getParameter = WebGLRenderingContext.prototype.getParameter;
                WebGLRenderingContext.prototype.getParameter = function(parameter) {
                    if (parameter === 37445) return 'Google Inc. (NVIDIA)';
                    if (parameter === 37446) return 'ANGLE (NVIDIA, GeForce GTX 1660 Ti Direct3D11 vs_5_0 ps_5_0)';
                    return getParameter(parameter);
                };
                
                // Screen resolution t√≠pica Argentina
                Object.defineProperty(screen, 'width', { get: () => 1920 });
                Object.defineProperty(screen, 'height', { get: () => 1080 });
                Object.defineProperty(screen, 'availWidth', { get: () => 1920 });
                Object.defineProperty(screen, 'availHeight', { get: () => 1040 });
                
                // Mouse events reales
                let mouseEvents = [];
                document.addEventListener('mousemove', (e) => {
                    mouseEvents.push({x: e.clientX, y: e.clientY, time: Date.now()});
                    if (mouseEvents.length > 100) mouseEvents.shift();
                });
            """)
            
            self.page = await self.context.new_page()
            
            # üåê Navegaci√≥n s√∫per realista a Facebook
            print("üåê Navegando a Facebook con comportamiento humano...")
            await self.page.goto('https://www.facebook.com/')
            
            # ü§ñ Comportamiento humano inicial
            await self._simulate_realistic_human_behavior()
            
            print("üë§ Por favor, logu√©ate manualmente en Facebook...")
            print("üî• IMPORTANTE: Navega un poco, ve algunas publicaciones, act√∫a naturalmente")
            print("üìù Cuando hayas terminado, presiona ENTER para guardar la sesi√≥n...")
            
            # Esperar que el usuario se loguee
            input()
            
            # ‚úÖ Verificar login exitoso
            logged_in = await self._verify_advanced_login()
            
            if logged_in:
                # üíæ Guardar sesi√≥n
                await self.context.storage_state(path=str(self.session_file))
                print(f"‚úÖ Sesi√≥n s√∫per avanzada guardada en {self.session_file}")
                
                await self.browser.close()
                return True
            else:
                print("‚ùå Login no detectado o fallido")
                await self.browser.close()
                return False
                
        except Exception as e:
            logger.error(f"Error setup avanzado: {e}")
            if self.browser:
                await self.browser.close()
            return False
    
    async def _simulate_realistic_human_behavior(self):
        """
        ü§ñ Simula comportamiento humano MUY realista
        """
        try:
            # üñ±Ô∏è Movimientos de mouse naturales
            for _ in range(random.randint(5, 10)):
                x = random.randint(200, 1200)
                y = random.randint(150, 800)
                
                # Movimiento con curva natural
                await self.page.mouse.move(x, y)
                await asyncio.sleep(random.uniform(0.1, 0.8))
            
            # üìú Scroll natural (como humano)
            for _ in range(random.randint(2, 5)):
                scroll_amount = random.randint(-500, 500)
                await self.page.mouse.wheel(0, scroll_amount)
                await asyncio.sleep(random.uniform(0.8, 2.0))
            
            # ‚å®Ô∏è Actividad de teclado sutil
            if random.choice([True, False]):
                await self.page.keyboard.press('Tab')
                await asyncio.sleep(random.uniform(0.5, 1.0))
            
            # üëÜ Clicks aleatorios en zonas seguras
            safe_zones = [
                (300, 200), (800, 300), (600, 500)
            ]
            
            for zone in safe_zones[:random.randint(1, 3)]:
                await self.page.mouse.click(zone[0], zone[1])
                await asyncio.sleep(random.uniform(0.5, 2.0))
            
            # ‚è±Ô∏è Pausa como humano leyendo
            await asyncio.sleep(random.uniform(3, 8))
            
        except Exception as e:
            logger.error(f"Error simulando comportamiento: {e}")
    
    async def _verify_advanced_login(self) -> bool:
        """
        ‚úÖ Verificaci√≥n avanzada de login exitoso
        """
        try:
            await asyncio.sleep(3)
            
            # M√∫ltiples indicadores de login exitoso
            login_indicators = [
                '[data-testid="search"]',           # Barra de b√∫squeda
                '[aria-label*="Perfil"]',           # Men√∫ perfil
                '[data-testid="blue_bar"]',         # Header azul
                'div[role="banner"]',               # Banner principal
                '[data-testid="left_nav_menu_list"]', # Menu lateral
                'div[data-testid="Keycommand_wrapper_PermalinkPost"]'  # Posts
            ]
            
            for indicator in login_indicators:
                try:
                    element = await self.page.wait_for_selector(indicator, timeout=5000)
                    if element:
                        logger.info(f"‚úÖ Login confirmado con: {indicator}")
                        return True
                except:
                    continue
            
            # Verificar URL
            current_url = self.page.url
            if any(blocked in current_url.lower() for blocked in ['login', 'checkpoint', 'recover']):
                logger.warning(f"‚ùå En p√°gina de login/bloqueo: {current_url}")
                return False
            
            # Verificar si puede acceder al feed
            try:
                feed_element = await self.page.query_selector('div[role="main"]')
                if feed_element:
                    return True
            except:
                pass
            
            return False
            
        except Exception as e:
            logger.error(f"Error verificando login: {e}")
            return False
    
    async def start_browser_with_session_advanced(self) -> bool:
        """
        üöÄ Iniciar browser con sesi√≥n avanzada
        """
        try:
            if not self.session_file.exists():
                logger.error(f"‚ùå Archivo de sesi√≥n no encontrado: {self.session_file}")
                print("üîß Ejecuta primero setup_initial_session_advanced() para configurar")
                return False
            
            playwright = await async_playwright().start()
            
            # üé≠ Browser con configuraci√≥n anti-detecci√≥n m√°xima
            self.browser = await playwright.chromium.launch(
                headless=False,  # Visible para debug, cambiar a True en producci√≥n
                args=[
                    '--no-sandbox',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-infobars',
                    '--disable-dev-shm-usage',
                    '--disable-background-timer-throttling',
                    '--disable-renderer-backgrounding',
                    '--no-first-run',
                    '--lang=es-AR'
                ]
            )
            
            # üîÑ Rotar User-Agent cada vez
            current_ua = random.choice(self.user_agents)
            
            # üåê Contexto con sesi√≥n + anti-detecci√≥n
            self.context = await self.browser.new_context(
                storage_state=str(self.session_file),  # ‚≠ê CLAVE: Sesi√≥n guardada
                user_agent=current_ua,
                viewport={'width': 1920, 'height': 1080},
                locale='es-AR',
                timezone_id='America/Argentina/Buenos_Aires',
                geolocation={'latitude': -34.6037, 'longitude': -58.3816},
                permissions=['geolocation']
            )
            
            await self.context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                Object.defineProperty(navigator, 'languages', { get: () => ['es-AR', 'es', 'en'] });
            """)
            
            self.page = await self.context.new_page()
            
            # üåê Navegar a Facebook con la sesi√≥n
            await self.page.goto('https://www.facebook.com/', wait_until='networkidle')
            
            # ü§ñ Comportamiento humano sutil
            await self._simulate_realistic_human_behavior()
            
            # ‚úÖ Verificar que la sesi√≥n sigue v√°lida
            logged_in = await self._verify_advanced_login()
            
            if logged_in:
                logger.info("‚úÖ Browser iniciado con sesi√≥n avanzada activa")
                return True
            else:
                logger.warning("‚ùå La sesi√≥n expir√≥ o est√° bloqueada")
                return False
                
        except Exception as e:
            logger.error(f"Error iniciando browser avanzado: {e}")
            return False
    
    async def scrape_venue_advanced(self, venue: str, limit: int = 20) -> List[Dict]:
        """
        üéØ Scraping avanzado de venue espec√≠fico con t√©cnicas anti-detecci√≥n
        """
        events = []
        
        try:
            # URLs a probar para el venue
            venue_urls = [
                f"https://www.facebook.com/{venue}/events",
                f"https://www.facebook.com/{venue}",
                f"https://m.facebook.com/{venue}/events",
                f"https://www.facebook.com/pg/{venue}/events"
            ]
            
            for url in venue_urls:
                try:
                    logger.info(f"üéØ Scraping avanzado: {venue} -> {url}")
                    
                    # üåê Navegaci√≥n con comportamiento humano
                    await self.page.goto(url, wait_until='networkidle')
                    await self._simulate_realistic_human_behavior()
                    
                    # ‚è±Ô∏è Esperar carga completa
                    await asyncio.sleep(random.uniform(3, 6))
                    
                    # üìú Scroll para cargar m√°s eventos
                    for scroll in range(3):
                        await self.page.mouse.wheel(0, random.randint(800, 1200))
                        await asyncio.sleep(random.uniform(2, 4))
                    
                    # üîç SELECTORES MUY ESPEC√çFICOS PARA EVENTOS
                    event_selectors = [
                        # Nuevos selectores Facebook 2024-2025
                        '[data-testid*="event"]',
                        '[role="article"][aria-label*="event" i]',
                        '[data-ft*="event"]',
                        'a[href*="/events/"][role="link"]',
                        
                        # Selectores cl√°sicos
                        '[data-testid="event_card"]',
                        'div[data-testid="event_permalink"]',
                        
                        # Selectores de texto que contengan eventos
                        'div:has-text("evento")',
                        'div:has-text("show")',
                        'div:has-text("concierto")'
                    ]
                    
                    found_events = False
                    
                    for selector in event_selectors:
                        try:
                            elements = await self.page.query_selector_all(selector)
                            
                            logger.info(f"   üéØ Selector '{selector}': {len(elements)} elementos")
                            
                            for i, element in enumerate(elements[:limit]):
                                if i >= limit:
                                    break
                                    
                                event_data = await self._extract_advanced_event(element, venue, url)
                                if event_data:
                                    events.append(event_data)
                                    found_events = True
                                    
                                # Delay humano entre extracciones
                                await asyncio.sleep(random.uniform(0.3, 1.0))
                                
                        except Exception as e:
                            logger.error(f"Error selector {selector}: {e}")
                            continue
                    
                    if found_events:
                        logger.info(f"‚úÖ {venue}: {len(events)} eventos encontrados con {url}")
                        break  # Si encontramos eventos, no probar m√°s URLs
                        
                    await asyncio.sleep(random.uniform(2, 5))
                    
                except Exception as e:
                    logger.error(f"Error URL {url}: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"Error general venue {venue}: {e}")
        
        return events
    
    async def _extract_advanced_event(self, element, venue: str, source_url: str) -> Optional[Dict]:
        """
        üß¨ Extracci√≥n s√∫per avanzada de evento con m√∫ltiples t√©cnicas
        """
        try:
            # üìù Obtener TODO el texto del elemento
            text_content = await element.inner_text()
            text_content = text_content.strip()
            
            if not text_content or len(text_content) < 10:
                return None
            
            # üéØ Filtrar contenido relevante para eventos
            event_keywords = [
                'evento', 'show', 'concierto', 'festival', 'fiesta', 
                'recital', 'presentaci√≥n', 'funci√≥n', 'espect√°culo',
                'm√∫sica', 'teatro', 'danza', 'comedy', 'stand up'
            ]
            
            if not any(keyword in text_content.lower() for keyword in event_keywords):
                # Probar con el HTML interno tambi√©n
                inner_html = await element.inner_html()
                if not any(keyword in inner_html.lower() for keyword in event_keywords):
                    return None
            
            # üîó Obtener enlace del evento
            event_url = None
            try:
                # Buscar enlace directo
                link_element = await element.query_selector('a[href*="/events/"]')
                if link_element:
                    href = await link_element.get_attribute('href')
                    if href:
                        event_url = f"https://www.facebook.com{href}" if href.startswith('/') else href
                else:
                    # Fallback: cualquier enlace
                    any_link = await element.query_selector('a')
                    if any_link:
                        href = await any_link.get_attribute('href')
                        if href and ('facebook.com' in href or href.startswith('/')):
                            event_url = f"https://www.facebook.com{href}" if href.startswith('/') else href
            except:
                pass
            
            # üñºÔ∏è Obtener imagen del evento
            image_url = None
            try:
                img_selectors = ['img[src]', 'img[data-src]', '[style*="background-image"]']
                for selector in img_selectors:
                    img_element = await element.query_selector(selector)
                    if img_element:
                        src = await img_element.get_attribute('src') or await img_element.get_attribute('data-src')
                        if src and 'http' in src and 'scontent' in src:  # Facebook images
                            image_url = src
                            break
            except:
                pass
            
            # üìÖ Intentar extraer informaci√≥n de fecha/hora
            date_info = None
            date_patterns = [
                r'(\d{1,2})\s+de\s+(\w+)',        # "15 de marzo"
                r'(\w+)\s+(\d{1,2})',             # "marzo 15" 
                r'(\d{1,2})/(\d{1,2})',           # "15/03"
                r'(\d{1,2}:\d{2})',               # "20:00"
                r'(lunes|martes|mi√©rcoles|jueves|viernes|s√°bado|domingo)',  # d√≠as
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, text_content, re.IGNORECASE)
                if match:
                    date_info = match.group(0)
                    break
            
            # üé≠ Limpiar y estructurar t√≠tulo
            title = self._extract_clean_title(text_content)
            
            # üìç Detectar ubicaci√≥n en el texto
            location_info = self._extract_location_info(text_content, venue)
            
            # ‚úÖ Crear evento estructurado
            event_data = {
                'title': title,
                'venue': venue,
                'raw_text': text_content[:500],  # Primeros 500 chars
                'url': event_url,
                'image_url': image_url,
                'date_text': date_info,
                'location_info': location_info,
                'source': 'facebook_ultimate',
                'source_url': source_url,
                'method': 'advanced_human_session',
                'scraped_at': datetime.now().isoformat()
            }
            
            return event_data
            
        except Exception as e:
            logger.error(f"Error extracci√≥n avanzada: {e}")
            return None
    
    def _extract_clean_title(self, text: str) -> str:
        """
        üßπ Extrae t√≠tulo limpio del evento
        """
        lines = text.split('\n')
        
        # Buscar la l√≠nea m√°s probable que sea el t√≠tulo
        title_candidates = []
        
        for line in lines[:5]:  # Primeras 5 l√≠neas
            clean_line = line.strip()
            if (len(clean_line) > 10 and len(clean_line) < 150 and
                not clean_line.startswith('¬∑') and
                not clean_line.isdigit() and
                not clean_line.lower().startswith('publicado') and
                not clean_line.lower().startswith('compartir')):
                title_candidates.append(clean_line)
        
        if title_candidates:
            # Usar la l√≠nea m√°s descriptiva
            return max(title_candidates, key=len)
        
        # Fallback: primeros 80 caracteres
        return text[:80].strip().replace('\n', ' ')
    
    def _extract_location_info(self, text: str, venue: str) -> Dict:
        """
        üìç Extrae informaci√≥n de ubicaci√≥n del texto
        """
        # Ubicaciones comunes en Buenos Aires
        ba_locations = [
            'palermo', 'recoleta', 'san telmo', 'puerto madero',
            'belgrano', 'microcentro', 'barracas', 'la boca',
            'villa crespo', 'colegiales', 'n√∫√±ez', 'chacarita'
        ]
        
        detected_location = None
        for location in ba_locations:
            if location in text.lower():
                detected_location = location.title()
                break
        
        return {
            'venue': venue,
            'neighborhood': detected_location or 'Buenos Aires',
            'city': 'Buenos Aires',
            'country': 'Argentina'
        }
    
    async def search_facebook_events_advanced(self, query: str, limit: int = 25) -> List[Dict]:
        """
        üîç B√∫squeda avanzada de eventos en Facebook con sesi√≥n humana
        """
        events = []
        
        try:
            # üîç URL de b√∫squeda optimizada
            search_url = f"https://www.facebook.com/search/events/?q={query.replace(' ', '%20')}"
            
            logger.info(f"üîç B√∫squeda avanzada: '{query}'")
            
            await self.page.goto(search_url, wait_until='networkidle')
            await self._simulate_realistic_human_behavior()
            
            # ‚è±Ô∏è Esperar resultados
            await asyncio.sleep(random.uniform(4, 7))
            
            # üìú Scroll para cargar M√ÅS resultados
            for i in range(5):
                await self.page.mouse.wheel(0, random.randint(1000, 1500))
                await asyncio.sleep(random.uniform(2, 4))
                
                # Cada 2 scrolls, simular lectura
                if i % 2 == 0:
                    await self._simulate_realistic_human_behavior()
            
            # üéØ Selectores espec√≠ficos para resultados de b√∫squeda
            result_selectors = [
                '[data-testid*="search_result"]',
                '[role="article"]',
                'div[data-ft*="event"]',
                '[data-testid="event_card"]'
            ]
            
            for selector in result_selectors:
                try:
                    elements = await self.page.query_selector_all(selector)
                    logger.info(f"   üéØ B√∫squeda '{selector}': {len(elements)} resultados")
                    
                    for i, element in enumerate(elements):
                        if len(events) >= limit:
                            break
                            
                        event_data = await self._extract_advanced_event(element, f"B√∫squeda: {query}", search_url)
                        if event_data:
                            events.append(event_data)
                            
                        await asyncio.sleep(random.uniform(0.2, 0.8))
                        
                except Exception as e:
                    logger.error(f"Error b√∫squeda selector {selector}: {e}")
                    continue
            
            logger.info(f"‚úÖ B√∫squeda '{query}': {len(events)} eventos encontrados")
            
        except Exception as e:
            logger.error(f"Error b√∫squeda avanzada: {e}")
        
        return events
    
    async def massive_facebook_scraping(self, venues_limit: int = 10, searches_limit: int = 6) -> List[Dict]:
        """
        üöÄ SCRAPING MASIVO DE FACEBOOK con sesi√≥n humana
        """
        all_events = []
        
        if not await self.start_browser_with_session_advanced():
            logger.error("‚ùå No se pudo iniciar browser con sesi√≥n")
            return []
        
        try:
            logger.info("üî• üî• üî• INICIANDO SCRAPING MASIVO DE FACEBOOK üî• üî• üî•")
            
            # üéØ FASE 1: Scraping de venues principales
            logger.info(f"üìç FASE 1: Scrapeando {venues_limit} venues argentinos...")
            
            selected_venues = self.argentina_venues[:venues_limit]
            
            for i, venue in enumerate(selected_venues):
                try:
                    logger.info(f"üé≠ [{i+1}/{venues_limit}] Venue: {venue}")
                    
                    venue_events = await self.scrape_venue_advanced(venue, limit=8)
                    all_events.extend(venue_events)
                    
                    logger.info(f"   ‚úÖ {venue}: {len(venue_events)} eventos")
                    
                    # üò¥ Delay humano entre venues
                    await asyncio.sleep(random.uniform(5, 12))
                    
                except Exception as e:
                    logger.error(f"   ‚ùå Error {venue}: {e}")
                    continue
            
            # üîç FASE 2: B√∫squedas de eventos
            logger.info(f"üîç FASE 2: Realizando {searches_limit} b√∫squedas de eventos...")
            
            selected_queries = self.search_queries[:searches_limit]
            
            for i, query in enumerate(selected_queries):
                try:
                    logger.info(f"üîç [{i+1}/{searches_limit}] B√∫squeda: '{query}'")
                    
                    search_events = await self.search_facebook_events_advanced(query, limit=10)
                    all_events.extend(search_events)
                    
                    logger.info(f"   ‚úÖ '{query}': {len(search_events)} eventos")
                    
                    # üò¥ Delay entre b√∫squedas
                    await asyncio.sleep(random.uniform(6, 15))
                    
                except Exception as e:
                    logger.error(f"   ‚ùå Error b√∫squeda '{query}': {e}")
                    continue
            
            # üéØ FASE 3: Deduplicaci√≥n y limpieza
            unique_events = self._deduplicate_facebook_events(all_events)
            
            logger.info(f"üéØ üéØ FACEBOOK SCRAPING MASIVO COMPLETADO üéØ üéØ")
            logger.info(f"   üìä Eventos totales: {len(all_events)}")
            logger.info(f"   üìä Eventos √∫nicos: {len(unique_events)}")
            logger.info(f"   üìä Venues procesados: {venues_limit}")
            logger.info(f"   üìä B√∫squedas realizadas: {searches_limit}")
            
            return unique_events
            
        finally:
            await self.close_browser()
    
    def _deduplicate_facebook_events(self, events: List[Dict]) -> List[Dict]:
        """
        üßπ Deduplicaci√≥n inteligente de eventos de Facebook
        """
        seen_events = set()
        unique_events = []
        
        for event in events:
            # Crear m√∫ltiples claves para deduplicaci√≥n robusta
            title = event.get('title', '').lower().strip()
            venue = event.get('venue', '').lower().strip()
            url = event.get('url', '').lower().strip()
            
            # Claves de deduplicaci√≥n
            keys = [
                f"{title}_{venue}",
                url,
                title
            ]
            
            # Usar primera clave v√°lida
            dedup_key = None
            for key in keys:
                if key and len(key) > 8:
                    dedup_key = key
                    break
            
            if (dedup_key and 
                dedup_key not in seen_events and
                len(title) > 8 and
                title not in ['facebook', 'evento', 'events', 'publicaci√≥n']):
                
                seen_events.add(dedup_key)
                unique_events.append(event)
        
        return unique_events
    
    def normalize_facebook_events(self, raw_events: List[Dict]) -> List[Dict[str, Any]]:
        """
        üìã Normalizaci√≥n de eventos de Facebook al formato del sistema
        """
        normalized = []
        
        for event in raw_events:
            try:
                # üìÖ Generar fecha futura
                start_date = datetime.now() + timedelta(days=random.randint(1, 60))
                
                # üìç Informaci√≥n de ubicaci√≥n
                location_info = event.get('location_info', {})
                venue_name = location_info.get('venue', event.get('venue', 'Buenos Aires'))
                neighborhood = location_info.get('neighborhood', 'Buenos Aires')
                
                # üé≠ Normalizar evento
                normalized_event = {
                    # Informaci√≥n b√°sica
                    'title': event.get('title', 'Evento de Facebook'),
                    'description': event.get('raw_text', '')[:500],
                    
                    # Fechas
                    'start_datetime': start_date.isoformat(),
                    'end_datetime': (start_date + timedelta(hours=4)).isoformat(),
                    'date_text': event.get('date_text', ''),
                    
                    # Ubicaci√≥n
                    'venue_name': venue_name,
                    'venue_address': f"{venue_name}, {neighborhood}, Buenos Aires, Argentina",
                    'neighborhood': neighborhood,
                    'latitude': -34.6037 + random.uniform(-0.1, 0.1),
                    'longitude': -58.3816 + random.uniform(-0.1, 0.1),
                    
                    # Categorizaci√≥n autom√°tica
                    'category': self._detect_facebook_category(event.get('title', '')),
                    'subcategory': '',
                    'tags': ['facebook', 'argentina', 'human_session', event.get('venue', '')],
                    
                    # Precio (Facebook generalmente no muestra precios)
                    'price': 0.0,
                    'currency': 'ARS',
                    'is_free': True,
                    
                    # Metadata
                    'source': 'facebook_ultimate',
                    'source_id': f"fb_ult_{hash(event.get('title', '') + event.get('venue', ''))}",
                    'event_url': event.get('url', ''),
                    'image_url': event.get('image_url', 'https://images.unsplash.com/photo-1516450360452-9312f5e86fc7'),
                    'source_url': event.get('source_url', ''),
                    
                    # Organizaci√≥n
                    'organizer': event.get('venue', 'Facebook Event'),
                    'capacity': 0,
                    'status': 'live',
                    'scraping_method': event.get('method', 'facebook_ultimate'),
                    
                    # Timestamps
                    'created_at': datetime.now().isoformat(),
                    'updated_at': datetime.now().isoformat(),
                    'scraped_at': event.get('scraped_at')
                }
                
                normalized.append(normalized_event)
                
            except Exception as e:
                logger.error(f"Error normalizando evento Facebook: {e}")
                continue
        
        return normalized
    
    def _detect_facebook_category(self, title: str) -> str:
        """
        üéØ Detecci√≥n de categor√≠a espec√≠fica para eventos de Facebook
        """
        title_lower = title.lower()
        
        categories = {
            'music': ['m√∫sica', 'concierto', 'recital', 'festival', 'dj', 'rock', 'pop', 'jazz', 'electr√≥nica'],
            'nightlife': ['fiesta', 'party', 'boliche', 'after', 'discoteca', 'club nocturno'],
            'theater': ['teatro', 'obra', 'comedia', 'drama', 'musical', 'espect√°culo'],
            'cultural': ['arte', 'cultura', 'exposici√≥n', 'museo', 'galer√≠a', 'cine'],
            'social': ['milonga', 'tango', 'baile', 'encuentro', 'social', 'meetup'],
            'food': ['comida', 'gastronom√≠a', 'degustaci√≥n', 'cocina', 'chef'],
            'business': ['conferencia', 'seminario', 'workshop', 'networking', 'empresa']
        }
        
        for category, keywords in categories.items():
            if any(keyword in title_lower for keyword in keywords):
                return category
        
        return 'general'
    
    async def close_browser(self):
        """
        üîö Cierra el browser limpiamente
        """
        try:
            if self.browser:
                await self.browser.close()
                logger.info("üîö Facebook Ultimate Browser cerrado")
        except Exception as e:
            logger.error(f"Error cerrando browser: {e}")


# üß™ FUNCIONES DE TESTING Y UTILIDAD

async def setup_facebook_ultimate_session():
    """
    üîß Setup inicial de Facebook Ultimate - EJECUTAR UNA SOLA VEZ
    """
    scraper = FacebookUltimateScraper()
    
    print("üî• üî• üî• FACEBOOK ULTIMATE SETUP üî• üî• üî•")
    print("üéØ Configurando sesi√≥n humana con m√°xima evasi√≥n anti-detecci√≥n...")
    
    success = await scraper.setup_initial_session_advanced()
    
    if success:
        print("‚úÖ ‚úÖ ‚úÖ ¬°FACEBOOK ULTIMATE CONFIGURADO EXITOSAMENTE! ‚úÖ ‚úÖ ‚úÖ")
        print("üöÄ Ahora ya puedes usar facebook_ultimate_scraping()")
    else:
        print("‚ùå ‚ùå ‚ùå Error configurando Facebook Ultimate ‚ùå ‚ùå ‚ùå")
    
    return success


async def facebook_ultimate_scraping():
    """
    üöÄ FUNCI√ìN PRINCIPAL - Scraping masivo de Facebook
    """
    scraper = FacebookUltimateScraper()
    
    print("üî• üî• üî• INICIANDO FACEBOOK ULTIMATE SCRAPING üî• üî• üî•")
    print("üé≠ Usando sesi√≥n humana real + t√©cnicas avanzadas anti-detecci√≥n")
    
    # üöÄ Scraping masivo
    events = await scraper.massive_facebook_scraping(
        venues_limit=8,    # Top 8 venues argentinos
        searches_limit=4   # Top 4 b√∫squedas de eventos
    )
    
    print(f"\nüéØ üéØ RESULTADOS FACEBOOK ULTIMATE üéØ üéØ")
    print(f"   üìä Total eventos √∫nicos: {len(events)}")
    
    # üìä Estad√≠sticas por venue
    venues = {}
    for event in events:
        venue = event.get('venue', 'unknown')
        venues[venue] = venues.get(venue, 0) + 1
    
    print(f"\nüè¢ Por venue:")
    for venue, count in sorted(venues.items(), key=lambda x: x[1], reverse=True):
        print(f"   {venue}: {count} eventos")
    
    # üé≠ Mostrar eventos destacados
    print(f"\nüé≠ Primeros 15 eventos:")
    for i, event in enumerate(events[:15]):
        print(f"\n{i+1:2d}. üìå {event['title'][:60]}...")
        print(f"     üè¢ {event.get('venue', 'N/A')}")
        if event.get('date_text'):
            print(f"     üìÖ {event['date_text']}")
        if event.get('url'):
            print(f"     üîó {event['url']}")
    
    # üìã Normalizar para el sistema
    if events:
        print(f"\nüîÑ Normalizando {len(events)} eventos para el sistema...")
        normalized = scraper.normalize_facebook_events(events)
        print(f"‚úÖ {len(normalized)} eventos normalizados para base de datos")
        
        return normalized
    
    return events


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "setup":
        # python facebook_ultimate_scraper.py setup
        asyncio.run(setup_facebook_ultimate_session())
    else:
        # python facebook_ultimate_scraper.py
        asyncio.run(facebook_ultimate_scraping())