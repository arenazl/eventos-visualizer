# ğŸ¤– INSTRUCCIONES PARA SCRAPING AUTÃ“NOMO

**Ãšltima actualizaciÃ³n:** 2025-11-09 08:00

---

## ğŸ¯ OBJETIVO

Completar el scraping de las **49 regiones pendientes** de forma autÃ³noma sin intervenciÃ³n manual.

**Progreso actual:** 31/80 regiones (38.75%)
**Falta:** 49 regiones (61.25%)
**Tiempo estimado:** 2.5 - 3 horas

---

## ğŸš€ EJECUCIÃ“N RÃPIDA (UN SOLO COMANDO)

### OpciÃ³n 1: Modo TEST (Probar con 2 regiones)
```bash
cd backend
python automation/autonomous_scraping_pipeline.py --mode test
```
**DuraciÃ³n:** ~6-8 minutos
**Procesa:** ManabÃ­ (Ecuador) + La Paz (Bolivia)

### OpciÃ³n 2: Modo CONTINUAR (Recomendado)
```bash
cd backend
python automation/autonomous_scraping_pipeline.py --mode continue
```
**DuraciÃ³n:** ~2.5-3 horas
**Procesa:** Las 49 regiones pendientes automÃ¡ticamente

### OpciÃ³n 3: Modo COMPLETO
```bash
cd backend
python automation/autonomous_scraping_pipeline.py --mode full
```
**Igual que CONTINUAR** pero reprocesa todo si es necesario

---

## ğŸ“‹ QUÃ‰ HACE EL PIPELINE AUTÃ“NOMO

El script `autonomous_scraping_pipeline.py` ejecuta **3 pasos automÃ¡ticos**:

### 1. ğŸ” SCRAPING (Gemini + Playwright)
- Abre Gemini en browser headless
- EnvÃ­a prompt: "eventos prÃ³ximos en {regiÃ³n}, {paÃ­s}..."
- Espera respuesta (20s)
- Guarda en `backend/data/ai_scraped/{region}_gemini_response.json`

### 2. ğŸ§¹ CURACIÃ“N (ValidaciÃ³n + ImÃ¡genes)
- Valida calidad de eventos (elimina datos pobres)
- Detecta y elimina duplicados
- Busca imÃ¡genes de alta calidad (Unsplash)
- Guarda en `backend/data/curated/curated_{region}_gemini_response.json`

### 3. ğŸ’¾ INSERCIÃ“N EN MYSQL
- Inserta eventos curados en tabla `events`
- Maneja duplicados (actualiza si existe)
- Registra en tabla `scraping_runs`

**TODO AUTOMÃTICO - SIN INTERVENCIÃ“N MANUAL**

---

## ğŸ“Š ESTADÃSTICAS ACTUALES

### Ya en Base de Datos
```sql
SELECT COUNT(*) FROM events WHERE source = 'gemini_auto';
-- Resultado esperado: ~97 eventos
```

### Archivos Generados
```bash
# Ver scraping bruto
ls backend/data/ai_scraped/*_gemini_response.json
# Resultado: 31 archivos

# Ver eventos curados
ls backend/data/curated/curated_*_gemini_response.json
# Resultado: 31 archivos

# Ver consolidado
cat backend/data/curated/all_curated_*.json | jq '.stats'
```

---

## ğŸ—ºï¸ REGIONES PENDIENTES (49 regiones)

### Ecuador - 1 regiÃ³n
- ManabÃ­ (Portoviejo)

### Bolivia - 4 regiones
- La Paz
- Santa Cruz
- Cochabamba
- Chuquisaca

### Paraguay - 4 regiones
- AsunciÃ³n
- Central
- Alto ParanÃ¡
- ItapÃºa

### Uruguay - 4 regiones
- Montevideo
- Canelones
- Maldonado
- Salto

### Costa Rica - 4 regiones
- San JosÃ©
- Alajuela
- Cartago
- Heredia

### PanamÃ¡ - 4 regiones
- PanamÃ¡
- ColÃ³n
- ChiriquÃ­
- Bocas del Toro

### Guatemala - 4 regiones
- Guatemala
- Quetzaltenango
- Escuintla
- Alta Verapaz

### Honduras - 4 regiones
- Francisco MorazÃ¡n (Tegucigalpa)
- CortÃ©s (San Pedro Sula)
- AtlÃ¡ntida (La Ceiba)
- Islas de la BahÃ­a (RoatÃ¡n)

### El Salvador - 4 regiones
- San Salvador
- La Libertad
- Santa Ana
- San Miguel

### Nicaragua - 4 regiones
- Managua
- LeÃ³n
- Granada
- Masaya

### RepÃºblica Dominicana - 4 regiones
- Distrito Nacional (Santo Domingo)
- Santo Domingo
- Santiago
- La Altagracia (Punta Cana)

### Cuba - 4 regiones
- La Habana
- Santiago de Cuba
- Villa Clara
- Matanzas

### Puerto Rico - 4 regiones
- San Juan
- BayamÃ³n
- Ponce
- MayagÃ¼ez

---

## âš™ï¸ CONFIGURACIÃ“N

### Variables de Entorno (.env)
```bash
# MySQL Database
DATABASE_URL=mysql+pymysql://root:password@localhost:3306/eventos_db

# Gemini API (opcional - usa web interface)
GEMINI_API_KEY=tu_api_key_aqui
```

### Archivos de ConfiguraciÃ³n
- **Regiones:** `backend/data/latinamerica_regions_sample.json` (80 regiones)
- **Prompt:** `backend/scripts/prompt_config.py` (editable)
- **Pipeline:** `backend/automation/autonomous_scraping_pipeline.py`

---

## ğŸ”§ SOLUCIÃ“N DE PROBLEMAS

### Error: "Playwright no instalado"
```bash
pip install playwright
playwright install
```

### Error: "No se puede conectar a MySQL"
```bash
# Verificar que MySQL estÃ¡ corriendo
netstat -ano | findstr :3306

# Verificar credenciales en .env
cat .env | grep DATABASE_URL
```

### Error: "Browser se queda colgado"
- El script usa `headless=True` (sin UI)
- Cada regiÃ³n tiene timeout de 20s
- Si falla, continÃºa con la siguiente

### Ver logs en tiempo real
```bash
cd backend
python automation/autonomous_scraping_pipeline.py --mode continue 2>&1 | tee scraping.log
```

---

## ğŸ“ˆ MONITOREO DEL PROGRESO

### Ver eventos insertados en tiempo real
```sql
-- Abrir MySQL Workbench o CLI
SELECT COUNT(*), country
FROM events
WHERE source = 'gemini_auto'
GROUP BY country
ORDER BY COUNT(*) DESC;
```

### Ver archivos generados
```bash
# Contar regiones procesadas
ls backend/data/ai_scraped/*_gemini_response.json | wc -l

# Ver Ãºltimas 5 regiones procesadas
ls -lt backend/data/ai_scraped/*.json | head -5
```

### Verificar scraping_runs
```sql
SELECT * FROM scraping_runs
ORDER BY started_at DESC
LIMIT 10;
```

---

## ğŸ¯ RESULTADOS ESPERADOS

### Al finalizar (80/80 regiones)
```
ğŸ“Š ESTADÃSTICAS FINALES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” Regiones scrapeadas:      49
ğŸ“¥ Eventos encontrados:       ~150-200
âœ… Eventos curados:           ~125-170 (85% calidad)
ğŸ’¾ Eventos insertados:        ~125-170
ğŸ”„ Eventos actualizados:      ~10-20
âŒ Errores:                   <5
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Base de datos final
```sql
SELECT COUNT(*) FROM events WHERE source = 'gemini_auto';
-- Esperado: ~220-270 eventos totales (97 actuales + 125-170 nuevos)
```

---

## ğŸ“ SIGUIENTE PASO DESPUÃ‰S DEL SCRAPING

Una vez completado el scraping autÃ³nomo:

### 1. Verificar calidad de datos
```bash
cd backend
python -c "
from sqlalchemy import create_engine
import os
from dotenv import load_dotenv

load_dotenv()
engine = create_engine(os.getenv('DATABASE_URL'))

with engine.connect() as conn:
    result = conn.execute('SELECT country, COUNT(*) as total FROM events WHERE source=\"gemini_auto\" GROUP BY country')
    print('\nğŸ“Š EVENTOS POR PAÃS:')
    for row in result:
        print(f'{row[0]:25} {row[1]:>5} eventos')
"
```

### 2. Mejorar fechas (opcional)
Las fechas actualmente estÃ¡n en texto ("7 y 8 de noviembre de 2025").
Considerar crear parser de fechas en espaÃ±ol.

### 3. Enriquecer con APIs reales (Opcional)
- Eventbrite API
- Ticketmaster API
- Meetup API

### 4. Probar en Frontend
```bash
cd frontend
npm run dev
# Abrir http://localhost:5174
# Buscar eventos por ciudad
```

---

## ğŸš¨ NOTAS IMPORTANTES

### âš ï¸ Limitaciones Conocidas
1. **Fechas en texto:** No parseadas a datetime (usa 2025-12-31 genÃ©rica)
2. **Brasil bajo rendimiento:** Eventos no pasan validaciÃ³n (info pobre)
3. **Regiones sin eventos:** CABA, Ciudad de MÃ©xico, Mendoza

### âœ… Funcionando Perfectamente
1. **ImÃ¡genes:** 100% de eventos con imagen de calidad
2. **Duplicados:** Sistema de detecciÃ³n funcionando (0 duplicados)
3. **ValidaciÃ³n:** 83.9% de eventos pasan filtros de calidad
4. **InserciÃ³n DB:** Sin errores, manejo de duplicados correcto

### ğŸ”„ Delays y Rate Limiting
- **5 segundos** entre regiones
- **10 segundos** entre paÃ­ses
- **20 segundos** esperando respuesta de Gemini

**Tiempo total estimado:** 2.5-3 horas para 49 regiones

---

## ğŸ“ COMANDOS DE EMERGENCIA

### Cancelar ejecuciÃ³n
```bash
Ctrl + C  # Interrumpir pipeline
```

### Limpiar y reiniciar
```bash
# NO HACER A MENOS QUE SEA NECESARIO
# Esto borra todo el progreso

# OpciÃ³n 1: Solo limpiar scraped (mantener curated y DB)
rm backend/data/ai_scraped/*_gemini_response.json

# OpciÃ³n 2: Limpiar todo (PELIGROSO)
rm backend/data/ai_scraped/*_gemini_response.json
rm backend/data/curated/curated_*_gemini_response.json
# DB: DELETE FROM events WHERE source = 'gemini_auto';
```

### Ver proceso corriendo
```bash
# Windows
tasklist | findstr python

# Si necesitas matar el proceso
taskkill /F /PID <pid>
```

---

## ğŸ‰ Ã‰XITO

Cuando veas este mensaje, el scraping estÃ¡ completo:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š ESTADÃSTICAS FINALES DEL PIPELINE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” Regiones scrapeadas:      49
ğŸ“¥ Eventos encontrados:       XXX
âœ… Eventos curados:           XXX
ğŸ’¾ Eventos insertados:        XXX
ğŸ”„ Eventos actualizados:      XX
âŒ Errores:                   X
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**Â¡Sistema listo para producciÃ³n!** ğŸš€

---

## ğŸ“š ARCHIVOS DE REFERENCIA

- `PROGRESS_SCRAPING.md` - Estado actual detallado
- `autonomous_scraping_pipeline.py` - Script principal
- `curate_ai_events.py` - Curador de eventos
- `process_scraped.py` - InserciÃ³n a DB
- `latinamerica_regions_sample.json` - 80 regiones

---

**Ãšltima ejecuciÃ³n exitosa:** 2025-11-09 08:00
**Eventos en DB:** 97 eventos
**PrÃ³ximo paso:** Ejecutar `--mode continue` para completar
